2024-06-19 15:22:45,348 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:22:45,364 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:22:45,430 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:22:45,451 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:22:45,452 - INFO - info on query passed: Ticketing and the class name: <weaviate.Collection config={
  "name": "Admin_test_class_7",
  "description": null,
  "generative_config": null,
  "inverted_index_config": {
    "bm25": {
      "b": 0.75,
      "k1": 1.2
    },
    "cleanup_interval_seconds": 60,
    "index_null_state": false,
    "index_property_length": false,
    "index_timestamps": false,
    "stopwords": {
      "preset": "en",
      "additions": null,
      "removals": null
    }
  },
  "multi_tenancy_config": {
    "enabled": false,
    "auto_tenant_creation": false,
    "auto_tenant_activation": false
  },
  "properties": [
    {
      "name": "document_title",
      "description": null,
      "data_type": "text",
      "index_filterable": true,
      "index_searchable": true,
      "nested_properties": null,
      "tokenization": "word",
      "vectorizer_config": {
        "skip": false,
        "vectorize_property_name": true
      },
      "vectorizer": "text2vec-huggingface"
    },
    {
      "name": "page_content",
      "description": null,
      "data_type": "text",
      "index_filterable": true,
      "index_searchable": true,
      "nested_properties": null,
      "tokenization": "word",
      "vectorizer_config": {
        "skip": false,
        "vectorize_property_name": true
      },
      "vectorizer": "text2vec-huggingface"
    }
  ],
  "references": [],
  "replication_config": {
    "factor": 1
  },
  "reranker_config": null,
  "sharding_config": {
    "virtual_per_physical": 128,
    "desired_count": 1,
    "actual_count": 1,
    "desired_virtual_count": 128,
    "actual_virtual_count": 128,
    "key": "_id",
    "strategy": "hash",
    "function": "murmur3"
  },
  "vector_index_config": {
    "quantizer": null,
    "cleanup_interval_seconds": 300,
    "distance_metric": "cosine",
    "dynamic_ef_min": 100,
    "dynamic_ef_max": 500,
    "dynamic_ef_factor": 8,
    "ef": -1,
    "ef_construction": 128,
    "flat_search_cutoff": 40000,
    "max_connections": 64,
    "skip": false,
    "vector_cache_max_objects": 1000000000000
  },
  "vector_index_type": "hnsw",
  "vectorizer_config": {
    "vectorizer": "text2vec-huggingface",
    "model": {
      "model": "sentence-transformers/all-MiniLM-L6-v2"
    },
    "vectorize_collection_name": true
  },
  "vectorizer": "text2vec-huggingface",
  "vector_config": null
}>
2024-06-19 15:22:45,477 - INFO - logged properties: {'page_content': 'Countdown clock for UEFA\nEuro 2024 in front of\nDÃ¼sseldorf City HallPlayer Offence(s) Suspension(s)\n Giorgi Loria\n  in qualifying vs Greece (26 March 2024) Group F vs Turkey (matchday 1; 18 June 2024)', 'document_title': 'UEFA_Euro_2024.pdf'}
2024-06-19 15:22:45,477 - INFO - logged distance: 0.737564742565155
2024-06-19 15:22:45,477 - INFO - logged properties: {'page_content': 'Source: UEFA[95]\nA player is automatically suspended for the next match for the following of fences:[69]\nReceiving a red card (red card suspensions can be extended for serious offences)', 'document_title': 'UEFA_Euro_2024.pdf'}
2024-06-19 15:22:45,477 - INFO - logged distance: 0.7729887962341309
2024-06-19 15:22:45,477 - INFO - Here is the response after similarity search request: QueryReturn(objects=[Object(uuid=_WeaviateUUIDInt('d7d484e0-357a-4715-a6c4-1e2183bc538e'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=0.737564742565155, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'page_content': 'Countdown clock for UEFA\nEuro 2024 in front of\nDÃ¼sseldorf City HallPlayer Offence(s) Suspension(s)\n Giorgi Loria\n  in qualifying vs Greece (26 March 2024) Group F vs Turkey (matchday 1; 18 June 2024)', 'document_title': 'UEFA_Euro_2024.pdf'}, references=None, vector={}, collection='Admin_test_class_7'), Object(uuid=_WeaviateUUIDInt('7e662657-5fbc-466a-ae19-c575cf0ffe03'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=0.7729887962341309, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'page_content': 'Source: UEFA[95]\nA player is automatically suspended for the next match for the following of fences:[69]\nReceiving a red card (red card suspensions can be extended for serious offences)', 'document_title': 'UEFA_Euro_2024.pdf'}, references=None, vector={}, collection='Admin_test_class_7')])
2024-06-19 15:24:03,102 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:24:03,119 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:24:03,179 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:24:03,230 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7f3b69dc0a00>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:24:03,230 - INFO - Checking the embedder : sentence-transformers/all-MiniLM-L6-v2 and the current llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:24:03,247 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:24:03,263 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:24:03,325 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:24:03,344 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 15:24:04,158 - INFO - collection is admin_test_class_7, and client: <weaviate.client.WeaviateClient object at 0x7f3b94c14940>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 15:24:04,160 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:24:04,162 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:24:04,163 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f3b84327e50>
2024-06-19 15:24:04,164 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f3b84327e50>
2024-06-19 15:24:04,164 - INFO - Checkpoint RAG
2024-06-19 15:24:04,235 - INFO - Checking prompt before RAG: input_variables=['context', 'question'] metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: {question} \nContext: {context} \nAnswer:"))] and checking if retriever exists: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f3b84327e50>, checking llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:24:04,236 - INFO - logging the rag chain: first={
  context: VectorStoreRetriever(tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'], vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f3b84327e50>),
  question: RunnablePassthrough()
} middle=[ChatPromptTemplate(input_variables=['context', 'question'], metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: {question} \nContext: {context} \nAnswer:"))]), VLLMOpenAI(client=<openai.resources.completions.Completions object at 0x7f3b94c59e70>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7f3b69dc08b0>, model_name='meta-llama/Llama-2-7b-chat-hf', model_kwargs={'stop': ['.']}, openai_api_key='EMPTY', openai_api_base='http://localhost:8500/v1', openai_proxy='', streaming=True)] last=StrOutputParser() and retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f3b84327e50>
2024-06-19 15:24:04,280 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 400 Bad Request"
2024-06-19 15:24:04,280 - ERROR - An error occurred: Error code: 400 - {'object': 'error', 'message': "This model's maximum context length is 512 tokens. However, you requested 698 tokens (442 in the messages, 256 in the completion). Please reduce the length of the messages or completion.", 'type': 'BadRequestError', 'param': None, 'code': 400}
2024-06-19 15:26:53,759 - INFO - At initilization stage embedding model is: None and vllm model is: None
2024-06-19 15:26:59,222 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:26:59,240 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:26:59,306 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:26:59,705 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7f7e7dcee3e0>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:26:59,705 - INFO - Checking the embedder : sentence-transformers/all-MiniLM-L6-v2 and the current llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:26:59,722 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:26:59,738 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:26:59,799 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:26:59,817 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 15:27:00,617 - INFO - collection is admin_test_class_7, and client: <weaviate.client.WeaviateClient object at 0x7f7da4578b80>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 15:27:00,619 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:27:00,620 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:27:00,621 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f7da4408130>
2024-06-19 15:27:00,621 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f7da4408130>
2024-06-19 15:27:00,621 - INFO - Checkpoint RAG
2024-06-19 15:27:00,622 - INFO - Checking prompt before RAG: input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))] and checking if retriever exists: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f7da4408130>, checking llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:27:00,623 - INFO - logging the rag chain: first={
  context: VectorStoreRetriever(tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'], vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f7da4408130>),
  question: RunnablePassthrough()
} middle=[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))]), VLLMOpenAI(client=<openai.resources.completions.Completions object at 0x7f7db4b52770>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7f7da455e770>, model_name='meta-llama/Llama-2-7b-chat-hf', model_kwargs={'stop': ['.']}, openai_api_key='EMPTY', openai_api_base='http://localhost:8500/v1', openai_proxy='', streaming=True)] last=StrOutputParser() and retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f7da4408130>
2024-06-19 15:27:01,046 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 400 Bad Request"
2024-06-19 15:27:01,048 - ERROR - An error occurred: Error code: 400 - {'object': 'error', 'message': "This model's maximum context length is 512 tokens. However, you requested 712 tokens (456 in the messages, 256 in the completion). Please reduce the length of the messages or completion.", 'type': 'BadRequestError', 'param': None, 'code': 400}
2024-06-19 15:34:36,626 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:34:36,648 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:34:36,707 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:34:36,771 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7f7d8bb27a90>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:34:36,771 - INFO - Checking the embedder : sentence-transformers/all-MiniLM-L6-v2 and the current llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:34:36,791 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:34:36,808 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:34:36,871 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:34:36,891 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 15:34:38,037 - INFO - collection is admin_test_class_7, and client: <weaviate.client.WeaviateClient object at 0x7f7d8b9cec80>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 15:34:38,040 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:34:38,042 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:34:38,042 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f7da42ed420>
2024-06-19 15:34:38,044 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f7da42ed420>
2024-06-19 15:34:38,045 - INFO - Checkpoint RAG
2024-06-19 15:34:38,045 - INFO - Checking prompt before RAG: input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))] and checking if retriever exists: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f7da42ed420>, checking llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:34:38,046 - INFO - logging the rag chain: first={
  context: VectorStoreRetriever(tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'], vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f7da42ed420>),
  question: RunnablePassthrough()
} middle=[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))]), VLLMOpenAI(client=<openai.resources.completions.Completions object at 0x7f7e7dcee3e0>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7f7d8b9cca30>, model_name='meta-llama/Llama-2-7b-chat-hf', model_kwargs={'stop': ['.']}, openai_api_key='EMPTY', openai_api_base='http://localhost:8500/v1', openai_proxy='', streaming=True)] last=StrOutputParser() and retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f7da42ed420>
2024-06-19 15:34:38,097 - INFO - Retrying request to /completions in 0.961940 seconds
2024-06-19 15:34:39,063 - INFO - Retrying request to /completions in 1.786003 seconds
2024-06-19 15:34:40,859 - ERROR - An error occurred: Connection error.
2024-06-19 15:35:05,888 - INFO - At initilization stage embedding model is: None and vllm model is: None
2024-06-19 15:35:57,001 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:35:57,019 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:35:57,079 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:35:57,098 - INFO - HTTP Request: GET http://localhost:8080/v1/schema "HTTP/1.1 200 OK"
2024-06-19 15:35:57,099 - INFO - classes: {'Admin_test_class_5': _CollectionConfigSimple(name='Admin_test_class_5', description=None, generative_config=None, properties=[_Property(name='title', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='body', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='page_content', description="This property was generated by Weaviate's auto-schema feature on Wed Jun 19 14:44:12 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface'), _Property(name='document_title', description="This property was generated by Weaviate's auto-schema feature on Wed Jun 19 14:44:12 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface')], references=[], reranker_config=None, vectorizer_config=_VectorizerConfig(vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, model={'model': 'sentence-transformers/all-MiniLM-L6-v2'}, vectorize_collection_name=True), vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, vector_config=None), 'Admin_test_class_6': _CollectionConfigSimple(name='Admin_test_class_6', description=None, generative_config=None, properties=[_Property(name='title', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='body', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='page_content', description="This property was generated by Weaviate's auto-schema feature on Wed Jun 19 14:50:16 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface'), _Property(name='document_title', description="This property was generated by Weaviate's auto-schema feature on Wed Jun 19 14:50:16 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface')], references=[], reranker_config=None, vectorizer_config=_VectorizerConfig(vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, model={'model': 'sentence-transformers/all-MiniLM-L6-v2'}, vectorize_collection_name=True), vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, vector_config=None), 'Admin_test_class_4': _CollectionConfigSimple(name='Admin_test_class_4', description=None, generative_config=None, properties=[_Property(name='title', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='body', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='page_content', description="This property was generated by Weaviate's auto-schema feature on Wed Jun 19 13:59:24 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface'), _Property(name='document_title', description="This property was generated by Weaviate's auto-schema feature on Wed Jun 19 13:59:24 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface')], references=[], reranker_config=None, vectorizer_config=_VectorizerConfig(vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, model={'model': 'sentence-transformers/all-MiniLM-L6-v2'}, vectorize_collection_name=True), vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, vector_config=None), 'Admin_test_class': _CollectionConfigSimple(name='Admin_test_class', description=None, generative_config=None, properties=[_Property(name='title', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='body', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='page_content', description="This property was generated by Weaviate's auto-schema feature on Tue Jun 18 20:45:56 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface'), _Property(name='document_title', description="This property was generated by Weaviate's auto-schema feature on Tue Jun 18 20:45:56 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface')], references=[], reranker_config=None, vectorizer_config=_VectorizerConfig(vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, model={'model': 'sentence-transformers/all-MiniLM-L6-v2'}, vectorize_collection_name=True), vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, vector_config=None), 'Admin_test_class_2': _CollectionConfigSimple(name='Admin_test_class_2', description=None, generative_config=None, properties=[_Property(name='title', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='body', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='source', description="This property was generated by Weaviate's auto-schema feature on Wed Jun 19 13:43:15 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface'), _Property(name='page', description="This property was generated by Weaviate's auto-schema feature on Wed Jun 19 13:43:15 2024", data_type=<DataType.NUMBER: 'number'>, index_filterable=True, index_searchable=False, nested_properties=None, tokenization=None, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface'), _Property(name='text', description="This property was generated by Weaviate's auto-schema feature on Wed Jun 19 13:43:15 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface')], references=[], reranker_config=None, vectorizer_config=_VectorizerConfig(vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, model={'model': 'sentence-transformers/all-MiniLM-L6-v2'}, vectorize_collection_name=True), vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, vector_config=None), 'Admin_test_class_3': _CollectionConfigSimple(name='Admin_test_class_3', description=None, generative_config=None, properties=[_Property(name='title', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='body', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='page_content', description="This property was generated by Weaviate's auto-schema feature on Wed Jun 19 13:23:21 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface'), _Property(name='document_title', description="This property was generated by Weaviate's auto-schema feature on Wed Jun 19 13:23:21 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface')], references=[], reranker_config=None, vectorizer_config=_VectorizerConfig(vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, model={'model': 'sentence-transformers/all-MiniLM-L6-v2'}, vectorize_collection_name=True), vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, vector_config=None), 'Admin_test_class_7': _CollectionConfigSimple(name='Admin_test_class_7', description=None, generative_config=None, properties=[_Property(name='document_title', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='page_content', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface')], references=[], reranker_config=None, vectorizer_config=_VectorizerConfig(vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, model={'model': 'sentence-transformers/all-MiniLM-L6-v2'}, vectorize_collection_name=True), vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, vector_config=None)}: %s
2024-06-19 15:36:13,687 - INFO - query success: 1 documents found
2024-06-19 15:36:26,358 - INFO - query success: 1 documents found
2024-06-19 15:37:04,780 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:37:04,804 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:37:04,860 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:37:05,383 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7f23cc42a3b0>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:37:05,383 - INFO - Checking the embedder : sentence-transformers/all-MiniLM-L6-v2 and the current llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:37:05,399 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:37:05,416 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:37:05,475 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:37:05,493 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 15:37:06,531 - INFO - collection is admin_test_class_7, and client: <weaviate.client.WeaviateClient object at 0x7f22dcc88c10>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 15:37:06,533 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:37:06,535 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:37:06,535 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22dc8ff430>
2024-06-19 15:37:06,535 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22dc8ff430>
2024-06-19 15:37:06,536 - INFO - Checkpoint RAG
2024-06-19 15:37:06,536 - INFO - Checking prompt before RAG: input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))] and checking if retriever exists: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22dc8ff430>, checking llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:37:06,538 - INFO - logging the rag chain: first={
  context: VectorStoreRetriever(tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'], vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22dc8ff430>),
  question: RunnablePassthrough()
} middle=[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))]), VLLMOpenAI(client=<openai.resources.completions.Completions object at 0x7f22fb9f4c10>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7f22dcc6a800>, model_name='meta-llama/Llama-2-7b-chat-hf', model_kwargs={'stop': ['.']}, openai_api_key='EMPTY', openai_api_base='http://localhost:8500/v1', openai_proxy='', streaming=True)] last=StrOutputParser() and retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22dc8ff430>
2024-06-19 15:37:07,106 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 15:37:08,216 - INFO - Checking the response of do rag:  The prize for the winner of UEFA Euro 2024 is â¬28
2024-06-19 15:37:08,218 - INFO - Checking the response of do rag:  The prize for the winner of UEFA Euro 2024 is â¬28
2024-06-19 15:37:38,228 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:37:38,244 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:37:38,307 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:37:38,355 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7f22fb9f6980>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:37:38,356 - INFO - Checking the embedder : sentence-transformers/all-MiniLM-L6-v2 and the current llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:37:38,374 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:37:38,390 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:37:38,449 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:37:38,469 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 15:37:39,163 - INFO - collection is admin_test_class_7, and client: <weaviate.client.WeaviateClient object at 0x7f22d6b04460>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 15:37:39,165 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:37:39,167 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:37:39,167 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22dccbc0a0>
2024-06-19 15:37:39,169 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22dccbc0a0>
2024-06-19 15:37:39,169 - INFO - Checkpoint RAG
2024-06-19 15:37:39,169 - INFO - Checking prompt before RAG: input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))] and checking if retriever exists: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22dccbc0a0>, checking llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:37:39,170 - INFO - logging the rag chain: first={
  context: VectorStoreRetriever(tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'], vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22dccbc0a0>),
  question: RunnablePassthrough()
} middle=[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))]), VLLMOpenAI(client=<openai.resources.completions.Completions object at 0x7f22d6c1b2e0>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7f22d6ab26e0>, model_name='meta-llama/Llama-2-7b-chat-hf', model_kwargs={'stop': ['.']}, openai_api_key='EMPTY', openai_api_base='http://localhost:8500/v1', openai_proxy='', streaming=True)] last=StrOutputParser() and retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22dccbc0a0>
2024-06-19 15:37:39,220 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 15:37:40,311 - INFO - Checking the response of do rag:  The price range for tickets at the Euro 2024 is â¬2
2024-06-19 15:37:40,312 - INFO - Checking the response of do rag:  The price range for tickets at the Euro 2024 is â¬2
2024-06-19 15:38:07,160 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:38:07,176 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:38:07,236 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:38:07,284 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7f22d6c1ad10>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:38:07,284 - INFO - Checking the embedder : sentence-transformers/all-MiniLM-L6-v2 and the current llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:38:07,300 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:38:07,315 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:38:07,371 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:38:07,391 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 15:38:08,131 - INFO - collection is admin_test_class_7, and client: <weaviate.client.WeaviateClient object at 0x7f22d6b07280>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 15:38:08,133 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:38:08,135 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:38:08,135 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22d6b1e7d0>
2024-06-19 15:38:08,137 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22d6b1e7d0>
2024-06-19 15:38:08,137 - INFO - Checkpoint RAG
2024-06-19 15:38:08,138 - INFO - Checking prompt before RAG: input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))] and checking if retriever exists: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22d6b1e7d0>, checking llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:38:08,138 - INFO - logging the rag chain: first={
  context: VectorStoreRetriever(tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'], vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22d6b1e7d0>),
  question: RunnablePassthrough()
} middle=[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))]), VLLMOpenAI(client=<openai.resources.completions.Completions object at 0x7f22d6c1b4c0>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7f22d6e2f5e0>, model_name='meta-llama/Llama-2-7b-chat-hf', model_kwargs={'stop': ['.']}, openai_api_key='EMPTY', openai_api_base='http://localhost:8500/v1', openai_proxy='', streaming=True)] last=StrOutputParser() and retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22d6b1e7d0>
2024-06-19 15:38:08,181 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 15:38:09,397 - INFO - Checking the response of do rag:  I apologize, but I don't know who hosts the Euro 2024
2024-06-19 15:38:09,399 - INFO - Checking the response of do rag:  I apologize, but I don't know who hosts the Euro 2024
2024-06-19 15:38:30,234 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:38:30,250 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:38:30,312 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:38:30,359 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7f22d6b05900>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:38:30,360 - INFO - Checking the embedder : sentence-transformers/all-MiniLM-L6-v2 and the current llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:38:30,375 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:38:30,391 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:38:30,447 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:38:30,471 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 15:38:31,143 - INFO - collection is admin_test_class_7, and client: <weaviate.client.WeaviateClient object at 0x7f22d693c0d0>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 15:38:31,145 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:38:31,147 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:38:31,147 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22d6be2a40>
2024-06-19 15:38:31,149 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22d6be2a40>
2024-06-19 15:38:31,149 - INFO - Checkpoint RAG
2024-06-19 15:38:31,150 - INFO - Checking prompt before RAG: input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))] and checking if retriever exists: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22d6be2a40>, checking llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:38:31,150 - INFO - logging the rag chain: first={
  context: VectorStoreRetriever(tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'], vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22d6be2a40>),
  question: RunnablePassthrough()
} middle=[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))]), VLLMOpenAI(client=<openai.resources.completions.Completions object at 0x7f22d6c1acb0>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7f22f80b8c40>, model_name='meta-llama/Llama-2-7b-chat-hf', model_kwargs={'stop': ['.']}, openai_api_key='EMPTY', openai_api_base='http://localhost:8500/v1', openai_proxy='', streaming=True)] last=StrOutputParser() and retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22d6be2a40>
2024-06-19 15:38:31,219 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 15:38:33,238 - INFO - Checking the response of do rag: 
        Certainly! The Euro 2024 tournament is scheduled to take place in Germany, as voted by UEFA members in September 2018
2024-06-19 15:38:33,240 - INFO - Checking the response of do rag: 
        Certainly! The Euro 2024 tournament is scheduled to take place in Germany, as voted by UEFA members in September 2018
2024-06-19 15:39:22,337 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:39:22,353 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:39:22,411 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:39:22,458 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7f22dcc88730>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:39:22,458 - INFO - Checking the embedder : sentence-transformers/all-MiniLM-L6-v2 and the current llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:39:22,473 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:39:22,488 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:39:22,543 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:39:22,561 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 15:39:23,336 - INFO - collection is admin_test_class_7, and client: <weaviate.client.WeaviateClient object at 0x7f22d69f59c0>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 15:39:23,338 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:39:23,340 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:39:23,341 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22d693cee0>
2024-06-19 15:39:23,342 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22d693cee0>
2024-06-19 15:39:23,342 - INFO - Checkpoint RAG
2024-06-19 15:39:23,343 - INFO - Checking prompt before RAG: input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))] and checking if retriever exists: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22d693cee0>, checking llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:39:23,343 - INFO - logging the rag chain: first={
  context: VectorStoreRetriever(tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'], vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22d693cee0>),
  question: RunnablePassthrough()
} middle=[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))]), VLLMOpenAI(client=<openai.resources.completions.Completions object at 0x7f22dcc8add0>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7f22d6b1e980>, model_name='meta-llama/Llama-2-7b-chat-hf', model_kwargs={'stop': ['.']}, openai_api_key='EMPTY', openai_api_base='http://localhost:8500/v1', openai_proxy='', streaming=True)] last=StrOutputParser() and retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22d693cee0>
2024-06-19 15:39:23,384 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 15:39:25,053 - INFO - Checking the response of do rag:  The participating teams in the UEFA Euro 2024 tournament are Group A, Group B, Group C, and so on
2024-06-19 15:39:25,055 - INFO - Checking the response of do rag:  The participating teams in the UEFA Euro 2024 tournament are Group A, Group B, Group C, and so on
2024-06-19 15:42:33,112 - INFO - At initilization stage embedding model is: None and vllm model is: None
2024-06-19 15:43:08,884 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:43:08,901 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:43:08,959 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:43:09,363 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7fb4a7a8a3b0>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 15:43:09,364 - INFO - Checking the embedder : sentence-transformers/all-MiniLM-L6-v2 and the current llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 15:43:09,379 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:43:09,396 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:43:09,455 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:43:09,473 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 15:43:10,390 - INFO - collection is admin_test_class_7, and client: <weaviate.client.WeaviateClient object at 0x7fb3dc2c0bb0>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 15:43:10,392 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:43:10,394 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:43:10,395 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fb3dc2f4160>
2024-06-19 15:43:10,395 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fb3dc2f4160>
2024-06-19 15:43:10,395 - INFO - Checkpoint RAG
2024-06-19 15:43:10,396 - INFO - Checking prompt before RAG: input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))] and checking if retriever exists: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fb3dc2f4160>, checking llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 15:43:10,397 - INFO - logging the rag chain: first={
  context: VectorStoreRetriever(tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'], vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fb3dc2f4160>),
  question: RunnablePassthrough()
} middle=[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))]), VLLMOpenAI(client=<openai.resources.completions.Completions object at 0x7fb3de04a7a0>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7fb3dc4a67a0>, model_name='meta-llama/Llama-2-7b-chat-hf', max_tokens=1084, model_kwargs={'stop': ['.']}, openai_api_key='EMPTY', openai_api_base='http://localhost:8500/v1', openai_proxy='')] last=StrOutputParser() and retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fb3dc2f4160>
2024-06-19 15:43:12,982 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 15:43:12,994 - INFO - Checking the response of do rag:  The participating teams in the UEFA Euro 2024 tournament are Group A, Group B, Group C, Group D, Group E, Group F, and Group G
2024-06-19 15:43:12,995 - INFO - Checking the response of do rag:  The participating teams in the UEFA Euro 2024 tournament are Group A, Group B, Group C, Group D, Group E, Group F, and Group G
2024-06-19 15:51:11,354 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:51:11,370 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:51:11,431 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:51:11,491 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7fb3dc6e24d0>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 15:51:11,491 - INFO - Checking the embedder : sentence-transformers/all-MiniLM-L6-v2 and the current llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 15:51:11,518 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:51:11,543 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:51:11,602 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:51:11,620 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 15:51:12,417 - INFO - collection is admin_test_class_7, and client: <weaviate.client.WeaviateClient object at 0x7fb3b57837f0>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 15:51:12,419 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:51:12,421 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:51:12,421 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fb3dc18f940>
2024-06-19 15:51:12,423 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fb3dc18f940>
2024-06-19 15:51:12,423 - INFO - Checkpoint RAG
2024-06-19 15:51:12,424 - INFO - Checking prompt before RAG: input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))] and checking if retriever exists: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fb3dc18f940>, checking llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 15:51:12,424 - INFO - logging the rag chain: first={
  context: VectorStoreRetriever(tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'], vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fb3dc18f940>),
  question: RunnablePassthrough()
} middle=[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))]), VLLMOpenAI(client=<openai.resources.completions.Completions object at 0x7fb4a7a8beb0>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7fb3b5781ab0>, model_name='meta-llama/Llama-2-7b-chat-hf', max_tokens=1084, model_kwargs={'stop': ['.']}, openai_api_key='EMPTY', openai_api_base='http://localhost:8500/v1', openai_proxy='')] last=StrOutputParser() and retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fb3dc18f940>
2024-06-19 15:51:14,445 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 15:51:14,447 - INFO - Checking the response of do rag:  The stadiums for the UEFA Euro 2024 are Berlin, Cologne, Dortmund, Hamburg, Leipzig, Munich, Stuttgart, and Zurich
2024-06-19 15:51:14,448 - INFO - Checking the response of do rag:  The stadiums for the UEFA Euro 2024 are Berlin, Cologne, Dortmund, Hamburg, Leipzig, Munich, Stuttgart, and Zurich
2024-06-19 15:52:59,803 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:52:59,819 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:52:59,883 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:52:59,930 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7fb3de114d00>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 15:52:59,930 - INFO - Checking the embedder : sentence-transformers/all-MiniLM-L6-v2 and the current llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 15:52:59,945 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:52:59,961 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:53:00,018 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:53:00,042 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 15:53:00,708 - INFO - collection is admin_test_class_7, and client: <weaviate.client.WeaviateClient object at 0x7fb3b582c640>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 15:53:00,710 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:53:00,712 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:53:00,712 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fb3dc6e1ae0>
2024-06-19 15:53:00,714 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fb3dc6e1ae0>
2024-06-19 15:53:00,714 - INFO - Checkpoint RAG
2024-06-19 15:53:00,714 - INFO - Checking prompt before RAG: input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))] and checking if retriever exists: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fb3dc6e1ae0>, checking llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 15:53:00,715 - INFO - logging the rag chain: first={
  context: VectorStoreRetriever(tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'], vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fb3dc6e1ae0>),
  question: RunnablePassthrough()
} middle=[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))]), VLLMOpenAI(client=<openai.resources.completions.Completions object at 0x7fb3b5783730>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7fb3dc6e1ff0>, model_name='meta-llama/Llama-2-7b-chat-hf', max_tokens=1084, model_kwargs={'stop': ['.']}, openai_api_key='EMPTY', openai_api_base='http://localhost:8500/v1', openai_proxy='')] last=StrOutputParser() and retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fb3dc6e1ae0>
2024-06-19 15:53:06,194 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 15:53:06,197 - INFO - Checking the response of do rag:  The UEFA Euro 2024 tournament will be hosted in 10 different cities across Germany, including:
         - Dortmund
         - DÃ¼sseldorf
         - Frankfurt
         - Gelsenkirchen
         - Hamburg
         - Munich
         - Berlin
         - Cologne
         - Stuttgart
         - Leipzig

I don't know the exact number of venues, as the context doesn't provide that information
2024-06-19 15:53:06,199 - INFO - Checking the response of do rag:  The UEFA Euro 2024 tournament will be hosted in 10 different cities across Germany, including:
         - Dortmund
         - DÃ¼sseldorf
         - Frankfurt
         - Gelsenkirchen
         - Hamburg
         - Munich
         - Berlin
         - Cologne
         - Stuttgart
         - Leipzig

I don't know the exact number of venues, as the context doesn't provide that information
2024-06-19 18:15:59,089 - INFO - At initilization stage embedding model is: None and vllm model is: None
2024-06-19 18:16:09,522 - INFO - checking the request/ username='admin' class_name='test_class_8' mode='create_collection' vectorDB_type='Weaviate' file_path=None file_title=None query='how many venues?' model='meta-llama/Llama-2-7b-chat-hf' inference_endpoint='http://localhost:8500/v1' memory=False conversation_number=-1 embedder='sentence-transformers/all-MiniLM-L6-v2': %s
2024-06-19 18:16:09,538 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 18:16:09,555 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 18:16:09,612 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 18:16:09,641 - INFO - HTTP Request: POST http://localhost:8080/v1/schema "HTTP/1.1 200 OK"
2024-06-19 18:16:35,510 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 18:16:35,527 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 18:16:35,585 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 18:16:35,606 - INFO - HTTP Request: GET http://localhost:8080/v1/schema "HTTP/1.1 200 OK"
2024-06-19 18:16:35,607 - INFO - classes: {'Admin_test_class_5': _CollectionConfigSimple(name='Admin_test_class_5', description=None, generative_config=None, properties=[_Property(name='title', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='body', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='page_content', description="This property was generated by Weaviate's auto-schema feature on Wed Jun 19 14:44:12 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface'), _Property(name='document_title', description="This property was generated by Weaviate's auto-schema feature on Wed Jun 19 14:44:12 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface')], references=[], reranker_config=None, vectorizer_config=_VectorizerConfig(vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, model={'model': 'sentence-transformers/all-MiniLM-L6-v2'}, vectorize_collection_name=True), vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, vector_config=None), 'Admin_test_class_8': _CollectionConfigSimple(name='Admin_test_class_8', description=None, generative_config=None, properties=[_Property(name='document_title', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='page_content', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface')], references=[], reranker_config=None, vectorizer_config=_VectorizerConfig(vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, model={'model': 'sentence-transformers/all-MiniLM-L6-v2'}, vectorize_collection_name=True), vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, vector_config=None), 'Admin_test_class_6': _CollectionConfigSimple(name='Admin_test_class_6', description=None, generative_config=None, properties=[_Property(name='title', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='body', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='page_content', description="This property was generated by Weaviate's auto-schema feature on Wed Jun 19 14:50:16 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface'), _Property(name='document_title', description="This property was generated by Weaviate's auto-schema feature on Wed Jun 19 14:50:16 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface')], references=[], reranker_config=None, vectorizer_config=_VectorizerConfig(vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, model={'model': 'sentence-transformers/all-MiniLM-L6-v2'}, vectorize_collection_name=True), vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, vector_config=None), 'Admin_test_class_4': _CollectionConfigSimple(name='Admin_test_class_4', description=None, generative_config=None, properties=[_Property(name='title', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='body', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='page_content', description="This property was generated by Weaviate's auto-schema feature on Wed Jun 19 13:59:24 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface'), _Property(name='document_title', description="This property was generated by Weaviate's auto-schema feature on Wed Jun 19 13:59:24 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface')], references=[], reranker_config=None, vectorizer_config=_VectorizerConfig(vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, model={'model': 'sentence-transformers/all-MiniLM-L6-v2'}, vectorize_collection_name=True), vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, vector_config=None), 'Admin_test_class': _CollectionConfigSimple(name='Admin_test_class', description=None, generative_config=None, properties=[_Property(name='title', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='body', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='page_content', description="This property was generated by Weaviate's auto-schema feature on Tue Jun 18 20:45:56 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface'), _Property(name='document_title', description="This property was generated by Weaviate's auto-schema feature on Tue Jun 18 20:45:56 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface')], references=[], reranker_config=None, vectorizer_config=_VectorizerConfig(vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, model={'model': 'sentence-transformers/all-MiniLM-L6-v2'}, vectorize_collection_name=True), vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, vector_config=None), 'Admin_test_class_2': _CollectionConfigSimple(name='Admin_test_class_2', description=None, generative_config=None, properties=[_Property(name='title', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='body', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='source', description="This property was generated by Weaviate's auto-schema feature on Wed Jun 19 13:43:15 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface'), _Property(name='page', description="This property was generated by Weaviate's auto-schema feature on Wed Jun 19 13:43:15 2024", data_type=<DataType.NUMBER: 'number'>, index_filterable=True, index_searchable=False, nested_properties=None, tokenization=None, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface'), _Property(name='text', description="This property was generated by Weaviate's auto-schema feature on Wed Jun 19 13:43:15 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface')], references=[], reranker_config=None, vectorizer_config=_VectorizerConfig(vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, model={'model': 'sentence-transformers/all-MiniLM-L6-v2'}, vectorize_collection_name=True), vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, vector_config=None), 'Admin_test_class_3': _CollectionConfigSimple(name='Admin_test_class_3', description=None, generative_config=None, properties=[_Property(name='title', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='body', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='page_content', description="This property was generated by Weaviate's auto-schema feature on Wed Jun 19 13:23:21 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface'), _Property(name='document_title', description="This property was generated by Weaviate's auto-schema feature on Wed Jun 19 13:23:21 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface')], references=[], reranker_config=None, vectorizer_config=_VectorizerConfig(vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, model={'model': 'sentence-transformers/all-MiniLM-L6-v2'}, vectorize_collection_name=True), vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, vector_config=None), 'Admin_test_class_7': _CollectionConfigSimple(name='Admin_test_class_7', description=None, generative_config=None, properties=[_Property(name='document_title', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='page_content', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface')], references=[], reranker_config=None, vectorizer_config=_VectorizerConfig(vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, model={'model': 'sentence-transformers/all-MiniLM-L6-v2'}, vectorize_collection_name=True), vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, vector_config=None)}: %s
2024-06-19 18:17:27,617 - INFO - actors creation successful [Actor(WeaviateEmbedder, c30007921b6cabd7cfeaed6401000000), Actor(WeaviateEmbedder, d671dd55d2a35b096c99dc6401000000), Actor(WeaviateEmbedder, 1e68b254861ab0c275f8d86101000000)]: %s
2024-06-19 18:17:27,619 - INFO - check 1st step of ray was successful
2024-06-19 18:17:27,619 - INFO - check if ray was successful:
2024-06-19 18:17:27,619 - INFO - Quick check of the embedder: None
2024-06-19 18:17:27,619 - INFO - response: {'status': 'success', 'message': 'Processed 83 documents in batches for class admin_test_class_8.'}: %s
2024-06-19 18:17:27,620 - INFO - request processed successfully username='admin' class_name='test_class_8' mode='add_to_collection' vectorDB_type='Weaviate' file_path='/home/nilsb/LLM-Chat-Bot/API/received_files/1bd4ae0c6fda1c9f' file_title=None query='how many venues?' model='meta-llama/Llama-2-7b-chat-hf' inference_endpoint='http://localhost:8500/v1' memory=False conversation_number=-1 embedder='sentence-transformers/all-MiniLM-L6-v2': %s
2024-06-19 18:17:54,932 - INFO - query success: 1 documents found
2024-06-19 18:18:50,659 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 18:18:50,676 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 18:18:50,737 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 18:18:50,947 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7feda2c4e380>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 18:18:50,947 - INFO - Checking the embedder : sentence-transformers/all-MiniLM-L6-v2 and the current llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 18:18:50,963 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 18:18:50,978 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 18:18:51,039 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 18:18:51,059 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 18:18:52,006 - INFO - collection is admin_test_class_8, and client: <weaviate.client.WeaviateClient object at 0x7fecb4f0e890>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 18:18:52,008 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 18:18:52,010 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 18:18:52,011 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb294d870>
2024-06-19 18:18:52,011 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb294d870>
2024-06-19 18:18:52,011 - INFO - Checkpoint RAG
2024-06-19 18:18:52,012 - INFO - Checking prompt before RAG: input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))] and checking if retriever exists: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb294d870>, checking llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 18:18:52,014 - INFO - logging the rag chain: first={
  context: VectorStoreRetriever(tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'], vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb294d870>),
  question: RunnablePassthrough()
} middle=[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))]), VLLMOpenAI(client=<openai.resources.completions.Completions object at 0x7fecd9a65120>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7fecb4f0c640>, model_name='meta-llama/Llama-2-7b-chat-hf', max_tokens=1084, model_kwargs={'stop': ['.']}, openai_api_key='EMPTY', openai_api_base='http://localhost:8500/v1', openai_proxy='')] last=StrOutputParser() and retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb294d870>
2024-06-19 18:18:53,303 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 18:18:53,314 - INFO - Checking the response of do rag:  Text splitters are used for formatting and processing text inputs
2024-06-19 18:18:53,316 - INFO - Checking the response of do rag:  Text splitters are used for formatting and processing text inputs
2024-06-19 18:19:35,129 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 18:19:35,145 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 18:19:35,212 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 18:19:35,260 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7fecb50e2200>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 18:19:35,260 - INFO - Checking the embedder : sentence-transformers/all-MiniLM-L6-v2 and the current llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 18:19:35,277 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 18:19:35,293 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 18:19:35,350 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 18:19:35,368 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 18:19:36,193 - INFO - collection is admin_test_class_8, and client: <weaviate.client.WeaviateClient object at 0x7fecb331b640>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 18:19:36,196 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 18:19:36,197 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 18:19:36,198 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb3435c00>
2024-06-19 18:19:36,200 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb3435c00>
2024-06-19 18:19:36,200 - INFO - Checkpoint RAG
2024-06-19 18:19:36,200 - INFO - Checking prompt before RAG: input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))] and checking if retriever exists: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb3435c00>, checking llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 18:19:36,201 - INFO - logging the rag chain: first={
  context: VectorStoreRetriever(tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'], vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb3435c00>),
  question: RunnablePassthrough()
} middle=[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))]), VLLMOpenAI(client=<openai.resources.completions.Completions object at 0x7feda2c4e380>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7fecb3319900>, model_name='meta-llama/Llama-2-7b-chat-hf', max_tokens=1084, model_kwargs={'stop': ['.']}, openai_api_key='EMPTY', openai_api_base='http://localhost:8500/v1', openai_proxy='')] last=StrOutputParser() and retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb3435c00>
2024-06-19 18:19:36,534 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 18:19:36,536 - INFO - Checking the response of do rag: 1
2024-06-19 18:19:36,538 - INFO - Checking the response of do rag: 1
2024-06-19 18:19:40,512 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 18:19:40,528 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 18:19:40,587 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 18:19:40,635 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7fecb337eb90>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 18:19:40,635 - INFO - Checking the embedder : sentence-transformers/all-MiniLM-L6-v2 and the current llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 18:19:40,651 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 18:19:40,667 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 18:19:40,724 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 18:19:40,742 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 18:19:41,520 - INFO - collection is admin_test_class_8, and client: <weaviate.client.WeaviateClient object at 0x7fecb50e2b00>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 18:19:41,522 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 18:19:41,524 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 18:19:41,524 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb50e1ea0>
2024-06-19 18:19:41,526 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb50e1ea0>
2024-06-19 18:19:41,526 - INFO - Checkpoint RAG
2024-06-19 18:19:41,527 - INFO - Checking prompt before RAG: input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))] and checking if retriever exists: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb50e1ea0>, checking llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 18:19:41,527 - INFO - logging the rag chain: first={
  context: VectorStoreRetriever(tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'], vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb50e1ea0>),
  question: RunnablePassthrough()
} middle=[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))]), VLLMOpenAI(client=<openai.resources.completions.Completions object at 0x7fecb337d870>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7fecb3473e80>, model_name='meta-llama/Llama-2-7b-chat-hf', max_tokens=1084, model_kwargs={'stop': ['.']}, openai_api_key='EMPTY', openai_api_base='http://localhost:8500/v1', openai_proxy='')] last=StrOutputParser() and retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb50e1ea0>
2024-06-19 18:19:42,653 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 18:19:42,656 - INFO - Checking the response of do rag: 
        A tool is composed of several components, including:

1
2024-06-19 18:19:42,658 - INFO - Checking the response of do rag: 
        A tool is composed of several components, including:

1
2024-06-19 18:20:33,397 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 18:20:33,413 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 18:20:33,473 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 18:20:33,519 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7fecb331b010>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 18:20:33,519 - INFO - Checking the embedder : sentence-transformers/all-MiniLM-L6-v2 and the current llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 18:20:33,535 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 18:20:33,551 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 18:20:33,610 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 18:20:33,633 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 18:20:34,357 - INFO - collection is admin_test_class_8, and client: <weaviate.client.WeaviateClient object at 0x7fecb33650f0>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 18:20:34,359 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 18:20:34,362 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 18:20:34,362 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb4f24700>
2024-06-19 18:20:34,364 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb4f24700>
2024-06-19 18:20:34,364 - INFO - Checkpoint RAG
2024-06-19 18:20:34,365 - INFO - Checking prompt before RAG: input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))] and checking if retriever exists: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb4f24700>, checking llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 18:20:34,366 - INFO - logging the rag chain: first={
  context: VectorStoreRetriever(tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'], vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb4f24700>),
  question: RunnablePassthrough()
} middle=[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))]), VLLMOpenAI(client=<openai.resources.completions.Completions object at 0x7fecb331b8b0>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7fecd8934af0>, model_name='meta-llama/Llama-2-7b-chat-hf', max_tokens=1084, model_kwargs={'stop': ['.']}, openai_api_key='EMPTY', openai_api_base='http://localhost:8500/v1', openai_proxy='')] last=StrOutputParser() and retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb4f24700>
2024-06-19 18:20:35,794 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 18:20:35,796 - INFO - Checking the response of do rag:  LCEL (LangChain Expression Language) is a declarative way to chain LangChain components
2024-06-19 18:20:35,798 - INFO - Checking the response of do rag:  LCEL (LangChain Expression Language) is a declarative way to chain LangChain components
2024-06-19 18:21:37,973 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 18:21:37,991 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 18:21:38,048 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 18:21:38,095 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7fecb337ff40>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 18:21:38,095 - INFO - Checking the embedder : sentence-transformers/all-MiniLM-L6-v2 and the current llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 18:21:38,111 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 18:21:38,126 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 18:21:38,185 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 18:21:38,203 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 18:21:39,157 - INFO - collection is admin_test_class_8, and client: <weaviate.client.WeaviateClient object at 0x7fecb33efa30>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 18:21:39,159 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 18:21:39,160 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 18:21:39,161 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb3677010>
2024-06-19 18:21:39,162 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb3677010>
2024-06-19 18:21:39,163 - INFO - Checkpoint RAG
2024-06-19 18:21:39,163 - INFO - Checking prompt before RAG: input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))] and checking if retriever exists: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb3677010>, checking llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 18:21:39,164 - INFO - logging the rag chain: first={
  context: VectorStoreRetriever(tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'], vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb3677010>),
  question: RunnablePassthrough()
} middle=[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))]), VLLMOpenAI(client=<openai.resources.completions.Completions object at 0x7fecb337f3a0>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7fecb29c3580>, model_name='meta-llama/Llama-2-7b-chat-hf', max_tokens=1084, model_kwargs={'stop': ['.']}, openai_api_key='EMPTY', openai_api_base='http://localhost:8500/v1', openai_proxy='')] last=StrOutputParser() and retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb3677010>
2024-06-19 18:21:40,571 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 18:21:40,573 - INFO - Checking the response of do rag:  Chat models are language models that use a sequence of messages as inputs and return chat messages as outputs
2024-06-19 18:21:40,574 - INFO - Checking the response of do rag:  Chat models are language models that use a sequence of messages as inputs and return chat messages as outputs
2024-06-19 18:22:21,482 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 18:22:21,498 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 18:22:21,557 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 18:22:21,617 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7fecb337ee00>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 18:22:21,617 - INFO - Checking the embedder : sentence-transformers/all-MiniLM-L6-v2 and the current llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 18:22:21,633 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 18:22:21,650 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 18:22:21,708 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 18:22:21,726 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 18:22:22,415 - INFO - collection is admin_test_class_8, and client: <weaviate.client.WeaviateClient object at 0x7fecb2eac580>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 18:22:22,416 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 18:22:22,418 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 18:22:22,419 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb33ac340>
2024-06-19 18:22:22,420 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb33ac340>
2024-06-19 18:22:22,421 - INFO - Checkpoint RAG
2024-06-19 18:22:22,421 - INFO - Checking prompt before RAG: input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))] and checking if retriever exists: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb33ac340>, checking llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 18:22:22,422 - INFO - logging the rag chain: first={
  context: VectorStoreRetriever(tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'], vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb33ac340>),
  question: RunnablePassthrough()
} middle=[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))]), VLLMOpenAI(client=<openai.resources.completions.Completions object at 0x7fecb337ef20>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7fecb33654e0>, model_name='meta-llama/Llama-2-7b-chat-hf', max_tokens=1084, model_kwargs={'stop': ['.']}, openai_api_key='EMPTY', openai_api_base='http://localhost:8500/v1', openai_proxy='')] last=StrOutputParser() and retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb33ac340>
2024-06-19 18:22:23,341 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 18:22:23,343 - INFO - Checking the response of do rag:  The file format that supports streaming is OpenAI Functions
2024-06-19 18:22:23,344 - INFO - Checking the response of do rag:  The file format that supports streaming is OpenAI Functions
2024-06-19 18:23:20,623 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 18:23:20,638 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 18:23:20,698 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 18:23:20,744 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7fecb3473af0>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 18:23:20,744 - INFO - Checking the embedder : sentence-transformers/all-MiniLM-L6-v2 and the current llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 18:23:20,760 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 18:23:20,776 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 18:23:20,834 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 18:23:20,856 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 18:23:21,721 - INFO - collection is admin_test_class_8, and client: <weaviate.client.WeaviateClient object at 0x7fecb326c310>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 18:23:21,723 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 18:23:21,724 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 18:23:21,725 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb4f24c70>
2024-06-19 18:23:21,727 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb4f24c70>
2024-06-19 18:23:21,727 - INFO - Checkpoint RAG
2024-06-19 18:23:21,727 - INFO - Checking prompt before RAG: input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))] and checking if retriever exists: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb4f24c70>, checking llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 18:23:21,728 - INFO - logging the rag chain: first={
  context: VectorStoreRetriever(tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'], vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb4f24c70>),
  question: RunnablePassthrough()
} middle=[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))]), VLLMOpenAI(client=<openai.resources.completions.Completions object at 0x7fecb33eee60>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7fecb337f220>, model_name='meta-llama/Llama-2-7b-chat-hf', max_tokens=1084, model_kwargs={'stop': ['.']}, openai_api_key='EMPTY', openai_api_base='http://localhost:8500/v1', openai_proxy='')] last=StrOutputParser() and retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb4f24c70>
2024-06-19 18:23:23,391 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 18:23:23,393 - INFO - Checking the response of do rag:  According to the provided documents, agents are the main component of an application's cognitive architecture in the LangChain framework
2024-06-19 18:23:23,395 - INFO - Checking the response of do rag:  According to the provided documents, agents are the main component of an application's cognitive architecture in the LangChain framework
2024-06-19 18:24:13,691 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 18:24:13,707 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 18:24:13,766 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 18:24:13,813 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7fecb33acaf0>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 18:24:13,813 - INFO - Checking the embedder : sentence-transformers/all-MiniLM-L6-v2 and the current llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 18:24:13,828 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 18:24:13,844 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 18:24:13,900 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 18:24:13,918 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 18:24:14,707 - INFO - collection is admin_test_class_8, and client: <weaviate.client.WeaviateClient object at 0x7fecb2ead840>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 18:24:14,709 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 18:24:14,711 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 18:24:14,711 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb326e860>
2024-06-19 18:24:14,713 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb326e860>
2024-06-19 18:24:14,713 - INFO - Checkpoint RAG
2024-06-19 18:24:14,714 - INFO - Checking prompt before RAG: input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))] and checking if retriever exists: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb326e860>, checking llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 18:24:14,714 - INFO - logging the rag chain: first={
  context: VectorStoreRetriever(tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'], vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb326e860>),
  question: RunnablePassthrough()
} middle=[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))]), VLLMOpenAI(client=<openai.resources.completions.Completions object at 0x7fecb33ad480>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7fecb2e7c6a0>, model_name='meta-llama/Llama-2-7b-chat-hf', max_tokens=1084, model_kwargs={'stop': ['.']}, openai_api_key='EMPTY', openai_api_base='http://localhost:8500/v1', openai_proxy='')] last=StrOutputParser() and retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb326e860>
2024-06-19 18:24:17,052 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 18:24:17,054 - INFO - Checking the response of do rag:  According to the retrieved context, tools are:

* Intermediate results: The ability to access the results of intermediate steps in a chain, even before the final output is produced
2024-06-19 18:24:17,056 - INFO - Checking the response of do rag:  According to the retrieved context, tools are:

* Intermediate results: The ability to access the results of intermediate steps in a chain, even before the final output is produced
2024-06-19 18:24:51,794 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 18:24:51,812 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 18:24:51,871 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 18:24:51,919 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7fecb2e7e1d0>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 18:24:51,919 - INFO - Checking the embedder : sentence-transformers/all-MiniLM-L6-v2 and the current llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 18:24:51,934 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 18:24:51,950 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 18:24:52,008 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 18:24:52,026 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 18:24:52,713 - INFO - collection is admin_test_class_8, and client: <weaviate.client.WeaviateClient object at 0x7fecb326f340>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 18:24:52,715 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 18:24:52,717 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 18:24:52,717 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb326c100>
2024-06-19 18:24:52,719 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb326c100>
2024-06-19 18:24:52,719 - INFO - Checkpoint RAG
2024-06-19 18:24:52,719 - INFO - Checking prompt before RAG: input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))] and checking if retriever exists: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb326c100>, checking llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 18:24:52,720 - INFO - logging the rag chain: first={
  context: VectorStoreRetriever(tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'], vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb326c100>),
  question: RunnablePassthrough()
} middle=[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))]), VLLMOpenAI(client=<openai.resources.completions.Completions object at 0x7fecb2e7f100>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7fecb33ac9d0>, model_name='meta-llama/Llama-2-7b-chat-hf', max_tokens=1084, model_kwargs={'stop': ['.']}, openai_api_key='EMPTY', openai_api_base='http://localhost:8500/v1', openai_proxy='')] last=StrOutputParser() and retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fecb326c100>
2024-06-19 18:24:54,116 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 18:24:54,118 - INFO - Checking the response of do rag:  According to the retrieved context, the main components of tools in LangChain are:

1
2024-06-19 18:24:54,120 - INFO - Checking the response of do rag:  According to the retrieved context, the main components of tools in LangChain are:

1
2024-06-19 18:25:43,395 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 18:25:43,411 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 18:25:43,472 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 18:25:43,491 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 18:25:43,492 - INFO - info on query passed: the main component of tools? and the class name: <weaviate.Collection config={
  "name": "Admin_test_class_8",
  "description": null,
  "generative_config": null,
  "inverted_index_config": {
    "bm25": {
      "b": 0.75,
      "k1": 1.2
    },
    "cleanup_interval_seconds": 60,
    "index_null_state": false,
    "index_property_length": false,
    "index_timestamps": false,
    "stopwords": {
      "preset": "en",
      "additions": null,
      "removals": null
    }
  },
  "multi_tenancy_config": {
    "enabled": false,
    "auto_tenant_creation": false,
    "auto_tenant_activation": false
  },
  "properties": [
    {
      "name": "document_title",
      "description": null,
      "data_type": "text",
      "index_filterable": true,
      "index_searchable": true,
      "nested_properties": null,
      "tokenization": "word",
      "vectorizer_config": {
        "skip": false,
        "vectorize_property_name": true
      },
      "vectorizer": "text2vec-huggingface"
    },
    {
      "name": "page_content",
      "description": null,
      "data_type": "text",
      "index_filterable": true,
      "index_searchable": true,
      "nested_properties": null,
      "tokenization": "word",
      "vectorizer_config": {
        "skip": false,
        "vectorize_property_name": true
      },
      "vectorizer": "text2vec-huggingface"
    }
  ],
  "references": [],
  "replication_config": {
    "factor": 1
  },
  "reranker_config": null,
  "sharding_config": {
    "virtual_per_physical": 128,
    "desired_count": 1,
    "actual_count": 1,
    "desired_virtual_count": 128,
    "actual_virtual_count": 128,
    "key": "_id",
    "strategy": "hash",
    "function": "murmur3"
  },
  "vector_index_config": {
    "quantizer": null,
    "cleanup_interval_seconds": 300,
    "distance_metric": "cosine",
    "dynamic_ef_min": 100,
    "dynamic_ef_max": 500,
    "dynamic_ef_factor": 8,
    "ef": -1,
    "ef_construction": 128,
    "flat_search_cutoff": 40000,
    "max_connections": 64,
    "skip": false,
    "vector_cache_max_objects": 1000000000000
  },
  "vector_index_type": "hnsw",
  "vectorizer_config": {
    "vectorizer": "text2vec-huggingface",
    "model": {
      "model": "sentence-transformers/all-MiniLM-L6-v2"
    },
    "vectorize_collection_name": true
  },
  "vectorizer": "text2vec-huggingface",
  "vector_config": null
}>
2024-06-19 18:25:43,622 - INFO - logged properties: {'page_content': 'ChatModelSingle string, list of chat messages or a PromptValueChatMessage\nLLM Single string, list of chat messages or a PromptValueString\nOutputParserThe output of an LLM or ChatModel Depends on the parser\nRetrieverSingle string List of Documents\nTool Single string or dictionary, depending on the toolDepends on the tool\nAll runnables expose input and output schemas to inspect the inputs and outputs:\ninput_schema: an input Pydantic model auto-generated from the structure of the Runnable\noutput_schema: an output Pydantic model auto-generated from the structure of the Runnable\nComponents\nLangChain provides standard, extendable interfaces and external integrations for various components useful for building with', 'document_title': 'langchain_guide.pdf'}
2024-06-19 18:25:43,623 - INFO - logged distance: 0.6133894920349121
2024-06-19 18:25:43,623 - INFO - logged properties: {'page_content': 'that input location)\nHumanMessage\nThis represents a message from the user.\nAIMessage\nThis represents a message from the model. In addition to the content property, these messages also have:\nresponse_metadata\nThe response_metadata property contains additional metadata about the response. The data here is often speciï¬c to each model\nprovider. This is where information like log-probs and token usage may be stored.\ntool_calls\nThese represent a decision from an language model to call a tool. They are included as part of an AIMessage output. They can be\naccessed from there with the .tool_calls property.\nThis property returns a list of dictionaries. Each dictionary has the following keys:\nname: The name of the tool that should be called.\nargs: The arguments to that tool.', 'document_title': 'langchain_guide.pdf'}
2024-06-19 18:25:43,623 - INFO - logged distance: 0.6500457525253296
2024-06-19 18:25:43,623 - INFO - Here is the response after similarity search request: QueryReturn(objects=[Object(uuid=_WeaviateUUIDInt('fdaee071-3866-457f-96c6-04fe4f3d3db7'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=0.6133894920349121, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'page_content': 'ChatModelSingle string, list of chat messages or a PromptValueChatMessage\nLLM Single string, list of chat messages or a PromptValueString\nOutputParserThe output of an LLM or ChatModel Depends on the parser\nRetrieverSingle string List of Documents\nTool Single string or dictionary, depending on the toolDepends on the tool\nAll runnables expose input and output schemas to inspect the inputs and outputs:\ninput_schema: an input Pydantic model auto-generated from the structure of the Runnable\noutput_schema: an output Pydantic model auto-generated from the structure of the Runnable\nComponents\nLangChain provides standard, extendable interfaces and external integrations for various components useful for building with', 'document_title': 'langchain_guide.pdf'}, references=None, vector={}, collection='Admin_test_class_8'), Object(uuid=_WeaviateUUIDInt('2fb60251-1215-4167-a730-44d42a6db582'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=0.6500457525253296, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'page_content': 'that input location)\nHumanMessage\nThis represents a message from the user.\nAIMessage\nThis represents a message from the model. In addition to the content property, these messages also have:\nresponse_metadata\nThe response_metadata property contains additional metadata about the response. The data here is often speciï¬c to each model\nprovider. This is where information like log-probs and token usage may be stored.\ntool_calls\nThese represent a decision from an language model to call a tool. They are included as part of an AIMessage output. They can be\naccessed from there with the .tool_calls property.\nThis property returns a list of dictionaries. Each dictionary has the following keys:\nname: The name of the tool that should be called.\nargs: The arguments to that tool.', 'document_title': 'langchain_guide.pdf'}, references=None, vector={}, collection='Admin_test_class_8')])
2024-06-19 18:27:23,143 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 18:27:23,159 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 18:27:23,220 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 18:27:23,244 - INFO - Here is the response after basic search request: QueryReturn(objects=[Object(uuid=_WeaviateUUIDInt('047c2f80-a8b1-4eea-bddd-bd550d13cb8a'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'page_content': 'parser. If that output parser\nerrors, then this will pass\nthe error message and the\nbad output to an LLM and\nask it to ï¬x the output.\nRetryWithError âstr|\nMessageWraps another output\nparser. If that output parser\nerrors, then this will pass\nthe original inputs, the bad\noutput, and the error\nmessage to an LLM and ask\nit to ï¬x it. Compared to\nOutputFixingParser, this\none also sends the original\ninstructions.\nPydantic âstr|\nMessagepydantic.BaseModelTakes a user deï¬ned\nPydantic model and returns\ndata in that format.\nYAML âstr|\nMessagepydantic.BaseModelTakes a user deï¬ned\nPydantic model and returns\ndata in that format. Uses\nYAML to encode it.\nPandasDataFrame âstr|\nMessagedictUseful for doing operations\nwith pandas DataFrames.\nEnum âstr|\nMessageEnumParses response into one of', 'document_title': 'langchain_guide.pdf'}, references=None, vector={}, collection='Admin_test_class_8'), Object(uuid=_WeaviateUUIDInt('05709948-1fda-4dc9-a3ab-629e68e3cc18'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'page_content': 'interface includes:\nstream: stream back chunks of the response\ninvoke: call the chain on an input\nbatch: call the chain on a list of inputs\nThese also have corresponding async methods that should be used with asyncioawait syntax for concurrency:\nastream: stream back chunks of the response async\nainvoke: call the chain on an input async\nabatch: call the chain on a list of inputs async\nastream_log: stream back intermediate steps as they happen, in addition to the ï¬nal response\nastream_events: beta stream events as they happen in the chain (introduced in langchain-core 0.1.14)\nThe input type and output type varies by component:\nComponent Input Type Output Type\nPromptDictionary PromptValue\nChatModelSingle string, list of chat messages or a PromptValueChatMessage', 'document_title': 'langchain_guide.pdf'}, references=None, vector={}, collection='Admin_test_class_8')])
2024-06-19 19:44:52,337 - INFO - At initilization stage embedding model is: None and vllm model is: None
2024-06-19 19:45:31,407 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7fefc40e41c0>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 19:45:31,423 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 19:45:31,439 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 19:45:31,499 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 19:45:31,515 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 19:45:32,908 - INFO - collection is admin_test_class_8, and client: <weaviate.client.WeaviateClient object at 0x7fefc51ea980>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 19:45:32,910 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 19:45:32,912 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 19:45:32,913 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fefb42f2ec0>
2024-06-19 19:45:32,913 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fefb42f2ec0>
2024-06-19 19:45:35,114 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 19:45:35,126 - INFO - Checking the hypo documents: 

            I apologize, but I cannot provide a full answer to this question as the term "langchain" does not exist in any standard documentation or resource that I have access to
2024-06-19 19:45:35,535 - INFO - Logging the documents in hyde: [Document(page_content='exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximum observability and\ndebuggability.\nSeamless LangServe deployment Any chain created with LCEL can be easily deployed using LangServe.\nRunnable interface\nTo make it as easy as possible to create custom chains, we\'ve implemented a "Runnable" protocol. Many LangChain components\nimplement the Runnable protocol, including chat models, LLMs, output parsers, retrievers, prompt templates, and more. There\nare also several useful primitives for working with runnables, which you can read about below.\nThis is a standard interface, which makes it easy to deï¬ne custom chains as well as invoke them in a standard way. The standard\ninterface includes:\nstream: stream back chunks of the response', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content="langchain-openai, langchain-anthropic, etc). This was done in order to improve support for these important integrations.\nlangchain\nThe main langchain package contains chains, agents, and retrieval strategies that make up an application's cognitive\narchitecture. These are NOT third party integrations. All chains, agents, and retrieval strategies here are NOT speciï¬c to any one\nintegration, but rather generic across all integrations.\nlangchain-community\nThis package contains third party integrations that are maintained by the LangChain community. Key partner packages are\nseparated out (see below). This contains all integrations for various components (LLMs, vector stores, retrievers). All\ndependencies in this package are optional to keep the package as lightweight as possible.\nlanggraph", metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='LangChain 0.2 is out! Leave feedback on the v0.2 docs here. You can view the v0.1 docs here.\nConceptual guide\nConceptual guide\nThis section contains introductions to key parts of LangChain.\nArchitecture\nLangChain as a framework consists of a number of packages.\nlangchain-core\nThis package contains base abstractions of different components and ways to compose them together. The interfaces for core\ncomponents like LLMs, vector stores, retrievers and more are deï¬ned here. No third party integrations are deï¬ned here. The\ndependencies are kept purposefully very lightweight.\nPartner packages\nWhile the long tail of integrations are in langchain-community, we split popular integrations into their own packages (e.g.', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='reliability without any latency cost.\nAccess intermediate results For more complex chains itâs often very useful to access the results of intermediate steps even\nbefore the ï¬nal output is produced. This can be used to let end-users know something is happening, or even just to debug your\nchain. You can stream intermediate results, and itâs available on every LangServe server.\nInput and output schemas Input and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from\nthe structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.\nSeamless LangSmith tracing As your chains get more and more complex, it becomes increasingly important to understand what', metadata={'document_title': 'langchain_guide.pdf'})]
2024-06-19 19:45:35,551 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 19:45:37,194 - INFO - logging the response of hyde: 
            Answer: Based on the document provided, LangChain provides a framework for building and deploying custom cognitive architectures
2024-06-19 19:45:37,194 - INFO - CHecking after hyde the response: 
            Answer: Based on the document provided, LangChain provides a framework for building and deploying custom cognitive architectures
2024-06-19 19:47:23,561 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7fef9c84aa10>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 19:47:23,577 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 19:47:23,601 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 19:47:23,660 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 19:47:23,678 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 19:47:24,372 - INFO - collection is admin_test_class_8, and client: <weaviate.client.WeaviateClient object at 0x7fefc40e41c0>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 19:47:24,374 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 19:47:24,376 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 19:47:24,377 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fefb413f250>
2024-06-19 19:47:24,377 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fefb413f250>
2024-06-19 19:47:26,545 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 19:47:26,547 - INFO - Checking the hypo documents: 
        Langchain Agent: 
            A langchain agent is an AI-powered tool that enables users to create, manage, and optimize their content across various platforms and languages
2024-06-19 19:47:26,581 - INFO - Logging the documents in hyde: [Document(page_content="langchain-openai, langchain-anthropic, etc). This was done in order to improve support for these important integrations.\nlangchain\nThe main langchain package contains chains, agents, and retrieval strategies that make up an application's cognitive\narchitecture. These are NOT third party integrations. All chains, agents, and retrieval strategies here are NOT speciï¬c to any one\nintegration, but rather generic across all integrations.\nlangchain-community\nThis package contains third party integrations that are maintained by the LangChain community. Key partner packages are\nseparated out (see below). This contains all integrations for various components (LLMs, vector stores, retrievers). All\ndependencies in this package are optional to keep the package as lightweight as possible.\nlanggraph", metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='LLMs. Some components LangChain implements, some components we rely on third-party integrations for, and others are a mix.\nChat models\nLanguage models that use a sequence of messages as inputs and return chat messages as outputs (as opposed to using plain\ntext). These are traditionally newer models (older models are generally LLMs, see below). Chat models support the assignment of\ndistinct roles to conversation messages, helping to distinguish messages from the AI, users, and instructions such as system\nmessages.\nAlthough the underlying models are messages in, message out, the LangChain wrappers also allow these models to take a string\nas input. This means you can easily use chat models in place of LLMs.', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='langgraph\nlanggraph is an extension of langchain aimed at building robust and stateful multi-actor applications with LLMs by modeling\nsteps as edges and nodes in a graph.\nLangGraph exposes high level interfaces for creating common types of agents, as well as a low-level API for composing custom\nï¬ows.\nlangserve\nA package to deploy LangChain chains as REST APIs. Makes it easy to get a production ready API up and running.\nLangSmith\nA developer platform that lets you debug, test, evaluate, and monitor LLM applications.\nLangChain Expression Language \x00LCEL\x00\nLangChain Expression Language, or LCEL, is a declarative way to chain LangChain components. LCEL was designed from day 1 to', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='LangChain 0.2 is out! Leave feedback on the v0.2 docs here. You can view the v0.1 docs here.\nConceptual guide\nConceptual guide\nThis section contains introductions to key parts of LangChain.\nArchitecture\nLangChain as a framework consists of a number of packages.\nlangchain-core\nThis package contains base abstractions of different components and ways to compose them together. The interfaces for core\ncomponents like LLMs, vector stores, retrievers and more are deï¬ned here. No third party integrations are deï¬ned here. The\ndependencies are kept purposefully very lightweight.\nPartner packages\nWhile the long tail of integrations are in langchain-community, we split popular integrations into their own packages (e.g.', metadata={'document_title': 'langchain_guide.pdf'})]
2024-06-19 19:47:26,595 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 19:47:29,254 - INFO - logging the response of hyde: 
            Answer:
            A LangChain agent is an extension of LangChain aimed at building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph
2024-06-19 19:47:29,255 - INFO - CHecking after hyde the response: 
            Answer:
            A LangChain agent is an extension of LangChain aimed at building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph
2024-06-19 19:48:04,628 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7fef9c8ba6b0>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 19:48:04,644 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 19:48:04,659 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 19:48:04,730 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 19:48:04,748 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 19:48:05,908 - INFO - collection is admin_test_class_8, and client: <weaviate.client.WeaviateClient object at 0x7fef9c84aa10>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 19:48:05,910 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 19:48:05,912 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 19:48:05,912 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fef9c8e3a90>
2024-06-19 19:48:05,913 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fef9c8e3a90>
2024-06-19 19:48:08,409 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 19:48:08,411 - INFO - Checking the hypo documents: 

Langchain Agent: 

A Langchain agent is a decentralized, blockchain-based agent that is capable of autonomously navigating and executing tasks within a complex network of interconnected systems
2024-06-19 19:48:08,447 - INFO - Logging the documents in hyde: [Document(page_content='reliability without any latency cost.\nAccess intermediate results For more complex chains itâs often very useful to access the results of intermediate steps even\nbefore the ï¬nal output is produced. This can be used to let end-users know something is happening, or even just to debug your\nchain. You can stream intermediate results, and itâs available on every LangServe server.\nInput and output schemas Input and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from\nthe structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.\nSeamless LangSmith tracing As your chains get more and more complex, it becomes increasingly important to understand what', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content="langchain-openai, langchain-anthropic, etc). This was done in order to improve support for these important integrations.\nlangchain\nThe main langchain package contains chains, agents, and retrieval strategies that make up an application's cognitive\narchitecture. These are NOT third party integrations. All chains, agents, and retrieval strategies here are NOT speciï¬c to any one\nintegration, but rather generic across all integrations.\nlangchain-community\nThis package contains third party integrations that are maintained by the LangChain community. Key partner packages are\nseparated out (see below). This contains all integrations for various components (LLMs, vector stores, retrievers). All\ndependencies in this package are optional to keep the package as lightweight as possible.\nlanggraph", metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='langgraph\nlanggraph is an extension of langchain aimed at building robust and stateful multi-actor applications with LLMs by modeling\nsteps as edges and nodes in a graph.\nLangGraph exposes high level interfaces for creating common types of agents, as well as a low-level API for composing custom\nï¬ows.\nlangserve\nA package to deploy LangChain chains as REST APIs. Makes it easy to get a production ready API up and running.\nLangSmith\nA developer platform that lets you debug, test, evaluate, and monitor LLM applications.\nLangChain Expression Language \x00LCEL\x00\nLangChain Expression Language, or LCEL, is a declarative way to chain LangChain components. LCEL was designed from day 1 to', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='LangChain 0.2 is out! Leave feedback on the v0.2 docs here. You can view the v0.1 docs here.\nConceptual guide\nConceptual guide\nThis section contains introductions to key parts of LangChain.\nArchitecture\nLangChain as a framework consists of a number of packages.\nlangchain-core\nThis package contains base abstractions of different components and ways to compose them together. The interfaces for core\ncomponents like LLMs, vector stores, retrievers and more are deï¬ned here. No third party integrations are deï¬ned here. The\ndependencies are kept purposefully very lightweight.\nPartner packages\nWhile the long tail of integrations are in langchain-community, we split popular integrations into their own packages (e.g.', metadata={'document_title': 'langchain_guide.pdf'})]
2024-06-19 19:48:08,460 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 19:48:10,276 - INFO - logging the response of hyde: 
            Answer: 
            A langchain agent is an entity that performs a specific task or set of tasks within a LangChain application
2024-06-19 19:48:10,276 - INFO - CHecking after hyde the response: 
            Answer: 
            A langchain agent is an entity that performs a specific task or set of tasks within a LangChain application
2024-06-19 19:48:29,344 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7fefc40a0400>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 19:48:29,361 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 19:48:29,377 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 19:48:29,434 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 19:48:29,451 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 19:48:30,188 - INFO - collection is admin_test_class_8, and client: <weaviate.client.WeaviateClient object at 0x7fefc4a0e3e0>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 19:48:30,190 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 19:48:30,192 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 19:48:30,192 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fef9c8e3af0>
2024-06-19 19:48:30,193 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fef9c8e3af0>
2024-06-19 19:48:32,145 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 19:48:32,147 - INFO - Checking the hypo documents: 

            {'response': 'A langchain tool is a composite tool that integrates various components to facilitate the process of automating the production of natural language text
2024-06-19 19:48:32,177 - INFO - Logging the documents in hyde: [Document(page_content='ChatModelSingle string, list of chat messages or a PromptValueChatMessage\nLLM Single string, list of chat messages or a PromptValueString\nOutputParserThe output of an LLM or ChatModel Depends on the parser\nRetrieverSingle string List of Documents\nTool Single string or dictionary, depending on the toolDepends on the tool\nAll runnables expose input and output schemas to inspect the inputs and outputs:\ninput_schema: an input Pydantic model auto-generated from the structure of the Runnable\noutput_schema: an output Pydantic model auto-generated from the structure of the Runnable\nComponents\nLangChain provides standard, extendable interfaces and external integrations for various components useful for building with', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='langgraph\nlanggraph is an extension of langchain aimed at building robust and stateful multi-actor applications with LLMs by modeling\nsteps as edges and nodes in a graph.\nLangGraph exposes high level interfaces for creating common types of agents, as well as a low-level API for composing custom\nï¬ows.\nlangserve\nA package to deploy LangChain chains as REST APIs. Makes it easy to get a production ready API up and running.\nLangSmith\nA developer platform that lets you debug, test, evaluate, and monitor LLM applications.\nLangChain Expression Language \x00LCEL\x00\nLangChain Expression Language, or LCEL, is a declarative way to chain LangChain components. LCEL was designed from day 1 to', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='Output parsers\nNOTE\nThe information here refers to parsers that take a text output from a model try to parse it into a more structured\nrepresentation. More and more models are supporting function (or tool) calling, which handles this automatically. It is\nrecommended to use function/tool calling rather than output parsing. See documentation for that here.\nResponsible for taking the output of a model and transforming it to a more suitable format for downstream tasks. Useful when\nyou are using LLMs to generate structured data, or to normalize output from chat models and LLMs.\nLangChain has lots of different types of output parsers. This is a list of output parsers LangChain supports. The table below has\nvarious pieces of information:\nName: The name of the output parser', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='LLMs. Some components LangChain implements, some components we rely on third-party integrations for, and others are a mix.\nChat models\nLanguage models that use a sequence of messages as inputs and return chat messages as outputs (as opposed to using plain\ntext). These are traditionally newer models (older models are generally LLMs, see below). Chat models support the assignment of\ndistinct roles to conversation messages, helping to distinguish messages from the AI, users, and instructions such as system\nmessages.\nAlthough the underlying models are messages in, message out, the LangChain wrappers also allow these models to take a string\nas input. This means you can easily use chat models in place of LLMs.', metadata={'document_title': 'langchain_guide.pdf'})]
2024-06-19 19:48:32,190 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 19:48:33,776 - INFO - logging the response of hyde: 
            Answer:
            Based on the provided context, the components of a LangChain tool are:

1
2024-06-19 19:48:33,776 - INFO - CHecking after hyde the response: 
            Answer:
            Based on the provided context, the components of a LangChain tool are:

1
2024-06-19 19:49:37,622 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7fefc40a1e10>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 19:49:37,638 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 19:49:37,657 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 19:49:37,715 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 19:49:37,735 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 19:49:38,487 - INFO - collection is admin_test_class_8, and client: <weaviate.client.WeaviateClient object at 0x7fefc40a0400>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 19:49:38,489 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 19:49:38,491 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 19:49:38,491 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fef98e5ff10>
2024-06-19 19:49:38,491 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fef98e5ff10>
2024-06-19 19:49:40,057 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 19:49:40,059 - INFO - Checking the hypo documents: 

Langchain is a blockchain-based platform that aims to revolutionize the way we think about data ownership and sharing
2024-06-19 19:49:40,086 - INFO - Logging the documents in hyde: [Document(page_content='langgraph\nlanggraph is an extension of langchain aimed at building robust and stateful multi-actor applications with LLMs by modeling\nsteps as edges and nodes in a graph.\nLangGraph exposes high level interfaces for creating common types of agents, as well as a low-level API for composing custom\nï¬ows.\nlangserve\nA package to deploy LangChain chains as REST APIs. Makes it easy to get a production ready API up and running.\nLangSmith\nA developer platform that lets you debug, test, evaluate, and monitor LLM applications.\nLangChain Expression Language \x00LCEL\x00\nLangChain Expression Language, or LCEL, is a declarative way to chain LangChain components. LCEL was designed from day 1 to', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximum observability and\ndebuggability.\nSeamless LangServe deployment Any chain created with LCEL can be easily deployed using LangServe.\nRunnable interface\nTo make it as easy as possible to create custom chains, we\'ve implemented a "Runnable" protocol. Many LangChain components\nimplement the Runnable protocol, including chat models, LLMs, output parsers, retrievers, prompt templates, and more. There\nare also several useful primitives for working with runnables, which you can read about below.\nThis is a standard interface, which makes it easy to deï¬ne custom chains as well as invoke them in a standard way. The standard\ninterface includes:\nstream: stream back chunks of the response', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='LangChain 0.2 is out! Leave feedback on the v0.2 docs here. You can view the v0.1 docs here.\nConceptual guide\nConceptual guide\nThis section contains introductions to key parts of LangChain.\nArchitecture\nLangChain as a framework consists of a number of packages.\nlangchain-core\nThis package contains base abstractions of different components and ways to compose them together. The interfaces for core\ncomponents like LLMs, vector stores, retrievers and more are deï¬ned here. No third party integrations are deï¬ned here. The\ndependencies are kept purposefully very lightweight.\nPartner packages\nWhile the long tail of integrations are in langchain-community, we split popular integrations into their own packages (e.g.', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='window of past messages directly.\nThe concept of ChatHistory refers to a class in LangChain which can be used to wrap an arbitrary chain. This ChatHistory will\nkeep track of inputs and outputs of the underlying chain, and append them as messages to a message database. Future\ninteractions will then load those messages and pass them into the chain as part of the input.\nDocuments\nA Document object in LangChain contains information about some data. It has two attributes:\npage_content: str: The content of this document. Currently is only a string.\nmetadata: dict: Arbitrary metadata associated with this document. Can track the document id, ï¬le name, etc.\nDocument loaders', metadata={'document_title': 'langchain_guide.pdf'})]
2024-06-19 19:49:40,099 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 19:49:42,768 - INFO - logging the response of hyde: 
            Answer: 
LangChain is an extension of the langchain aimed at building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph
2024-06-19 19:49:42,768 - INFO - CHecking after hyde the response: 
            Answer: 
LangChain is an extension of the langchain aimed at building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph
2024-06-19 19:51:14,327 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7fef98e5e7a0>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 19:51:14,343 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 19:51:14,360 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 19:51:14,422 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 19:51:14,441 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 19:51:15,144 - INFO - collection is admin_test_class_8, and client: <weaviate.client.WeaviateClient object at 0x7fefc40a1e10>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 19:51:15,146 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 19:51:15,148 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 19:51:15,148 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fef98e5f160>
2024-06-19 19:51:15,148 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fef98e5f160>
2024-06-19 19:51:16,768 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 19:51:16,770 - INFO - Checking the hypo documents: 

            If you are unable to provide a full answer, please provide a brief explanation as to why you are unable to answer the question
2024-06-19 19:51:16,798 - INFO - Logging the documents in hyde: [Document(page_content='various pieces of information:\nName: The name of the output parser\nSupports Streaming: Whether the output parser supports streaming.\nHas Format Instructions: Whether the output parser has format instructions. This is generally available except when (a) the\ndesired schema is not speciï¬ed in the prompt but rather in other parameters (like OpenAI function calling), or (b) when the\nOutputParser wraps another OutputParser.\nCalls LLM: Whether this output parser itself calls an LLM. This is usually only done by output parsers that attempt to correct\nmisformatted output.\nInput Type: Expected input type. Most output parsers work on both strings and messages, but some (like OpenAI Functions) need\na message with speciï¬c kwargs.\nOutput Type: The output type of the object returned by the parser.', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='interface includes:\nstream: stream back chunks of the response\ninvoke: call the chain on an input\nbatch: call the chain on a list of inputs\nThese also have corresponding async methods that should be used with asyncioawait syntax for concurrency:\nastream: stream back chunks of the response async\nainvoke: call the chain on an input async\nabatch: call the chain on a list of inputs async\nastream_log: stream back intermediate steps as they happen, in addition to the ï¬nal response\nastream_events: beta stream events as they happen in the chain (introduced in langchain-core 0.1.14)\nThe input type and output type varies by component:\nComponent Input Type Output Type\nPromptDictionary PromptValue\nChatModelSingle string, list of chat messages or a PromptValueChatMessage', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='support putting prototypes in production, with no code changes, from the simplest âprompt + LLMâ chain to the most complex\nchains (weâve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you\nmight want to use LCEL:\nFirst-class streaming support When you build your chains with LCEL you get the best possible time-to-ï¬rst-token (time elapsed\nuntil the ï¬rst chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming\noutput parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw\ntokens.\nAsync support Any chain built with LCEL can be called both with the synchronous API (eg. in your Jupyter notebook while', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content="langchain-openai, langchain-anthropic, etc). This was done in order to improve support for these important integrations.\nlangchain\nThe main langchain package contains chains, agents, and retrieval strategies that make up an application's cognitive\narchitecture. These are NOT third party integrations. All chains, agents, and retrieval strategies here are NOT speciï¬c to any one\nintegration, but rather generic across all integrations.\nlangchain-community\nThis package contains third party integrations that are maintained by the LangChain community. Key partner packages are\nseparated out (see below). This contains all integrations for various components (LLMs, vector stores, retrievers). All\ndependencies in this package are optional to keep the package as lightweight as possible.\nlanggraph", metadata={'document_title': 'langchain_guide.pdf'})]
2024-06-19 19:51:16,811 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 19:51:18,854 - INFO - logging the response of hyde: 
            Answer: A document loader is a component in LangChain that loads a document from a file or buffer and returns it as a `Document` object
2024-06-19 19:51:18,855 - INFO - CHecking after hyde the response: 
            Answer: A document loader is a component in LangChain that loads a document from a file or buffer and returns it as a `Document` object
2024-06-19 19:52:24,911 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7fefc40a0130>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 19:52:24,927 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 19:52:24,943 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 19:52:25,001 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 19:52:25,019 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 19:52:25,800 - INFO - collection is admin_test_class_8, and client: <weaviate.client.WeaviateClient object at 0x7fef98e56ef0>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 19:52:25,802 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 19:52:25,804 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 19:52:25,804 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fef98e5d4e0>
2024-06-19 19:52:25,804 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fef98e5d4e0>
2024-06-19 19:52:27,703 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 19:52:27,706 - INFO - Checking the hypo documents: 

    Parser: 
        A string output parser is a tool or function that takes in a string as input and outputs a parsed version of that string
2024-06-19 19:52:27,736 - INFO - Logging the documents in hyde: [Document(page_content='various pieces of information:\nName: The name of the output parser\nSupports Streaming: Whether the output parser supports streaming.\nHas Format Instructions: Whether the output parser has format instructions. This is generally available except when (a) the\ndesired schema is not speciï¬ed in the prompt but rather in other parameters (like OpenAI function calling), or (b) when the\nOutputParser wraps another OutputParser.\nCalls LLM: Whether this output parser itself calls an LLM. This is usually only done by output parsers that attempt to correct\nmisformatted output.\nInput Type: Expected input type. Most output parsers work on both strings and messages, but some (like OpenAI Functions) need\na message with speciï¬c kwargs.\nOutput Type: The output type of the object returned by the parser.', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='Output parsers\nNOTE\nThe information here refers to parsers that take a text output from a model try to parse it into a more structured\nrepresentation. More and more models are supporting function (or tool) calling, which handles this automatically. It is\nrecommended to use function/tool calling rather than output parsing. See documentation for that here.\nResponsible for taking the output of a model and transforming it to a more suitable format for downstream tasks. Useful when\nyou are using LLMs to generate structured data, or to normalize output from chat models and LLMs.\nLangChain has lots of different types of output parsers. This is a list of output parsers LangChain supports. The table below has\nvarious pieces of information:\nName: The name of the output parser', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='ChatModelSingle string, list of chat messages or a PromptValueChatMessage\nLLM Single string, list of chat messages or a PromptValueString\nOutputParserThe output of an LLM or ChatModel Depends on the parser\nRetrieverSingle string List of Documents\nTool Single string or dictionary, depending on the toolDepends on the tool\nAll runnables expose input and output schemas to inspect the inputs and outputs:\ninput_schema: an input Pydantic model auto-generated from the structure of the Runnable\noutput_schema: an output Pydantic model auto-generated from the structure of the Runnable\nComponents\nLangChain provides standard, extendable interfaces and external integrations for various components useful for building with', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content="Output Type: The output type of the object returned by the parser.\nDescription: Our commentary on this output parser and when to use it.\nNameSupports\nStreamingHas Format\nInstructionsCalls\nLLMInput\nTypeOutput Type Description\nJSON ââstr|\nMessageJSON objectReturns a JSON object as\nspeciï¬ed. You can specify a\nPydantic model and it will\nreturn JSON for that model.\nProbably the most reliable\noutput parser for getting\nstructured data that does\nNOT use function calling.\nXML ââstr|\nMessagedictReturns a dictionary of tags.\nUse when XML output is\nneeded. Use with models\nthat are good at writing\nXML (like Anthropic's).\nCSV ââstr|\nMessageList[str]Returns a list of comma\nseparated values.\nOutputFixing âstr|\nMessageWraps another output\nparser. If that output parser\nerrors, then this will pass", metadata={'document_title': 'langchain_guide.pdf'})]
2024-06-19 19:52:27,749 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 19:52:29,136 - INFO - logging the response of hyde: 
            Answer: A string output parser is an output parser that returns a single string as its output
2024-06-19 19:52:29,136 - INFO - CHecking after hyde the response: 
            Answer: A string output parser is an output parser that returns a single string as its output
2024-06-19 20:50:46,191 - INFO - request processed successfully username='admin' class_name='test_class_8' mode='do_rag_mq' vectorDB_type='Weaviate' file_path='/home/nilsb/LLM-Chat-Bot/API/received_files/4bc33558fbf83a36' file_title=None query='What is langchain?' model='meta-llama/Llama-2-7b-chat-hf' inference_endpoint='http://localhost:8500/v1' memory=False conversation_number=-1 embedder='sentence-transformers/all-MiniLM-L6-v2': %s
2024-06-19 20:50:46,191 - ERROR - An error occurred: local variable 'response' referenced before assignment
2024-06-19 20:51:41,283 - INFO - At initilization stage embedding model is: None and vllm model is: None
2024-06-19 20:51:46,080 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7fa2cc0991e0>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 20:51:46,097 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 20:51:46,113 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 20:51:46,172 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 20:51:46,189 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 20:51:47,062 - INFO - collection is admin_test_class_8, and client: <weaviate.client.WeaviateClient object at 0x7fa2ce97eb60>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 20:51:47,064 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 20:51:47,066 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 20:51:47,067 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fa2abc282e0>
2024-06-19 20:51:47,067 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fa2abc282e0>
2024-06-19 20:51:53,876 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 20:51:54,472 - INFO - Checking the generated queries: [Document(page_content='MessagesPlaceholder\nThis prompt template is responsible for adding a list of messages in a particular place. In the above ChatPromptTemplate, we saw\nhow we could format two messages, each one a string. But what if we wanted the user to pass in a list of messages that we would\nslot into a particular spot? This is how you use MessagesPlaceholder.\nAPI Reference:ChatPromptTemplate | MessagesPlaceholder | HumanMessage\nThis will produce a list of two messages, the ï¬rst one being a system message, and the second one being the HumanMessage we\npassed in. If we had passed in 5 messages, then it would have produced 6 messages in total (the system message plus the 5\npassed in). This is useful for letting a list of messages be slotted into a particular spot.', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='LLMs. Some components LangChain implements, some components we rely on third-party integrations for, and others are a mix.\nChat models\nLanguage models that use a sequence of messages as inputs and return chat messages as outputs (as opposed to using plain\ntext). These are traditionally newer models (older models are generally LLMs, see below). Chat models support the assignment of\ndistinct roles to conversation messages, helping to distinguish messages from the AI, users, and instructions such as system\nmessages.\nAlthough the underlying models are messages in, message out, the LangChain wrappers also allow these models to take a string\nas input. This means you can easily use chat models in place of LLMs.', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='calling. Please see the tool calling section for more information.\nFor speciï¬cs on how to use chat models, see the relevant how-to guides here.\nMultimodality\nSome chat models are multimodal, accepting images, audio and even video as inputs. These are still less common, meaning\nmodel providers haven\'t standardized on the "best" way to deï¬ne the API. Multimodal outputs are even less common. As such,\nwe\'ve kept our multimodal abstractions fairly light weight and plan to further solidify the multimodal APIs and interaction\npatterns as the ï¬eld matures.\nIn LangChain, most chat models that support multimodal inputs also accept those values in OpenAI\'s content blocks format. So', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content="langchain-openai, langchain-anthropic, etc). This was done in order to improve support for these important integrations.\nlangchain\nThe main langchain package contains chains, agents, and retrieval strategies that make up an application's cognitive\narchitecture. These are NOT third party integrations. All chains, agents, and retrieval strategies here are NOT speciï¬c to any one\nintegration, but rather generic across all integrations.\nlangchain-community\nThis package contains third party integrations that are maintained by the LangChain community. Key partner packages are\nseparated out (see below). This contains all integrations for various components (LLMs, vector stores, retrievers). All\ndependencies in this package are optional to keep the package as lightweight as possible.\nlanggraph", metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='langgraph\nlanggraph is an extension of langchain aimed at building robust and stateful multi-actor applications with LLMs by modeling\nsteps as edges and nodes in a graph.\nLangGraph exposes high level interfaces for creating common types of agents, as well as a low-level API for composing custom\nï¬ows.\nlangserve\nA package to deploy LangChain chains as REST APIs. Makes it easy to get a production ready API up and running.\nLangSmith\nA developer platform that lets you debug, test, evaluate, and monitor LLM applications.\nLangChain Expression Language \x00LCEL\x00\nLangChain Expression Language, or LCEL, is a declarative way to chain LangChain components. LCEL was designed from day 1 to', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='as input. This means you can easily use chat models in place of LLMs.\nWhen a string is passed in as input, it is converted to a HumanMessage and then passed to the underlying model.\nLangChain does not host any Chat Models, rather we rely on third party integrations.\nWe have some standardized parameters when constructing ChatModels:\nmodel: the name of the model\nChatModels also accept other parameters that are speciï¬c to that integration.\nINFO\nTool Calling Some chat models have been ï¬ne-tuned for tool calling and provide a dedicated API for tool calling. Generally,\nsuch models are better at tool calling than non-ï¬ne-tuned models, and are recommended for use cases that require tool\ncalling. Please see the tool calling section for more information.', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='window of past messages directly.\nThe concept of ChatHistory refers to a class in LangChain which can be used to wrap an arbitrary chain. This ChatHistory will\nkeep track of inputs and outputs of the underlying chain, and append them as messages to a message database. Future\ninteractions will then load those messages and pass them into the chain as part of the input.\nDocuments\nA Document object in LangChain contains information about some data. It has two attributes:\npage_content: str: The content of this document. Currently is only a string.\nmetadata: dict: Arbitrary metadata associated with this document. Can track the document id, ï¬le name, etc.\nDocument loaders', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='string under the hood before being passed to the underlying model.\nLangChain does not host any LLMs, rather we rely on third party integrations.\nFor speciï¬cs on how to use LLMs, see the relevant how-to guides here.\nMessages\nSome language models take a list of messages as input and return a message. There are a few different types of messages. All\nmessages have a role, content, and response_metadata property.\nThe role describes WHO is saying the message. LangChain has different message classes for different roles.\nThe content property describes the content of the message. This can be a few different things:\nA string (most models deal this type of content)\nA List of dictionaries (this is used for multimodal input, where the dictionary contains information about that input type and', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='ChatModelSingle string, list of chat messages or a PromptValueChatMessage\nLLM Single string, list of chat messages or a PromptValueString\nOutputParserThe output of an LLM or ChatModel Depends on the parser\nRetrieverSingle string List of Documents\nTool Single string or dictionary, depending on the toolDepends on the tool\nAll runnables expose input and output schemas to inspect the inputs and outputs:\ninput_schema: an input Pydantic model auto-generated from the structure of the Runnable\noutput_schema: an output Pydantic model auto-generated from the structure of the Runnable\nComponents\nLangChain provides standard, extendable interfaces and external integrations for various components useful for building with', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='reliability without any latency cost.\nAccess intermediate results For more complex chains itâs often very useful to access the results of intermediate steps even\nbefore the ï¬nal output is produced. This can be used to let end-users know something is happening, or even just to debug your\nchain. You can stream intermediate results, and itâs available on every LangServe server.\nInput and output schemas Input and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from\nthe structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.\nSeamless LangSmith tracing As your chains get more and more complex, it becomes increasingly important to understand what', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='An alternative way to accomplish the same thing without using the MessagesPlaceholder class explicitly is:\nFor speciï¬cs on how to use prompt templates, see the relevant how-to guides here.\nExample selectors\nOne common prompting technique for achieving better performance is to include examples as part of the prompt. This gives the\nlanguage model concrete examples of how it should behave. Sometimes these examples are hardcoded into the prompt, but for\nmore advanced situations it may be nice to dynamically select them. Example Selectors are classes responsible for selecting and\nthen formatting examples into prompts.\nFor speciï¬cs on how to use example selectors, see the relevant how-to guides here.\nOutput parsers\nNOTE', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='LangChain 0.2 is out! Leave feedback on the v0.2 docs here. You can view the v0.1 docs here.\nConceptual guide\nConceptual guide\nThis section contains introductions to key parts of LangChain.\nArchitecture\nLangChain as a framework consists of a number of packages.\nlangchain-core\nThis package contains base abstractions of different components and ways to compose them together. The interfaces for core\ncomponents like LLMs, vector stores, retrievers and more are deï¬ned here. No third party integrations are deï¬ned here. The\ndependencies are kept purposefully very lightweight.\nPartner packages\nWhile the long tail of integrations are in langchain-community, we split popular integrations into their own packages (e.g.', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='prototyping) as well as with the asynchronous API (eg. in a LangServe server). This enables using the same code for prototypes\nand in production, with great performance, and the ability to handle many concurrent requests in the same server.\nOptimized parallel execution Whenever your LCEL chains have steps that can be executed in parallel (eg if you fetch documents\nfrom multiple retrievers) we automatically do it, both in the sync and the async interfaces, for the smallest possible latency.\nRetries and fallbacks Conï¬gure retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains\nmore reliable at scale. Weâre currently working on adding streaming support for retries/fallbacks, so you can get the added\nreliability without any latency cost.', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content="args: The arguments to that tool.\nid: The id of that tool call.\nSystemMessage\nThis represents a system message, which tells the model how to behave. Not every model provider supports this.\nFunctionMessage\nThis represents the result of a function call. In addition to role and content, this message has a name parameter which conveys\nthe name of the function that was called to produce this result.\nToolMessage\nThis represents the result of a tool call. This is distinct from a FunctionMessage in order to match OpenAI's function and tool\nmessage types. In addition to role and content, this message has a tool_call_id parameter which conveys the id of the call to\nthe tool that was called to produce this result.\nPrompt templates", metadata={'document_title': 'langchain_guide.pdf'})]
2024-06-19 20:51:54,742 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 20:51:54,815 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 400 Bad Request"
2024-06-19 20:51:54,816 - ERROR - An error occurred: Error code: 400 - {'object': 'error', 'message': "This model's maximum context length is 2048 tokens. However, you requested 2532 tokens (1448 in the messages, 1084 in the completion). Please reduce the length of the messages or completion.", 'type': 'BadRequestError', 'param': None, 'code': 400}
2024-06-19 20:54:38,975 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7fa2cc098d90>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 20:54:38,992 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 20:54:39,008 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 20:54:39,067 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 20:54:39,087 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 20:54:40,132 - INFO - collection is admin_test_class_8, and client: <weaviate.client.WeaviateClient object at 0x7fa2cc0991e0>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 20:54:40,134 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 20:54:40,137 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 20:54:40,137 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fa2a22cb910>
2024-06-19 20:54:40,138 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fa2a22cb910>
2024-06-19 20:54:40,152 - INFO - Retrying request to /completions in 0.924466 seconds
2024-06-19 20:54:41,083 - INFO - Retrying request to /completions in 1.900964 seconds
2024-06-19 20:54:42,995 - ERROR - An error occurred: Connection error.
2024-06-19 20:54:55,415 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7fa2cc0987c0>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 20:54:55,432 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 20:54:55,451 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 20:54:55,529 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 20:54:55,551 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 20:54:56,424 - INFO - collection is admin_test_class_8, and client: <weaviate.client.WeaviateClient object at 0x7fa2cc098d90>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 20:54:56,426 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 20:54:56,429 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 20:54:56,429 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fa2ab0ff4f0>
2024-06-19 20:54:56,430 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fa2ab0ff4f0>
2024-06-19 20:54:56,455 - INFO - Retrying request to /completions in 0.817152 seconds
2024-06-19 20:54:57,278 - INFO - Retrying request to /completions in 1.918717 seconds
2024-06-19 20:55:05,870 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 20:55:06,162 - INFO - Checking the generated queries: [Document(page_content='MessagesPlaceholder\nThis prompt template is responsible for adding a list of messages in a particular place. In the above ChatPromptTemplate, we saw\nhow we could format two messages, each one a string. But what if we wanted the user to pass in a list of messages that we would\nslot into a particular spot? This is how you use MessagesPlaceholder.\nAPI Reference:ChatPromptTemplate | MessagesPlaceholder | HumanMessage\nThis will produce a list of two messages, the ï¬rst one being a system message, and the second one being the HumanMessage we\npassed in. If we had passed in 5 messages, then it would have produced 6 messages in total (the system message plus the 5\npassed in). This is useful for letting a list of messages be slotted into a particular spot.', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='LLMs. Some components LangChain implements, some components we rely on third-party integrations for, and others are a mix.\nChat models\nLanguage models that use a sequence of messages as inputs and return chat messages as outputs (as opposed to using plain\ntext). These are traditionally newer models (older models are generally LLMs, see below). Chat models support the assignment of\ndistinct roles to conversation messages, helping to distinguish messages from the AI, users, and instructions such as system\nmessages.\nAlthough the underlying models are messages in, message out, the LangChain wrappers also allow these models to take a string\nas input. This means you can easily use chat models in place of LLMs.', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content="langchain-openai, langchain-anthropic, etc). This was done in order to improve support for these important integrations.\nlangchain\nThe main langchain package contains chains, agents, and retrieval strategies that make up an application's cognitive\narchitecture. These are NOT third party integrations. All chains, agents, and retrieval strategies here are NOT speciï¬c to any one\nintegration, but rather generic across all integrations.\nlangchain-community\nThis package contains third party integrations that are maintained by the LangChain community. Key partner packages are\nseparated out (see below). This contains all integrations for various components (LLMs, vector stores, retrievers). All\ndependencies in this package are optional to keep the package as lightweight as possible.\nlanggraph", metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='langgraph\nlanggraph is an extension of langchain aimed at building robust and stateful multi-actor applications with LLMs by modeling\nsteps as edges and nodes in a graph.\nLangGraph exposes high level interfaces for creating common types of agents, as well as a low-level API for composing custom\nï¬ows.\nlangserve\nA package to deploy LangChain chains as REST APIs. Makes it easy to get a production ready API up and running.\nLangSmith\nA developer platform that lets you debug, test, evaluate, and monitor LLM applications.\nLangChain Expression Language \x00LCEL\x00\nLangChain Expression Language, or LCEL, is a declarative way to chain LangChain components. LCEL was designed from day 1 to', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='string under the hood before being passed to the underlying model.\nLangChain does not host any LLMs, rather we rely on third party integrations.\nFor speciï¬cs on how to use LLMs, see the relevant how-to guides here.\nMessages\nSome language models take a list of messages as input and return a message. There are a few different types of messages. All\nmessages have a role, content, and response_metadata property.\nThe role describes WHO is saying the message. LangChain has different message classes for different roles.\nThe content property describes the content of the message. This can be a few different things:\nA string (most models deal this type of content)\nA List of dictionaries (this is used for multimodal input, where the dictionary contains information about that input type and', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='far this is restricted to image inputs. For models like Gemini which support video and other bytes input, the APIs also support the\nnative, model-speciï¬c representations.\nFor speciï¬cs on how to use multimodal models, see the relevant how-to guides here.\nFor a full list of LangChain model providers with multimodal models, check out this table.\nLLMs\nLanguage models that takes a string as input and returns a string. These are traditionally older models (newer models generally\nare Chat Models, see below).\nAlthough the underlying models are string in, string out, the LangChain wrappers also allow these models to take messages as\ninput. This gives them the same interface as Chat Models. When messages are passed in as input, they will be formatted into a', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='reliability without any latency cost.\nAccess intermediate results For more complex chains itâs often very useful to access the results of intermediate steps even\nbefore the ï¬nal output is produced. This can be used to let end-users know something is happening, or even just to debug your\nchain. You can stream intermediate results, and itâs available on every LangServe server.\nInput and output schemas Input and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from\nthe structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.\nSeamless LangSmith tracing As your chains get more and more complex, it becomes increasingly important to understand what', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='An alternative way to accomplish the same thing without using the MessagesPlaceholder class explicitly is:\nFor speciï¬cs on how to use prompt templates, see the relevant how-to guides here.\nExample selectors\nOne common prompting technique for achieving better performance is to include examples as part of the prompt. This gives the\nlanguage model concrete examples of how it should behave. Sometimes these examples are hardcoded into the prompt, but for\nmore advanced situations it may be nice to dynamically select them. Example Selectors are classes responsible for selecting and\nthen formatting examples into prompts.\nFor speciï¬cs on how to use example selectors, see the relevant how-to guides here.\nOutput parsers\nNOTE', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='LangChain 0.2 is out! Leave feedback on the v0.2 docs here. You can view the v0.1 docs here.\nConceptual guide\nConceptual guide\nThis section contains introductions to key parts of LangChain.\nArchitecture\nLangChain as a framework consists of a number of packages.\nlangchain-core\nThis package contains base abstractions of different components and ways to compose them together. The interfaces for core\ncomponents like LLMs, vector stores, retrievers and more are deï¬ned here. No third party integrations are deï¬ned here. The\ndependencies are kept purposefully very lightweight.\nPartner packages\nWhile the long tail of integrations are in langchain-community, we split popular integrations into their own packages (e.g.', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='prototyping) as well as with the asynchronous API (eg. in a LangServe server). This enables using the same code for prototypes\nand in production, with great performance, and the ability to handle many concurrent requests in the same server.\nOptimized parallel execution Whenever your LCEL chains have steps that can be executed in parallel (eg if you fetch documents\nfrom multiple retrievers) we automatically do it, both in the sync and the async interfaces, for the smallest possible latency.\nRetries and fallbacks Conï¬gure retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains\nmore reliable at scale. Weâre currently working on adding streaming support for retries/fallbacks, so you can get the added\nreliability without any latency cost.', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content="args: The arguments to that tool.\nid: The id of that tool call.\nSystemMessage\nThis represents a system message, which tells the model how to behave. Not every model provider supports this.\nFunctionMessage\nThis represents the result of a function call. In addition to role and content, this message has a name parameter which conveys\nthe name of the function that was called to produce this result.\nToolMessage\nThis represents the result of a tool call. This is distinct from a FunctionMessage in order to match OpenAI's function and tool\nmessage types. In addition to role and content, this message has a tool_call_id parameter which conveys the id of the call to\nthe tool that was called to produce this result.\nPrompt templates", metadata={'document_title': 'langchain_guide.pdf'})]
2024-06-19 20:56:07,487 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 20:56:09,188 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 400 Bad Request"
2024-06-19 20:56:09,189 - ERROR - An error occurred: Error code: 400 - {'object': 'error', 'message': "This model's maximum context length is 3000 tokens. However, you requested 3974 tokens (2890 in the messages, 1084 in the completion). Please reduce the length of the messages or completion.", 'type': 'BadRequestError', 'param': None, 'code': 400}
2024-06-19 20:56:11,491 - INFO - At initilization stage embedding model is: None and vllm model is: None
2024-06-19 20:56:21,891 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7f5998b1d0f0>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 20:56:21,913 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 20:56:21,930 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 20:56:21,991 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 20:56:22,009 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 20:56:23,221 - INFO - collection is admin_test_class_8, and client: <weaviate.client.WeaviateClient object at 0x7f5999a0ea70>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 20:56:23,223 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 20:56:23,225 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 20:56:23,225 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f5990276e60>
2024-06-19 20:56:23,225 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f5990276e60>
2024-06-19 20:56:32,265 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 20:56:33,051 - INFO - Checking the generated queries: [Document(page_content="args: The arguments to that tool.\nid: The id of that tool call.\nSystemMessage\nThis represents a system message, which tells the model how to behave. Not every model provider supports this.\nFunctionMessage\nThis represents the result of a function call. In addition to role and content, this message has a name parameter which conveys\nthe name of the function that was called to produce this result.\nToolMessage\nThis represents the result of a tool call. This is distinct from a FunctionMessage in order to match OpenAI's function and tool\nmessage types. In addition to role and content, this message has a tool_call_id parameter which conveys the id of the call to\nthe tool that was called to produce this result.\nPrompt templates", metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='reliability without any latency cost.\nAccess intermediate results For more complex chains itâs often very useful to access the results of intermediate steps even\nbefore the ï¬nal output is produced. This can be used to let end-users know something is happening, or even just to debug your\nchain. You can stream intermediate results, and itâs available on every LangServe server.\nInput and output schemas Input and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from\nthe structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.\nSeamless LangSmith tracing As your chains get more and more complex, it becomes increasingly important to understand what', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='far this is restricted to image inputs. For models like Gemini which support video and other bytes input, the APIs also support the\nnative, model-speciï¬c representations.\nFor speciï¬cs on how to use multimodal models, see the relevant how-to guides here.\nFor a full list of LangChain model providers with multimodal models, check out this table.\nLLMs\nLanguage models that takes a string as input and returns a string. These are traditionally older models (newer models generally\nare Chat Models, see below).\nAlthough the underlying models are string in, string out, the LangChain wrappers also allow these models to take messages as\ninput. This gives them the same interface as Chat Models. When messages are passed in as input, they will be formatted into a', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='window of past messages directly.\nThe concept of ChatHistory refers to a class in LangChain which can be used to wrap an arbitrary chain. This ChatHistory will\nkeep track of inputs and outputs of the underlying chain, and append them as messages to a message database. Future\ninteractions will then load those messages and pass them into the chain as part of the input.\nDocuments\nA Document object in LangChain contains information about some data. It has two attributes:\npage_content: str: The content of this document. Currently is only a string.\nmetadata: dict: Arbitrary metadata associated with this document. Can track the document id, ï¬le name, etc.\nDocument loaders', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='ChatModelSingle string, list of chat messages or a PromptValueChatMessage\nLLM Single string, list of chat messages or a PromptValueString\nOutputParserThe output of an LLM or ChatModel Depends on the parser\nRetrieverSingle string List of Documents\nTool Single string or dictionary, depending on the toolDepends on the tool\nAll runnables expose input and output schemas to inspect the inputs and outputs:\ninput_schema: an input Pydantic model auto-generated from the structure of the Runnable\noutput_schema: an output Pydantic model auto-generated from the structure of the Runnable\nComponents\nLangChain provides standard, extendable interfaces and external integrations for various components useful for building with', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='string under the hood before being passed to the underlying model.\nLangChain does not host any LLMs, rather we rely on third party integrations.\nFor speciï¬cs on how to use LLMs, see the relevant how-to guides here.\nMessages\nSome language models take a list of messages as input and return a message. There are a few different types of messages. All\nmessages have a role, content, and response_metadata property.\nThe role describes WHO is saying the message. LangChain has different message classes for different roles.\nThe content property describes the content of the message. This can be a few different things:\nA string (most models deal this type of content)\nA List of dictionaries (this is used for multimodal input, where the dictionary contains information about that input type and', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content="the tool that was called to produce this result.\nPrompt templates\nPrompt templates help to translate user input and parameters into instructions for a language model. This can be used to guide a\nmodel's response, helping it understand the context and generate relevant and coherent language-based output.\nPrompt Templates take as input a dictionary, where each key represents a variable in the prompt template to ï¬ll in.\nPrompt Templates output a PromptValue. This PromptValue can be passed to an LLM or a ChatModel, and can also be cast to a\nstring or a list of messages. The reason this PromptValue exists is to make it easy to switch between strings and messages.\nThere are a few different types of prompt templates:\nString PromptTemplates", metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='LangChain 0.2 is out! Leave feedback on the v0.2 docs here. You can view the v0.1 docs here.\nConceptual guide\nConceptual guide\nThis section contains introductions to key parts of LangChain.\nArchitecture\nLangChain as a framework consists of a number of packages.\nlangchain-core\nThis package contains base abstractions of different components and ways to compose them together. The interfaces for core\ncomponents like LLMs, vector stores, retrievers and more are deï¬ned here. No third party integrations are deï¬ned here. The\ndependencies are kept purposefully very lightweight.\nPartner packages\nWhile the long tail of integrations are in langchain-community, we split popular integrations into their own packages (e.g.', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content="Document loaders\nThese classes load Document objects. LangChain has hundreds of integrations with various data sources to load data from: Slack,\nNotion, Google Drive, etc.\nEach DocumentLoader has its own speciï¬c parameters, but they can all be invoked in the same way with the .load method. An\nexample use case is as follows:\nAPI Reference:CSVLoader\nFor speciï¬cs on how to use document loaders, see the relevant how-to guides here.\nText spli\ue009ers\nOnce you've loaded documents, you'll often want to transform them to better suit your application. The simplest example is you\nmay want to split a long document into smaller chunks that can ï¬t into your model's context window. LangChain has a number of", metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='langgraph\nlanggraph is an extension of langchain aimed at building robust and stateful multi-actor applications with LLMs by modeling\nsteps as edges and nodes in a graph.\nLangGraph exposes high level interfaces for creating common types of agents, as well as a low-level API for composing custom\nï¬ows.\nlangserve\nA package to deploy LangChain chains as REST APIs. Makes it easy to get a production ready API up and running.\nLangSmith\nA developer platform that lets you debug, test, evaluate, and monitor LLM applications.\nLangChain Expression Language \x00LCEL\x00\nLangChain Expression Language, or LCEL, is a declarative way to chain LangChain components. LCEL was designed from day 1 to', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='MessagesPlaceholder\nThis prompt template is responsible for adding a list of messages in a particular place. In the above ChatPromptTemplate, we saw\nhow we could format two messages, each one a string. But what if we wanted the user to pass in a list of messages that we would\nslot into a particular spot? This is how you use MessagesPlaceholder.\nAPI Reference:ChatPromptTemplate | MessagesPlaceholder | HumanMessage\nThis will produce a list of two messages, the ï¬rst one being a system message, and the second one being the HumanMessage we\npassed in. If we had passed in 5 messages, then it would have produced 6 messages in total (the system message plus the 5\npassed in). This is useful for letting a list of messages be slotted into a particular spot.', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content="langchain-openai, langchain-anthropic, etc). This was done in order to improve support for these important integrations.\nlangchain\nThe main langchain package contains chains, agents, and retrieval strategies that make up an application's cognitive\narchitecture. These are NOT third party integrations. All chains, agents, and retrieval strategies here are NOT speciï¬c to any one\nintegration, but rather generic across all integrations.\nlangchain-community\nThis package contains third party integrations that are maintained by the LangChain community. Key partner packages are\nseparated out (see below). This contains all integrations for various components (LLMs, vector stores, retrievers). All\ndependencies in this package are optional to keep the package as lightweight as possible.\nlanggraph", metadata={'document_title': 'langchain_guide.pdf'})]
2024-06-19 20:56:40,134 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 20:56:40,343 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 400 Bad Request"
2024-06-19 20:56:40,344 - ERROR - An error occurred: Error code: 400 - {'object': 'error', 'message': "This model's maximum context length is 3000 tokens. However, you requested 3462 tokens (2378 in the messages, 1084 in the completion). Please reduce the length of the messages or completion.", 'type': 'BadRequestError', 'param': None, 'code': 400}
2024-06-19 20:57:02,955 - INFO - At initilization stage embedding model is: None and vllm model is: None
2024-06-19 20:57:09,219 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7fe95c47d0c0>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 20:57:09,236 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 20:57:09,252 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 20:57:09,313 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 20:57:09,329 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 20:57:10,132 - INFO - collection is admin_test_class_8, and client: <weaviate.client.WeaviateClient object at 0x7fe95cb82a40>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 20:57:10,134 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 20:57:10,136 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 20:57:10,137 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fe954620130>
2024-06-19 20:57:10,137 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fe954620130>
2024-06-19 20:57:14,507 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 20:57:15,078 - INFO - Checking the generated queries: [Document(page_content='that input location)\nHumanMessage\nThis represents a message from the user.\nAIMessage\nThis represents a message from the model. In addition to the content property, these messages also have:\nresponse_metadata\nThe response_metadata property contains additional metadata about the response. The data here is often speciï¬c to each model\nprovider. This is where information like log-probs and token usage may be stored.\ntool_calls\nThese represent a decision from an language model to call a tool. They are included as part of an AIMessage output. They can be\naccessed from there with the .tool_calls property.\nThis property returns a list of dictionaries. Each dictionary has the following keys:\nname: The name of the tool that should be called.\nargs: The arguments to that tool.', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='langgraph\nlanggraph is an extension of langchain aimed at building robust and stateful multi-actor applications with LLMs by modeling\nsteps as edges and nodes in a graph.\nLangGraph exposes high level interfaces for creating common types of agents, as well as a low-level API for composing custom\nï¬ows.\nlangserve\nA package to deploy LangChain chains as REST APIs. Makes it easy to get a production ready API up and running.\nLangSmith\nA developer platform that lets you debug, test, evaluate, and monitor LLM applications.\nLangChain Expression Language \x00LCEL\x00\nLangChain Expression Language, or LCEL, is a declarative way to chain LangChain components. LCEL was designed from day 1 to', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='calling. Please see the tool calling section for more information.\nFor speciï¬cs on how to use chat models, see the relevant how-to guides here.\nMultimodality\nSome chat models are multimodal, accepting images, audio and even video as inputs. These are still less common, meaning\nmodel providers haven\'t standardized on the "best" way to deï¬ne the API. Multimodal outputs are even less common. As such,\nwe\'ve kept our multimodal abstractions fairly light weight and plan to further solidify the multimodal APIs and interaction\npatterns as the ï¬eld matures.\nIn LangChain, most chat models that support multimodal inputs also accept those values in OpenAI\'s content blocks format. So', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='reliability without any latency cost.\nAccess intermediate results For more complex chains itâs often very useful to access the results of intermediate steps even\nbefore the ï¬nal output is produced. This can be used to let end-users know something is happening, or even just to debug your\nchain. You can stream intermediate results, and itâs available on every LangServe server.\nInput and output schemas Input and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from\nthe structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.\nSeamless LangSmith tracing As your chains get more and more complex, it becomes increasingly important to understand what', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='LangChain 0.2 is out! Leave feedback on the v0.2 docs here. You can view the v0.1 docs here.\nConceptual guide\nConceptual guide\nThis section contains introductions to key parts of LangChain.\nArchitecture\nLangChain as a framework consists of a number of packages.\nlangchain-core\nThis package contains base abstractions of different components and ways to compose them together. The interfaces for core\ncomponents like LLMs, vector stores, retrievers and more are deï¬ned here. No third party integrations are deï¬ned here. The\ndependencies are kept purposefully very lightweight.\nPartner packages\nWhile the long tail of integrations are in langchain-community, we split popular integrations into their own packages (e.g.', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='far this is restricted to image inputs. For models like Gemini which support video and other bytes input, the APIs also support the\nnative, model-speciï¬c representations.\nFor speciï¬cs on how to use multimodal models, see the relevant how-to guides here.\nFor a full list of LangChain model providers with multimodal models, check out this table.\nLLMs\nLanguage models that takes a string as input and returns a string. These are traditionally older models (newer models generally\nare Chat Models, see below).\nAlthough the underlying models are string in, string out, the LangChain wrappers also allow these models to take messages as\ninput. This gives them the same interface as Chat Models. When messages are passed in as input, they will be formatted into a', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='ChatModelSingle string, list of chat messages or a PromptValueChatMessage\nLLM Single string, list of chat messages or a PromptValueString\nOutputParserThe output of an LLM or ChatModel Depends on the parser\nRetrieverSingle string List of Documents\nTool Single string or dictionary, depending on the toolDepends on the tool\nAll runnables expose input and output schemas to inspect the inputs and outputs:\ninput_schema: an input Pydantic model auto-generated from the structure of the Runnable\noutput_schema: an output Pydantic model auto-generated from the structure of the Runnable\nComponents\nLangChain provides standard, extendable interfaces and external integrations for various components useful for building with', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content="args: The arguments to that tool.\nid: The id of that tool call.\nSystemMessage\nThis represents a system message, which tells the model how to behave. Not every model provider supports this.\nFunctionMessage\nThis represents the result of a function call. In addition to role and content, this message has a name parameter which conveys\nthe name of the function that was called to produce this result.\nToolMessage\nThis represents the result of a tool call. This is distinct from a FunctionMessage in order to match OpenAI's function and tool\nmessage types. In addition to role and content, this message has a tool_call_id parameter which conveys the id of the call to\nthe tool that was called to produce this result.\nPrompt templates", metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content="langchain-openai, langchain-anthropic, etc). This was done in order to improve support for these important integrations.\nlangchain\nThe main langchain package contains chains, agents, and retrieval strategies that make up an application's cognitive\narchitecture. These are NOT third party integrations. All chains, agents, and retrieval strategies here are NOT speciï¬c to any one\nintegration, but rather generic across all integrations.\nlangchain-community\nThis package contains third party integrations that are maintained by the LangChain community. Key partner packages are\nseparated out (see below). This contains all integrations for various components (LLMs, vector stores, retrievers). All\ndependencies in this package are optional to keep the package as lightweight as possible.\nlanggraph", metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='MessagesPlaceholder\nThis prompt template is responsible for adding a list of messages in a particular place. In the above ChatPromptTemplate, we saw\nhow we could format two messages, each one a string. But what if we wanted the user to pass in a list of messages that we would\nslot into a particular spot? This is how you use MessagesPlaceholder.\nAPI Reference:ChatPromptTemplate | MessagesPlaceholder | HumanMessage\nThis will produce a list of two messages, the ï¬rst one being a system message, and the second one being the HumanMessage we\npassed in. If we had passed in 5 messages, then it would have produced 6 messages in total (the system message plus the 5\npassed in). This is useful for letting a list of messages be slotted into a particular spot.', metadata={'document_title': 'langchain_guide.pdf'})]
2024-06-19 20:57:17,787 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 20:57:19,934 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 20:57:19,936 - INFO - Checking the response after multi queries: 
        Answer: Langchain is a framework for building and deploying conversational AI models, including chatbots and language translation models
2024-06-19 20:57:19,936 - INFO - Checking the response after MQ RAG: 
        Answer: Langchain is a framework for building and deploying conversational AI models, including chatbots and language translation models
2024-06-19 20:57:47,148 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7fe95c47cca0>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 20:57:47,163 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 20:57:47,180 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 20:57:47,240 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 20:57:47,257 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 20:57:47,941 - INFO - collection is admin_test_class_8, and client: <weaviate.client.WeaviateClient object at 0x7fe95c47d0c0>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 20:57:47,943 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 20:57:47,944 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 20:57:47,945 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fe9544f73d0>
2024-06-19 20:57:47,945 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fe9544f73d0>
2024-06-19 20:57:50,632 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 20:57:50,739 - INFO - Checking the generated queries: [Document(page_content='langgraph\nlanggraph is an extension of langchain aimed at building robust and stateful multi-actor applications with LLMs by modeling\nsteps as edges and nodes in a graph.\nLangGraph exposes high level interfaces for creating common types of agents, as well as a low-level API for composing custom\nï¬ows.\nlangserve\nA package to deploy LangChain chains as REST APIs. Makes it easy to get a production ready API up and running.\nLangSmith\nA developer platform that lets you debug, test, evaluate, and monitor LLM applications.\nLangChain Expression Language \x00LCEL\x00\nLangChain Expression Language, or LCEL, is a declarative way to chain LangChain components. LCEL was designed from day 1 to', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='reliability without any latency cost.\nAccess intermediate results For more complex chains itâs often very useful to access the results of intermediate steps even\nbefore the ï¬nal output is produced. This can be used to let end-users know something is happening, or even just to debug your\nchain. You can stream intermediate results, and itâs available on every LangServe server.\nInput and output schemas Input and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from\nthe structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.\nSeamless LangSmith tracing As your chains get more and more complex, it becomes increasingly important to understand what', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='LangChain 0.2 is out! Leave feedback on the v0.2 docs here. You can view the v0.1 docs here.\nConceptual guide\nConceptual guide\nThis section contains introductions to key parts of LangChain.\nArchitecture\nLangChain as a framework consists of a number of packages.\nlangchain-core\nThis package contains base abstractions of different components and ways to compose them together. The interfaces for core\ncomponents like LLMs, vector stores, retrievers and more are deï¬ned here. No third party integrations are deï¬ned here. The\ndependencies are kept purposefully very lightweight.\nPartner packages\nWhile the long tail of integrations are in langchain-community, we split popular integrations into their own packages (e.g.', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='far this is restricted to image inputs. For models like Gemini which support video and other bytes input, the APIs also support the\nnative, model-speciï¬c representations.\nFor speciï¬cs on how to use multimodal models, see the relevant how-to guides here.\nFor a full list of LangChain model providers with multimodal models, check out this table.\nLLMs\nLanguage models that takes a string as input and returns a string. These are traditionally older models (newer models generally\nare Chat Models, see below).\nAlthough the underlying models are string in, string out, the LangChain wrappers also allow these models to take messages as\ninput. This gives them the same interface as Chat Models. When messages are passed in as input, they will be formatted into a', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='Output parsers\nNOTE\nThe information here refers to parsers that take a text output from a model try to parse it into a more structured\nrepresentation. More and more models are supporting function (or tool) calling, which handles this automatically. It is\nrecommended to use function/tool calling rather than output parsing. See documentation for that here.\nResponsible for taking the output of a model and transforming it to a more suitable format for downstream tasks. Useful when\nyou are using LLMs to generate structured data, or to normalize output from chat models and LLMs.\nLangChain has lots of different types of output parsers. This is a list of output parsers LangChain supports. The table below has\nvarious pieces of information:\nName: The name of the output parser', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content="args: The arguments to that tool.\nid: The id of that tool call.\nSystemMessage\nThis represents a system message, which tells the model how to behave. Not every model provider supports this.\nFunctionMessage\nThis represents the result of a function call. In addition to role and content, this message has a name parameter which conveys\nthe name of the function that was called to produce this result.\nToolMessage\nThis represents the result of a tool call. This is distinct from a FunctionMessage in order to match OpenAI's function and tool\nmessage types. In addition to role and content, this message has a tool_call_id parameter which conveys the id of the call to\nthe tool that was called to produce this result.\nPrompt templates", metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content="langchain-openai, langchain-anthropic, etc). This was done in order to improve support for these important integrations.\nlangchain\nThe main langchain package contains chains, agents, and retrieval strategies that make up an application's cognitive\narchitecture. These are NOT third party integrations. All chains, agents, and retrieval strategies here are NOT speciï¬c to any one\nintegration, but rather generic across all integrations.\nlangchain-community\nThis package contains third party integrations that are maintained by the LangChain community. Key partner packages are\nseparated out (see below). This contains all integrations for various components (LLMs, vector stores, retrievers). All\ndependencies in this package are optional to keep the package as lightweight as possible.\nlanggraph", metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='MessagesPlaceholder\nThis prompt template is responsible for adding a list of messages in a particular place. In the above ChatPromptTemplate, we saw\nhow we could format two messages, each one a string. But what if we wanted the user to pass in a list of messages that we would\nslot into a particular spot? This is how you use MessagesPlaceholder.\nAPI Reference:ChatPromptTemplate | MessagesPlaceholder | HumanMessage\nThis will produce a list of two messages, the ï¬rst one being a system message, and the second one being the HumanMessage we\npassed in. If we had passed in 5 messages, then it would have produced 6 messages in total (the system message plus the 5\npassed in). This is useful for letting a list of messages be slotted into a particular spot.', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='LLMs. Some components LangChain implements, some components we rely on third-party integrations for, and others are a mix.\nChat models\nLanguage models that use a sequence of messages as inputs and return chat messages as outputs (as opposed to using plain\ntext). These are traditionally newer models (older models are generally LLMs, see below). Chat models support the assignment of\ndistinct roles to conversation messages, helping to distinguish messages from the AI, users, and instructions such as system\nmessages.\nAlthough the underlying models are messages in, message out, the LangChain wrappers also allow these models to take a string\nas input. This means you can easily use chat models in place of LLMs.', metadata={'document_title': 'langchain_guide.pdf'})]
2024-06-19 20:57:55,330 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 20:57:55,507 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 400 Bad Request"
2024-06-19 20:57:55,508 - ERROR - An error occurred: Error code: 400 - {'object': 'error', 'message': "This model's maximum context length is 3000 tokens. However, you requested 3103 tokens (2019 in the messages, 1084 in the completion). Please reduce the length of the messages or completion.", 'type': 'BadRequestError', 'param': None, 'code': 400}
2024-06-19 20:57:59,930 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7fe95c47c820>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 20:57:59,946 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 20:57:59,962 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 20:58:00,028 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 20:58:00,044 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 20:58:00,779 - INFO - collection is admin_test_class_8, and client: <weaviate.client.WeaviateClient object at 0x7fe95c47cca0>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 20:58:00,781 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 20:58:00,783 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 20:58:00,784 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fe9303a51e0>
2024-06-19 20:58:00,784 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fe9303a51e0>
2024-06-19 20:58:06,011 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 20:58:06,169 - INFO - Checking the generated queries: [Document(page_content='window of past messages directly.\nThe concept of ChatHistory refers to a class in LangChain which can be used to wrap an arbitrary chain. This ChatHistory will\nkeep track of inputs and outputs of the underlying chain, and append them as messages to a message database. Future\ninteractions will then load those messages and pass them into the chain as part of the input.\nDocuments\nA Document object in LangChain contains information about some data. It has two attributes:\npage_content: str: The content of this document. Currently is only a string.\nmetadata: dict: Arbitrary metadata associated with this document. Can track the document id, ï¬le name, etc.\nDocument loaders', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='langgraph\nlanggraph is an extension of langchain aimed at building robust and stateful multi-actor applications with LLMs by modeling\nsteps as edges and nodes in a graph.\nLangGraph exposes high level interfaces for creating common types of agents, as well as a low-level API for composing custom\nï¬ows.\nlangserve\nA package to deploy LangChain chains as REST APIs. Makes it easy to get a production ready API up and running.\nLangSmith\nA developer platform that lets you debug, test, evaluate, and monitor LLM applications.\nLangChain Expression Language \x00LCEL\x00\nLangChain Expression Language, or LCEL, is a declarative way to chain LangChain components. LCEL was designed from day 1 to', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='as input. This means you can easily use chat models in place of LLMs.\nWhen a string is passed in as input, it is converted to a HumanMessage and then passed to the underlying model.\nLangChain does not host any Chat Models, rather we rely on third party integrations.\nWe have some standardized parameters when constructing ChatModels:\nmodel: the name of the model\nChatModels also accept other parameters that are speciï¬c to that integration.\nINFO\nTool Calling Some chat models have been ï¬ne-tuned for tool calling and provide a dedicated API for tool calling. Generally,\nsuch models are better at tool calling than non-ï¬ne-tuned models, and are recommended for use cases that require tool\ncalling. Please see the tool calling section for more information.', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='prototyping) as well as with the asynchronous API (eg. in a LangServe server). This enables using the same code for prototypes\nand in production, with great performance, and the ability to handle many concurrent requests in the same server.\nOptimized parallel execution Whenever your LCEL chains have steps that can be executed in parallel (eg if you fetch documents\nfrom multiple retrievers) we automatically do it, both in the sync and the async interfaces, for the smallest possible latency.\nRetries and fallbacks Conï¬gure retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains\nmore reliable at scale. Weâre currently working on adding streaming support for retries/fallbacks, so you can get the added\nreliability without any latency cost.', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='reliability without any latency cost.\nAccess intermediate results For more complex chains itâs often very useful to access the results of intermediate steps even\nbefore the ï¬nal output is produced. This can be used to let end-users know something is happening, or even just to debug your\nchain. You can stream intermediate results, and itâs available on every LangServe server.\nInput and output schemas Input and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from\nthe structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.\nSeamless LangSmith tracing As your chains get more and more complex, it becomes increasingly important to understand what', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content="the tool that was called to produce this result.\nPrompt templates\nPrompt templates help to translate user input and parameters into instructions for a language model. This can be used to guide a\nmodel's response, helping it understand the context and generate relevant and coherent language-based output.\nPrompt Templates take as input a dictionary, where each key represents a variable in the prompt template to ï¬ll in.\nPrompt Templates output a PromptValue. This PromptValue can be passed to an LLM or a ChatModel, and can also be cast to a\nstring or a list of messages. The reason this PromptValue exists is to make it easy to switch between strings and messages.\nThere are a few different types of prompt templates:\nString PromptTemplates", metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='LangChain 0.2 is out! Leave feedback on the v0.2 docs here. You can view the v0.1 docs here.\nConceptual guide\nConceptual guide\nThis section contains introductions to key parts of LangChain.\nArchitecture\nLangChain as a framework consists of a number of packages.\nlangchain-core\nThis package contains base abstractions of different components and ways to compose them together. The interfaces for core\ncomponents like LLMs, vector stores, retrievers and more are deï¬ned here. No third party integrations are deï¬ned here. The\ndependencies are kept purposefully very lightweight.\nPartner packages\nWhile the long tail of integrations are in langchain-community, we split popular integrations into their own packages (e.g.', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='ChatModelSingle string, list of chat messages or a PromptValueChatMessage\nLLM Single string, list of chat messages or a PromptValueString\nOutputParserThe output of an LLM or ChatModel Depends on the parser\nRetrieverSingle string List of Documents\nTool Single string or dictionary, depending on the toolDepends on the tool\nAll runnables expose input and output schemas to inspect the inputs and outputs:\ninput_schema: an input Pydantic model auto-generated from the structure of the Runnable\noutput_schema: an output Pydantic model auto-generated from the structure of the Runnable\nComponents\nLangChain provides standard, extendable interfaces and external integrations for various components useful for building with', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content="args: The arguments to that tool.\nid: The id of that tool call.\nSystemMessage\nThis represents a system message, which tells the model how to behave. Not every model provider supports this.\nFunctionMessage\nThis represents the result of a function call. In addition to role and content, this message has a name parameter which conveys\nthe name of the function that was called to produce this result.\nToolMessage\nThis represents the result of a tool call. This is distinct from a FunctionMessage in order to match OpenAI's function and tool\nmessage types. In addition to role and content, this message has a tool_call_id parameter which conveys the id of the call to\nthe tool that was called to produce this result.\nPrompt templates", metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content="langchain-openai, langchain-anthropic, etc). This was done in order to improve support for these important integrations.\nlangchain\nThe main langchain package contains chains, agents, and retrieval strategies that make up an application's cognitive\narchitecture. These are NOT third party integrations. All chains, agents, and retrieval strategies here are NOT speciï¬c to any one\nintegration, but rather generic across all integrations.\nlangchain-community\nThis package contains third party integrations that are maintained by the LangChain community. Key partner packages are\nseparated out (see below). This contains all integrations for various components (LLMs, vector stores, retrievers). All\ndependencies in this package are optional to keep the package as lightweight as possible.\nlanggraph", metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='MessagesPlaceholder\nThis prompt template is responsible for adding a list of messages in a particular place. In the above ChatPromptTemplate, we saw\nhow we could format two messages, each one a string. But what if we wanted the user to pass in a list of messages that we would\nslot into a particular spot? This is how you use MessagesPlaceholder.\nAPI Reference:ChatPromptTemplate | MessagesPlaceholder | HumanMessage\nThis will produce a list of two messages, the ï¬rst one being a system message, and the second one being the HumanMessage we\npassed in. If we had passed in 5 messages, then it would have produced 6 messages in total (the system message plus the 5\npassed in). This is useful for letting a list of messages be slotted into a particular spot.', metadata={'document_title': 'langchain_guide.pdf'})]
2024-06-19 20:58:08,240 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 20:58:09,729 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 20:58:09,731 - INFO - Checking the response after multi queries: 
        Answer: Langchain has the following features:

1
2024-06-19 20:58:09,732 - INFO - Checking the response after MQ RAG: 
        Answer: Langchain has the following features:

1
2024-06-19 20:58:33,081 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7fe95c47c520>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 20:58:33,096 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 20:58:33,115 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 20:58:33,178 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 20:58:33,194 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 20:58:33,861 - INFO - collection is admin_test_class_8, and client: <weaviate.client.WeaviateClient object at 0x7fe95c47c820>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 20:58:33,863 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 20:58:33,865 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 20:58:33,865 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fe954612590>
2024-06-19 20:58:33,865 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fe954612590>
2024-06-19 20:58:34,395 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 20:58:34,452 - INFO - Checking the generated queries: [Document(page_content='langgraph\nlanggraph is an extension of langchain aimed at building robust and stateful multi-actor applications with LLMs by modeling\nsteps as edges and nodes in a graph.\nLangGraph exposes high level interfaces for creating common types of agents, as well as a low-level API for composing custom\nï¬ows.\nlangserve\nA package to deploy LangChain chains as REST APIs. Makes it easy to get a production ready API up and running.\nLangSmith\nA developer platform that lets you debug, test, evaluate, and monitor LLM applications.\nLangChain Expression Language \x00LCEL\x00\nLangChain Expression Language, or LCEL, is a declarative way to chain LangChain components. LCEL was designed from day 1 to', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='interface includes:\nstream: stream back chunks of the response\ninvoke: call the chain on an input\nbatch: call the chain on a list of inputs\nThese also have corresponding async methods that should be used with asyncioawait syntax for concurrency:\nastream: stream back chunks of the response async\nainvoke: call the chain on an input async\nabatch: call the chain on a list of inputs async\nastream_log: stream back intermediate steps as they happen, in addition to the ï¬nal response\nastream_events: beta stream events as they happen in the chain (introduced in langchain-core 0.1.14)\nThe input type and output type varies by component:\nComponent Input Type Output Type\nPromptDictionary PromptValue\nChatModelSingle string, list of chat messages or a PromptValueChatMessage', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content="the tool that was called to produce this result.\nPrompt templates\nPrompt templates help to translate user input and parameters into instructions for a language model. This can be used to guide a\nmodel's response, helping it understand the context and generate relevant and coherent language-based output.\nPrompt Templates take as input a dictionary, where each key represents a variable in the prompt template to ï¬ll in.\nPrompt Templates output a PromptValue. This PromptValue can be passed to an LLM or a ChatModel, and can also be cast to a\nstring or a list of messages. The reason this PromptValue exists is to make it easy to switch between strings and messages.\nThere are a few different types of prompt templates:\nString PromptTemplates", metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='LangChain 0.2 is out! Leave feedback on the v0.2 docs here. You can view the v0.1 docs here.\nConceptual guide\nConceptual guide\nThis section contains introductions to key parts of LangChain.\nArchitecture\nLangChain as a framework consists of a number of packages.\nlangchain-core\nThis package contains base abstractions of different components and ways to compose them together. The interfaces for core\ncomponents like LLMs, vector stores, retrievers and more are deï¬ned here. No third party integrations are deï¬ned here. The\ndependencies are kept purposefully very lightweight.\nPartner packages\nWhile the long tail of integrations are in langchain-community, we split popular integrations into their own packages (e.g.', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='An alternative way to accomplish the same thing without using the MessagesPlaceholder class explicitly is:\nFor speciï¬cs on how to use prompt templates, see the relevant how-to guides here.\nExample selectors\nOne common prompting technique for achieving better performance is to include examples as part of the prompt. This gives the\nlanguage model concrete examples of how it should behave. Sometimes these examples are hardcoded into the prompt, but for\nmore advanced situations it may be nice to dynamically select them. Example Selectors are classes responsible for selecting and\nthen formatting examples into prompts.\nFor speciï¬cs on how to use example selectors, see the relevant how-to guides here.\nOutput parsers\nNOTE', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='ChatModelSingle string, list of chat messages or a PromptValueChatMessage\nLLM Single string, list of chat messages or a PromptValueString\nOutputParserThe output of an LLM or ChatModel Depends on the parser\nRetrieverSingle string List of Documents\nTool Single string or dictionary, depending on the toolDepends on the tool\nAll runnables expose input and output schemas to inspect the inputs and outputs:\ninput_schema: an input Pydantic model auto-generated from the structure of the Runnable\noutput_schema: an output Pydantic model auto-generated from the structure of the Runnable\nComponents\nLangChain provides standard, extendable interfaces and external integrations for various components useful for building with', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content="args: The arguments to that tool.\nid: The id of that tool call.\nSystemMessage\nThis represents a system message, which tells the model how to behave. Not every model provider supports this.\nFunctionMessage\nThis represents the result of a function call. In addition to role and content, this message has a name parameter which conveys\nthe name of the function that was called to produce this result.\nToolMessage\nThis represents the result of a tool call. This is distinct from a FunctionMessage in order to match OpenAI's function and tool\nmessage types. In addition to role and content, this message has a tool_call_id parameter which conveys the id of the call to\nthe tool that was called to produce this result.\nPrompt templates", metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content="langchain-openai, langchain-anthropic, etc). This was done in order to improve support for these important integrations.\nlangchain\nThe main langchain package contains chains, agents, and retrieval strategies that make up an application's cognitive\narchitecture. These are NOT third party integrations. All chains, agents, and retrieval strategies here are NOT speciï¬c to any one\nintegration, but rather generic across all integrations.\nlangchain-community\nThis package contains third party integrations that are maintained by the LangChain community. Key partner packages are\nseparated out (see below). This contains all integrations for various components (LLMs, vector stores, retrievers). All\ndependencies in this package are optional to keep the package as lightweight as possible.\nlanggraph", metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='MessagesPlaceholder\nThis prompt template is responsible for adding a list of messages in a particular place. In the above ChatPromptTemplate, we saw\nhow we could format two messages, each one a string. But what if we wanted the user to pass in a list of messages that we would\nslot into a particular spot? This is how you use MessagesPlaceholder.\nAPI Reference:ChatPromptTemplate | MessagesPlaceholder | HumanMessage\nThis will produce a list of two messages, the ï¬rst one being a system message, and the second one being the HumanMessage we\npassed in. If we had passed in 5 messages, then it would have produced 6 messages in total (the system message plus the 5\npassed in). This is useful for letting a list of messages be slotted into a particular spot.', metadata={'document_title': 'langchain_guide.pdf'})]
2024-06-19 20:58:38,497 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 20:58:38,671 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 400 Bad Request"
2024-06-19 20:58:38,672 - ERROR - An error occurred: Error code: 400 - {'object': 'error', 'message': "This model's maximum context length is 3000 tokens. However, you requested 3574 tokens (2490 in the messages, 1084 in the completion). Please reduce the length of the messages or completion.", 'type': 'BadRequestError', 'param': None, 'code': 400}
2024-06-19 20:59:00,961 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7fe95c47cd00>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 20:59:00,976 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 20:59:00,993 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 20:59:01,050 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 20:59:01,068 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 20:59:01,762 - INFO - collection is admin_test_class_8, and client: <weaviate.client.WeaviateClient object at 0x7fe95c47c520>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 20:59:01,764 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 20:59:01,766 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 20:59:01,766 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fe9544f70d0>
2024-06-19 20:59:01,767 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fe9544f70d0>
2024-06-19 20:59:02,421 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 20:59:02,466 - INFO - Checking the generated queries: [Document(page_content='as input. This means you can easily use chat models in place of LLMs.\nWhen a string is passed in as input, it is converted to a HumanMessage and then passed to the underlying model.\nLangChain does not host any Chat Models, rather we rely on third party integrations.\nWe have some standardized parameters when constructing ChatModels:\nmodel: the name of the model\nChatModels also accept other parameters that are speciï¬c to that integration.\nINFO\nTool Calling Some chat models have been ï¬ne-tuned for tool calling and provide a dedicated API for tool calling. Generally,\nsuch models are better at tool calling than non-ï¬ne-tuned models, and are recommended for use cases that require tool\ncalling. Please see the tool calling section for more information.', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='langgraph\nlanggraph is an extension of langchain aimed at building robust and stateful multi-actor applications with LLMs by modeling\nsteps as edges and nodes in a graph.\nLangGraph exposes high level interfaces for creating common types of agents, as well as a low-level API for composing custom\nï¬ows.\nlangserve\nA package to deploy LangChain chains as REST APIs. Makes it easy to get a production ready API up and running.\nLangSmith\nA developer platform that lets you debug, test, evaluate, and monitor LLM applications.\nLangChain Expression Language \x00LCEL\x00\nLangChain Expression Language, or LCEL, is a declarative way to chain LangChain components. LCEL was designed from day 1 to', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='There are a few different types of prompt templates:\nString PromptTemplates\nThese prompt templates are used to format a single string, and generally are used for simpler inputs. For example, a common way\nto construct and use a PromptTemplate is as follows:\nAPI Reference:PromptTemplate\nChatPromptTemplates\nThese prompt templates are used to format a list of messages. These "templates" consist of a list of templates themselves. For\nexample, a common way to construct and use a ChatPromptTemplate is as follows:\nAPI Reference:ChatPromptTemplate\nIn the above example, this ChatPromptTemplate will construct two messages when called. The ï¬rst is a system message, that has\nno variables to format. The second is a HumanMessage, and will be formatted by the topic variable the user passes in.', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='ChatModelSingle string, list of chat messages or a PromptValueChatMessage\nLLM Single string, list of chat messages or a PromptValueString\nOutputParserThe output of an LLM or ChatModel Depends on the parser\nRetrieverSingle string List of Documents\nTool Single string or dictionary, depending on the toolDepends on the tool\nAll runnables expose input and output schemas to inspect the inputs and outputs:\ninput_schema: an input Pydantic model auto-generated from the structure of the Runnable\noutput_schema: an output Pydantic model auto-generated from the structure of the Runnable\nComponents\nLangChain provides standard, extendable interfaces and external integrations for various components useful for building with', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content="args: The arguments to that tool.\nid: The id of that tool call.\nSystemMessage\nThis represents a system message, which tells the model how to behave. Not every model provider supports this.\nFunctionMessage\nThis represents the result of a function call. In addition to role and content, this message has a name parameter which conveys\nthe name of the function that was called to produce this result.\nToolMessage\nThis represents the result of a tool call. This is distinct from a FunctionMessage in order to match OpenAI's function and tool\nmessage types. In addition to role and content, this message has a tool_call_id parameter which conveys the id of the call to\nthe tool that was called to produce this result.\nPrompt templates", metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content="langchain-openai, langchain-anthropic, etc). This was done in order to improve support for these important integrations.\nlangchain\nThe main langchain package contains chains, agents, and retrieval strategies that make up an application's cognitive\narchitecture. These are NOT third party integrations. All chains, agents, and retrieval strategies here are NOT speciï¬c to any one\nintegration, but rather generic across all integrations.\nlangchain-community\nThis package contains third party integrations that are maintained by the LangChain community. Key partner packages are\nseparated out (see below). This contains all integrations for various components (LLMs, vector stores, retrievers). All\ndependencies in this package are optional to keep the package as lightweight as possible.\nlanggraph", metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='MessagesPlaceholder\nThis prompt template is responsible for adding a list of messages in a particular place. In the above ChatPromptTemplate, we saw\nhow we could format two messages, each one a string. But what if we wanted the user to pass in a list of messages that we would\nslot into a particular spot? This is how you use MessagesPlaceholder.\nAPI Reference:ChatPromptTemplate | MessagesPlaceholder | HumanMessage\nThis will produce a list of two messages, the ï¬rst one being a system message, and the second one being the HumanMessage we\npassed in. If we had passed in 5 messages, then it would have produced 6 messages in total (the system message plus the 5\npassed in). This is useful for letting a list of messages be slotted into a particular spot.', metadata={'document_title': 'langchain_guide.pdf'})]
2024-06-19 20:59:06,463 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 20:59:09,091 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 20:59:09,093 - INFO - Checking the response after multi queries: 
        Answer: In langchain, an agent is a type of component that is used to build robust and stateful multi-actor applications with LLMs
2024-06-19 20:59:09,093 - INFO - Checking the response after MQ RAG: 
        Answer: In langchain, an agent is a type of component that is used to build robust and stateful multi-actor applications with LLMs
2024-06-19 21:00:07,469 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7fe95c47ce20>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 21:00:07,487 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 21:00:07,503 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 21:00:07,559 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 21:00:07,576 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 21:00:08,354 - INFO - collection is admin_test_class_8, and client: <weaviate.client.WeaviateClient object at 0x7fe95c47cd00>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 21:00:08,356 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 21:00:08,358 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 21:00:08,358 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fe934c97220>
2024-06-19 21:00:08,359 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fe934c97220>
2024-06-19 21:00:23,014 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 21:00:23,354 - INFO - Checking the generated queries: [Document(page_content='langgraph\nlanggraph is an extension of langchain aimed at building robust and stateful multi-actor applications with LLMs by modeling\nsteps as edges and nodes in a graph.\nLangGraph exposes high level interfaces for creating common types of agents, as well as a low-level API for composing custom\nï¬ows.\nlangserve\nA package to deploy LangChain chains as REST APIs. Makes it easy to get a production ready API up and running.\nLangSmith\nA developer platform that lets you debug, test, evaluate, and monitor LLM applications.\nLangChain Expression Language \x00LCEL\x00\nLangChain Expression Language, or LCEL, is a declarative way to chain LangChain components. LCEL was designed from day 1 to', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='calling. Please see the tool calling section for more information.\nFor speciï¬cs on how to use chat models, see the relevant how-to guides here.\nMultimodality\nSome chat models are multimodal, accepting images, audio and even video as inputs. These are still less common, meaning\nmodel providers haven\'t standardized on the "best" way to deï¬ne the API. Multimodal outputs are even less common. As such,\nwe\'ve kept our multimodal abstractions fairly light weight and plan to further solidify the multimodal APIs and interaction\npatterns as the ï¬eld matures.\nIn LangChain, most chat models that support multimodal inputs also accept those values in OpenAI\'s content blocks format. So', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximum observability and\ndebuggability.\nSeamless LangServe deployment Any chain created with LCEL can be easily deployed using LangServe.\nRunnable interface\nTo make it as easy as possible to create custom chains, we\'ve implemented a "Runnable" protocol. Many LangChain components\nimplement the Runnable protocol, including chat models, LLMs, output parsers, retrievers, prompt templates, and more. There\nare also several useful primitives for working with runnables, which you can read about below.\nThis is a standard interface, which makes it easy to deï¬ne custom chains as well as invoke them in a standard way. The standard\ninterface includes:\nstream: stream back chunks of the response', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='various pieces of information:\nName: The name of the output parser\nSupports Streaming: Whether the output parser supports streaming.\nHas Format Instructions: Whether the output parser has format instructions. This is generally available except when (a) the\ndesired schema is not speciï¬ed in the prompt but rather in other parameters (like OpenAI function calling), or (b) when the\nOutputParser wraps another OutputParser.\nCalls LLM: Whether this output parser itself calls an LLM. This is usually only done by output parsers that attempt to correct\nmisformatted output.\nInput Type: Expected input type. Most output parsers work on both strings and messages, but some (like OpenAI Functions) need\na message with speciï¬c kwargs.\nOutput Type: The output type of the object returned by the parser.', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='reliability without any latency cost.\nAccess intermediate results For more complex chains itâs often very useful to access the results of intermediate steps even\nbefore the ï¬nal output is produced. This can be used to let end-users know something is happening, or even just to debug your\nchain. You can stream intermediate results, and itâs available on every LangServe server.\nInput and output schemas Input and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from\nthe structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.\nSeamless LangSmith tracing As your chains get more and more complex, it becomes increasingly important to understand what', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='There are a few different types of prompt templates:\nString PromptTemplates\nThese prompt templates are used to format a single string, and generally are used for simpler inputs. For example, a common way\nto construct and use a PromptTemplate is as follows:\nAPI Reference:PromptTemplate\nChatPromptTemplates\nThese prompt templates are used to format a list of messages. These "templates" consist of a list of templates themselves. For\nexample, a common way to construct and use a ChatPromptTemplate is as follows:\nAPI Reference:ChatPromptTemplate\nIn the above example, this ChatPromptTemplate will construct two messages when called. The ï¬rst is a system message, that has\nno variables to format. The second is a HumanMessage, and will be formatted by the topic variable the user passes in.', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='LangChain 0.2 is out! Leave feedback on the v0.2 docs here. You can view the v0.1 docs here.\nConceptual guide\nConceptual guide\nThis section contains introductions to key parts of LangChain.\nArchitecture\nLangChain as a framework consists of a number of packages.\nlangchain-core\nThis package contains base abstractions of different components and ways to compose them together. The interfaces for core\ncomponents like LLMs, vector stores, retrievers and more are deï¬ned here. No third party integrations are deï¬ned here. The\ndependencies are kept purposefully very lightweight.\nPartner packages\nWhile the long tail of integrations are in langchain-community, we split popular integrations into their own packages (e.g.', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='string under the hood before being passed to the underlying model.\nLangChain does not host any LLMs, rather we rely on third party integrations.\nFor speciï¬cs on how to use LLMs, see the relevant how-to guides here.\nMessages\nSome language models take a list of messages as input and return a message. There are a few different types of messages. All\nmessages have a role, content, and response_metadata property.\nThe role describes WHO is saying the message. LangChain has different message classes for different roles.\nThe content property describes the content of the message. This can be a few different things:\nA string (most models deal this type of content)\nA List of dictionaries (this is used for multimodal input, where the dictionary contains information about that input type and', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='far this is restricted to image inputs. For models like Gemini which support video and other bytes input, the APIs also support the\nnative, model-speciï¬c representations.\nFor speciï¬cs on how to use multimodal models, see the relevant how-to guides here.\nFor a full list of LangChain model providers with multimodal models, check out this table.\nLLMs\nLanguage models that takes a string as input and returns a string. These are traditionally older models (newer models generally\nare Chat Models, see below).\nAlthough the underlying models are string in, string out, the LangChain wrappers also allow these models to take messages as\ninput. This gives them the same interface as Chat Models. When messages are passed in as input, they will be formatted into a', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content="args: The arguments to that tool.\nid: The id of that tool call.\nSystemMessage\nThis represents a system message, which tells the model how to behave. Not every model provider supports this.\nFunctionMessage\nThis represents the result of a function call. In addition to role and content, this message has a name parameter which conveys\nthe name of the function that was called to produce this result.\nToolMessage\nThis represents the result of a tool call. This is distinct from a FunctionMessage in order to match OpenAI's function and tool\nmessage types. In addition to role and content, this message has a tool_call_id parameter which conveys the id of the call to\nthe tool that was called to produce this result.\nPrompt templates", metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content="langchain-openai, langchain-anthropic, etc). This was done in order to improve support for these important integrations.\nlangchain\nThe main langchain package contains chains, agents, and retrieval strategies that make up an application's cognitive\narchitecture. These are NOT third party integrations. All chains, agents, and retrieval strategies here are NOT speciï¬c to any one\nintegration, but rather generic across all integrations.\nlangchain-community\nThis package contains third party integrations that are maintained by the LangChain community. Key partner packages are\nseparated out (see below). This contains all integrations for various components (LLMs, vector stores, retrievers). All\ndependencies in this package are optional to keep the package as lightweight as possible.\nlanggraph", metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='MessagesPlaceholder\nThis prompt template is responsible for adding a list of messages in a particular place. In the above ChatPromptTemplate, we saw\nhow we could format two messages, each one a string. But what if we wanted the user to pass in a list of messages that we would\nslot into a particular spot? This is how you use MessagesPlaceholder.\nAPI Reference:ChatPromptTemplate | MessagesPlaceholder | HumanMessage\nThis will produce a list of two messages, the ï¬rst one being a system message, and the second one being the HumanMessage we\npassed in. If we had passed in 5 messages, then it would have produced 6 messages in total (the system message plus the 5\npassed in). This is useful for letting a list of messages be slotted into a particular spot.', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='LLMs. Some components LangChain implements, some components we rely on third-party integrations for, and others are a mix.\nChat models\nLanguage models that use a sequence of messages as inputs and return chat messages as outputs (as opposed to using plain\ntext). These are traditionally newer models (older models are generally LLMs, see below). Chat models support the assignment of\ndistinct roles to conversation messages, helping to distinguish messages from the AI, users, and instructions such as system\nmessages.\nAlthough the underlying models are messages in, message out, the LangChain wrappers also allow these models to take a string\nas input. This means you can easily use chat models in place of LLMs.', metadata={'document_title': 'langchain_guide.pdf'})]
2024-06-19 21:01:07,070 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 21:01:08,257 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 400 Bad Request"
2024-06-19 21:01:08,258 - ERROR - An error occurred: Error code: 400 - {'object': 'error', 'message': "This model's maximum context length is 3000 tokens. However, you requested 4771 tokens (3687 in the messages, 1084 in the completion). Please reduce the length of the messages or completion.", 'type': 'BadRequestError', 'param': None, 'code': 400}
2024-06-19 21:01:45,779 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7fe95c47cdf0>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 21:01:45,795 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 21:01:45,811 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 21:01:45,869 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 21:01:45,887 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 21:01:46,617 - INFO - collection is admin_test_class_8, and client: <weaviate.client.WeaviateClient object at 0x7fe954613100>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 21:01:46,619 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 21:01:46,620 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 21:01:46,621 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fe934c97df0>
2024-06-19 21:01:46,621 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fe934c97df0>
2024-06-19 21:01:53,366 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 21:01:53,572 - INFO - Checking the generated queries: [Document(page_content='with pandas DataFrames.\nEnum âstr|\nMessageEnumParses response into one of\nthe provided enum values.\nDatetime âstr|\nMessagedatetime.datetimeParses response into a\ndatetime string.\nStructured âstr|\nMessageDict[str, str]An output parser that\nreturns structured\ninformation. It is less\npowerful than other output\nparsers since it only allows\nfor ï¬elds to be strings. This\ncan be useful when you are\nworking with smaller LLMs.\nFor speciï¬cs on how to use output parsers, see the relevant how-to guides here.\nChat history\nMost LLM applications have a conversational interface. An essential component of a conversation is being able to refer to\ninformation introduced earlier in the conversation. At bare minimum, a conversational system should be able to access some\nwindow of past messages directly.', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='langgraph\nlanggraph is an extension of langchain aimed at building robust and stateful multi-actor applications with LLMs by modeling\nsteps as edges and nodes in a graph.\nLangGraph exposes high level interfaces for creating common types of agents, as well as a low-level API for composing custom\nï¬ows.\nlangserve\nA package to deploy LangChain chains as REST APIs. Makes it easy to get a production ready API up and running.\nLangSmith\nA developer platform that lets you debug, test, evaluate, and monitor LLM applications.\nLangChain Expression Language \x00LCEL\x00\nLangChain Expression Language, or LCEL, is a declarative way to chain LangChain components. LCEL was designed from day 1 to', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='various pieces of information:\nName: The name of the output parser\nSupports Streaming: Whether the output parser supports streaming.\nHas Format Instructions: Whether the output parser has format instructions. This is generally available except when (a) the\ndesired schema is not speciï¬ed in the prompt but rather in other parameters (like OpenAI function calling), or (b) when the\nOutputParser wraps another OutputParser.\nCalls LLM: Whether this output parser itself calls an LLM. This is usually only done by output parsers that attempt to correct\nmisformatted output.\nInput Type: Expected input type. Most output parsers work on both strings and messages, but some (like OpenAI Functions) need\na message with speciï¬c kwargs.\nOutput Type: The output type of the object returned by the parser.', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content="Output Type: The output type of the object returned by the parser.\nDescription: Our commentary on this output parser and when to use it.\nNameSupports\nStreamingHas Format\nInstructionsCalls\nLLMInput\nTypeOutput Type Description\nJSON ââstr|\nMessageJSON objectReturns a JSON object as\nspeciï¬ed. You can specify a\nPydantic model and it will\nreturn JSON for that model.\nProbably the most reliable\noutput parser for getting\nstructured data that does\nNOT use function calling.\nXML ââstr|\nMessagedictReturns a dictionary of tags.\nUse when XML output is\nneeded. Use with models\nthat are good at writing\nXML (like Anthropic's).\nCSV ââstr|\nMessageList[str]Returns a list of comma\nseparated values.\nOutputFixing âstr|\nMessageWraps another output\nparser. If that output parser\nerrors, then this will pass", metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='interface includes:\nstream: stream back chunks of the response\ninvoke: call the chain on an input\nbatch: call the chain on a list of inputs\nThese also have corresponding async methods that should be used with asyncioawait syntax for concurrency:\nastream: stream back chunks of the response async\nainvoke: call the chain on an input async\nabatch: call the chain on a list of inputs async\nastream_log: stream back intermediate steps as they happen, in addition to the ï¬nal response\nastream_events: beta stream events as they happen in the chain (introduced in langchain-core 0.1.14)\nThe input type and output type varies by component:\nComponent Input Type Output Type\nPromptDictionary PromptValue\nChatModelSingle string, list of chat messages or a PromptValueChatMessage', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='reliability without any latency cost.\nAccess intermediate results For more complex chains itâs often very useful to access the results of intermediate steps even\nbefore the ï¬nal output is produced. This can be used to let end-users know something is happening, or even just to debug your\nchain. You can stream intermediate results, and itâs available on every LangServe server.\nInput and output schemas Input and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from\nthe structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.\nSeamless LangSmith tracing As your chains get more and more complex, it becomes increasingly important to understand what', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='LangChain 0.2 is out! Leave feedback on the v0.2 docs here. You can view the v0.1 docs here.\nConceptual guide\nConceptual guide\nThis section contains introductions to key parts of LangChain.\nArchitecture\nLangChain as a framework consists of a number of packages.\nlangchain-core\nThis package contains base abstractions of different components and ways to compose them together. The interfaces for core\ncomponents like LLMs, vector stores, retrievers and more are deï¬ned here. No third party integrations are deï¬ned here. The\ndependencies are kept purposefully very lightweight.\nPartner packages\nWhile the long tail of integrations are in langchain-community, we split popular integrations into their own packages (e.g.', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='parser. If that output parser\nerrors, then this will pass\nthe error message and the\nbad output to an LLM and\nask it to ï¬x the output.\nRetryWithError âstr|\nMessageWraps another output\nparser. If that output parser\nerrors, then this will pass\nthe original inputs, the bad\noutput, and the error\nmessage to an LLM and ask\nit to ï¬x it. Compared to\nOutputFixingParser, this\none also sends the original\ninstructions.\nPydantic âstr|\nMessagepydantic.BaseModelTakes a user deï¬ned\nPydantic model and returns\ndata in that format.\nYAML âstr|\nMessagepydantic.BaseModelTakes a user deï¬ned\nPydantic model and returns\ndata in that format. Uses\nYAML to encode it.\nPandasDataFrame âstr|\nMessagedictUseful for doing operations\nwith pandas DataFrames.\nEnum âstr|\nMessageEnumParses response into one of', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='Output parsers\nNOTE\nThe information here refers to parsers that take a text output from a model try to parse it into a more structured\nrepresentation. More and more models are supporting function (or tool) calling, which handles this automatically. It is\nrecommended to use function/tool calling rather than output parsing. See documentation for that here.\nResponsible for taking the output of a model and transforming it to a more suitable format for downstream tasks. Useful when\nyou are using LLMs to generate structured data, or to normalize output from chat models and LLMs.\nLangChain has lots of different types of output parsers. This is a list of output parsers LangChain supports. The table below has\nvarious pieces of information:\nName: The name of the output parser', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content="args: The arguments to that tool.\nid: The id of that tool call.\nSystemMessage\nThis represents a system message, which tells the model how to behave. Not every model provider supports this.\nFunctionMessage\nThis represents the result of a function call. In addition to role and content, this message has a name parameter which conveys\nthe name of the function that was called to produce this result.\nToolMessage\nThis represents the result of a tool call. This is distinct from a FunctionMessage in order to match OpenAI's function and tool\nmessage types. In addition to role and content, this message has a tool_call_id parameter which conveys the id of the call to\nthe tool that was called to produce this result.\nPrompt templates", metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content="langchain-openai, langchain-anthropic, etc). This was done in order to improve support for these important integrations.\nlangchain\nThe main langchain package contains chains, agents, and retrieval strategies that make up an application's cognitive\narchitecture. These are NOT third party integrations. All chains, agents, and retrieval strategies here are NOT speciï¬c to any one\nintegration, but rather generic across all integrations.\nlangchain-community\nThis package contains third party integrations that are maintained by the LangChain community. Key partner packages are\nseparated out (see below). This contains all integrations for various components (LLMs, vector stores, retrievers). All\ndependencies in this package are optional to keep the package as lightweight as possible.\nlanggraph", metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='MessagesPlaceholder\nThis prompt template is responsible for adding a list of messages in a particular place. In the above ChatPromptTemplate, we saw\nhow we could format two messages, each one a string. But what if we wanted the user to pass in a list of messages that we would\nslot into a particular spot? This is how you use MessagesPlaceholder.\nAPI Reference:ChatPromptTemplate | MessagesPlaceholder | HumanMessage\nThis will produce a list of two messages, the ï¬rst one being a system message, and the second one being the HumanMessage we\npassed in. If we had passed in 5 messages, then it would have produced 6 messages in total (the system message plus the 5\npassed in). This is useful for letting a list of messages be slotted into a particular spot.', metadata={'document_title': 'langchain_guide.pdf'})]
2024-06-19 21:01:54,120 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 21:01:56,434 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 21:01:56,436 - INFO - Checking the response after multi queries: 
        Answer: Output parsers are used to convert the output of an LLM or ChatModel into a format that can be used by the application
2024-06-19 21:01:56,437 - INFO - Checking the response after MQ RAG: 
        Answer: Output parsers are used to convert the output of an LLM or ChatModel into a format that can be used by the application
2024-06-19 21:02:17,921 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7fe934d38d60>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 21:02:17,937 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 21:02:17,953 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 21:02:18,012 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 21:02:18,030 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 21:02:18,831 - INFO - collection is admin_test_class_8, and client: <weaviate.client.WeaviateClient object at 0x7fe95c47cdf0>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 21:02:18,834 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 21:02:18,835 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 21:02:18,836 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fe934c4d7e0>
2024-06-19 21:02:18,836 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fe934c4d7e0>
2024-06-19 21:02:20,848 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 21:02:20,850 - INFO - Checking the hypo documents: 

Output: 

Output parsers are used to convert raw data from various sources into a structured format that can be easily consumed and processed by software applications
2024-06-19 21:02:20,878 - INFO - Logging the documents in hyde: [Document(page_content='Output parsers\nNOTE\nThe information here refers to parsers that take a text output from a model try to parse it into a more structured\nrepresentation. More and more models are supporting function (or tool) calling, which handles this automatically. It is\nrecommended to use function/tool calling rather than output parsing. See documentation for that here.\nResponsible for taking the output of a model and transforming it to a more suitable format for downstream tasks. Useful when\nyou are using LLMs to generate structured data, or to normalize output from chat models and LLMs.\nLangChain has lots of different types of output parsers. This is a list of output parsers LangChain supports. The table below has\nvarious pieces of information:\nName: The name of the output parser', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='various pieces of information:\nName: The name of the output parser\nSupports Streaming: Whether the output parser supports streaming.\nHas Format Instructions: Whether the output parser has format instructions. This is generally available except when (a) the\ndesired schema is not speciï¬ed in the prompt but rather in other parameters (like OpenAI function calling), or (b) when the\nOutputParser wraps another OutputParser.\nCalls LLM: Whether this output parser itself calls an LLM. This is usually only done by output parsers that attempt to correct\nmisformatted output.\nInput Type: Expected input type. Most output parsers work on both strings and messages, but some (like OpenAI Functions) need\na message with speciï¬c kwargs.\nOutput Type: The output type of the object returned by the parser.', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content="Output Type: The output type of the object returned by the parser.\nDescription: Our commentary on this output parser and when to use it.\nNameSupports\nStreamingHas Format\nInstructionsCalls\nLLMInput\nTypeOutput Type Description\nJSON ââstr|\nMessageJSON objectReturns a JSON object as\nspeciï¬ed. You can specify a\nPydantic model and it will\nreturn JSON for that model.\nProbably the most reliable\noutput parser for getting\nstructured data that does\nNOT use function calling.\nXML ââstr|\nMessagedictReturns a dictionary of tags.\nUse when XML output is\nneeded. Use with models\nthat are good at writing\nXML (like Anthropic's).\nCSV ââstr|\nMessageList[str]Returns a list of comma\nseparated values.\nOutputFixing âstr|\nMessageWraps another output\nparser. If that output parser\nerrors, then this will pass", metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='with pandas DataFrames.\nEnum âstr|\nMessageEnumParses response into one of\nthe provided enum values.\nDatetime âstr|\nMessagedatetime.datetimeParses response into a\ndatetime string.\nStructured âstr|\nMessageDict[str, str]An output parser that\nreturns structured\ninformation. It is less\npowerful than other output\nparsers since it only allows\nfor ï¬elds to be strings. This\ncan be useful when you are\nworking with smaller LLMs.\nFor speciï¬cs on how to use output parsers, see the relevant how-to guides here.\nChat history\nMost LLM applications have a conversational interface. An essential component of a conversation is being able to refer to\ninformation introduced earlier in the conversation. At bare minimum, a conversational system should be able to access some\nwindow of past messages directly.', metadata={'document_title': 'langchain_guide.pdf'})]
2024-06-19 21:02:20,895 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 21:02:22,593 - INFO - logging the response of hyde: 
            Answer: Output parsers are used to transform the output of a model into a more suitable format for downstream tasks
2024-06-19 21:02:22,593 - INFO - CHecking after hyde the response: 
            Answer: Output parsers are used to transform the output of a model into a more suitable format for downstream tasks
2024-06-19 21:02:32,554 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7fe934c4e7a0>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 21:02:32,569 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 21:02:32,585 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 21:02:32,642 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 21:02:32,662 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 21:02:33,448 - INFO - collection is admin_test_class_8, and client: <weaviate.client.WeaviateClient object at 0x7fe934c94730>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 21:02:33,450 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 21:02:33,452 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_8 "HTTP/1.1 200 OK"
2024-06-19 21:02:33,453 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fe934c93cd0>
2024-06-19 21:02:33,453 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fe934c93cd0>
2024-06-19 21:02:35,411 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 21:02:35,414 - INFO - Checking the hypo documents: 

            Output: 
            Output parsers are used to convert raw data or files into a structured format that can be easily understood and processed by software applications
2024-06-19 21:02:35,442 - INFO - Logging the documents in hyde: [Document(page_content='Output parsers\nNOTE\nThe information here refers to parsers that take a text output from a model try to parse it into a more structured\nrepresentation. More and more models are supporting function (or tool) calling, which handles this automatically. It is\nrecommended to use function/tool calling rather than output parsing. See documentation for that here.\nResponsible for taking the output of a model and transforming it to a more suitable format for downstream tasks. Useful when\nyou are using LLMs to generate structured data, or to normalize output from chat models and LLMs.\nLangChain has lots of different types of output parsers. This is a list of output parsers LangChain supports. The table below has\nvarious pieces of information:\nName: The name of the output parser', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='various pieces of information:\nName: The name of the output parser\nSupports Streaming: Whether the output parser supports streaming.\nHas Format Instructions: Whether the output parser has format instructions. This is generally available except when (a) the\ndesired schema is not speciï¬ed in the prompt but rather in other parameters (like OpenAI function calling), or (b) when the\nOutputParser wraps another OutputParser.\nCalls LLM: Whether this output parser itself calls an LLM. This is usually only done by output parsers that attempt to correct\nmisformatted output.\nInput Type: Expected input type. Most output parsers work on both strings and messages, but some (like OpenAI Functions) need\na message with speciï¬c kwargs.\nOutput Type: The output type of the object returned by the parser.', metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content="Output Type: The output type of the object returned by the parser.\nDescription: Our commentary on this output parser and when to use it.\nNameSupports\nStreamingHas Format\nInstructionsCalls\nLLMInput\nTypeOutput Type Description\nJSON ââstr|\nMessageJSON objectReturns a JSON object as\nspeciï¬ed. You can specify a\nPydantic model and it will\nreturn JSON for that model.\nProbably the most reliable\noutput parser for getting\nstructured data that does\nNOT use function calling.\nXML ââstr|\nMessagedictReturns a dictionary of tags.\nUse when XML output is\nneeded. Use with models\nthat are good at writing\nXML (like Anthropic's).\nCSV ââstr|\nMessageList[str]Returns a list of comma\nseparated values.\nOutputFixing âstr|\nMessageWraps another output\nparser. If that output parser\nerrors, then this will pass", metadata={'document_title': 'langchain_guide.pdf'}), Document(page_content='with pandas DataFrames.\nEnum âstr|\nMessageEnumParses response into one of\nthe provided enum values.\nDatetime âstr|\nMessagedatetime.datetimeParses response into a\ndatetime string.\nStructured âstr|\nMessageDict[str, str]An output parser that\nreturns structured\ninformation. It is less\npowerful than other output\nparsers since it only allows\nfor ï¬elds to be strings. This\ncan be useful when you are\nworking with smaller LLMs.\nFor speciï¬cs on how to use output parsers, see the relevant how-to guides here.\nChat history\nMost LLM applications have a conversational interface. An essential component of a conversation is being able to refer to\ninformation introduced earlier in the conversation. At bare minimum, a conversational system should be able to access some\nwindow of past messages directly.', metadata={'document_title': 'langchain_guide.pdf'})]
2024-06-19 21:02:35,453 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 21:02:37,221 - INFO - logging the response of hyde: 
            Answer: Output parsers are used for transforming the output of a model into a more suitable format for downstream tasks
2024-06-19 21:02:37,221 - INFO - CHecking after hyde the response: 
            Answer: Output parsers are used for transforming the output of a model into a more suitable format for downstream tasks
