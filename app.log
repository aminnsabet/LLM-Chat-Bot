2024-06-19 15:22:45,348 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:22:45,364 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:22:45,430 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:22:45,451 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:22:45,452 - INFO - info on query passed: Ticketing and the class name: <weaviate.Collection config={
  "name": "Admin_test_class_7",
  "description": null,
  "generative_config": null,
  "inverted_index_config": {
    "bm25": {
      "b": 0.75,
      "k1": 1.2
    },
    "cleanup_interval_seconds": 60,
    "index_null_state": false,
    "index_property_length": false,
    "index_timestamps": false,
    "stopwords": {
      "preset": "en",
      "additions": null,
      "removals": null
    }
  },
  "multi_tenancy_config": {
    "enabled": false,
    "auto_tenant_creation": false,
    "auto_tenant_activation": false
  },
  "properties": [
    {
      "name": "document_title",
      "description": null,
      "data_type": "text",
      "index_filterable": true,
      "index_searchable": true,
      "nested_properties": null,
      "tokenization": "word",
      "vectorizer_config": {
        "skip": false,
        "vectorize_property_name": true
      },
      "vectorizer": "text2vec-huggingface"
    },
    {
      "name": "page_content",
      "description": null,
      "data_type": "text",
      "index_filterable": true,
      "index_searchable": true,
      "nested_properties": null,
      "tokenization": "word",
      "vectorizer_config": {
        "skip": false,
        "vectorize_property_name": true
      },
      "vectorizer": "text2vec-huggingface"
    }
  ],
  "references": [],
  "replication_config": {
    "factor": 1
  },
  "reranker_config": null,
  "sharding_config": {
    "virtual_per_physical": 128,
    "desired_count": 1,
    "actual_count": 1,
    "desired_virtual_count": 128,
    "actual_virtual_count": 128,
    "key": "_id",
    "strategy": "hash",
    "function": "murmur3"
  },
  "vector_index_config": {
    "quantizer": null,
    "cleanup_interval_seconds": 300,
    "distance_metric": "cosine",
    "dynamic_ef_min": 100,
    "dynamic_ef_max": 500,
    "dynamic_ef_factor": 8,
    "ef": -1,
    "ef_construction": 128,
    "flat_search_cutoff": 40000,
    "max_connections": 64,
    "skip": false,
    "vector_cache_max_objects": 1000000000000
  },
  "vector_index_type": "hnsw",
  "vectorizer_config": {
    "vectorizer": "text2vec-huggingface",
    "model": {
      "model": "sentence-transformers/all-MiniLM-L6-v2"
    },
    "vectorize_collection_name": true
  },
  "vectorizer": "text2vec-huggingface",
  "vector_config": null
}>
2024-06-19 15:22:45,477 - INFO - logged properties: {'page_content': 'Countdown clock for UEFA\nEuro 2024 in front of\nDÃ¼sseldorf City HallPlayer Offence(s) Suspension(s)\n Giorgi Loria\n  in qualifying vs Greece (26 March 2024) Group F vs Turkey (matchday 1; 18 June 2024)', 'document_title': 'UEFA_Euro_2024.pdf'}
2024-06-19 15:22:45,477 - INFO - logged distance: 0.737564742565155
2024-06-19 15:22:45,477 - INFO - logged properties: {'page_content': 'Source: UEFA[95]\nA player is automatically suspended for the next match for the following of fences:[69]\nReceiving a red card (red card suspensions can be extended for serious offences)', 'document_title': 'UEFA_Euro_2024.pdf'}
2024-06-19 15:22:45,477 - INFO - logged distance: 0.7729887962341309
2024-06-19 15:22:45,477 - INFO - Here is the response after similarity search request: QueryReturn(objects=[Object(uuid=_WeaviateUUIDInt('d7d484e0-357a-4715-a6c4-1e2183bc538e'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=0.737564742565155, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'page_content': 'Countdown clock for UEFA\nEuro 2024 in front of\nDÃ¼sseldorf City HallPlayer Offence(s) Suspension(s)\n Giorgi Loria\n  in qualifying vs Greece (26 March 2024) Group F vs Turkey (matchday 1; 18 June 2024)', 'document_title': 'UEFA_Euro_2024.pdf'}, references=None, vector={}, collection='Admin_test_class_7'), Object(uuid=_WeaviateUUIDInt('7e662657-5fbc-466a-ae19-c575cf0ffe03'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=0.7729887962341309, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'page_content': 'Source: UEFA[95]\nA player is automatically suspended for the next match for the following of fences:[69]\nReceiving a red card (red card suspensions can be extended for serious offences)', 'document_title': 'UEFA_Euro_2024.pdf'}, references=None, vector={}, collection='Admin_test_class_7')])
2024-06-19 15:24:03,102 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:24:03,119 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:24:03,179 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:24:03,230 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7f3b69dc0a00>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:24:03,230 - INFO - Checking the embedder : sentence-transformers/all-MiniLM-L6-v2 and the current llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:24:03,247 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:24:03,263 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:24:03,325 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:24:03,344 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 15:24:04,158 - INFO - collection is admin_test_class_7, and client: <weaviate.client.WeaviateClient object at 0x7f3b94c14940>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 15:24:04,160 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:24:04,162 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:24:04,163 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f3b84327e50>
2024-06-19 15:24:04,164 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f3b84327e50>
2024-06-19 15:24:04,164 - INFO - Checkpoint RAG
2024-06-19 15:24:04,235 - INFO - Checking prompt before RAG: input_variables=['context', 'question'] metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: {question} \nContext: {context} \nAnswer:"))] and checking if retriever exists: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f3b84327e50>, checking llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:24:04,236 - INFO - logging the rag chain: first={
  context: VectorStoreRetriever(tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'], vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f3b84327e50>),
  question: RunnablePassthrough()
} middle=[ChatPromptTemplate(input_variables=['context', 'question'], metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: {question} \nContext: {context} \nAnswer:"))]), VLLMOpenAI(client=<openai.resources.completions.Completions object at 0x7f3b94c59e70>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7f3b69dc08b0>, model_name='meta-llama/Llama-2-7b-chat-hf', model_kwargs={'stop': ['.']}, openai_api_key='EMPTY', openai_api_base='http://localhost:8500/v1', openai_proxy='', streaming=True)] last=StrOutputParser() and retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f3b84327e50>
2024-06-19 15:24:04,280 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 400 Bad Request"
2024-06-19 15:24:04,280 - ERROR - An error occurred: Error code: 400 - {'object': 'error', 'message': "This model's maximum context length is 512 tokens. However, you requested 698 tokens (442 in the messages, 256 in the completion). Please reduce the length of the messages or completion.", 'type': 'BadRequestError', 'param': None, 'code': 400}
2024-06-19 15:26:53,759 - INFO - At initilization stage embedding model is: None and vllm model is: None
2024-06-19 15:26:59,222 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:26:59,240 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:26:59,306 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:26:59,705 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7f7e7dcee3e0>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:26:59,705 - INFO - Checking the embedder : sentence-transformers/all-MiniLM-L6-v2 and the current llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:26:59,722 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:26:59,738 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:26:59,799 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:26:59,817 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 15:27:00,617 - INFO - collection is admin_test_class_7, and client: <weaviate.client.WeaviateClient object at 0x7f7da4578b80>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 15:27:00,619 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:27:00,620 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:27:00,621 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f7da4408130>
2024-06-19 15:27:00,621 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f7da4408130>
2024-06-19 15:27:00,621 - INFO - Checkpoint RAG
2024-06-19 15:27:00,622 - INFO - Checking prompt before RAG: input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))] and checking if retriever exists: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f7da4408130>, checking llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:27:00,623 - INFO - logging the rag chain: first={
  context: VectorStoreRetriever(tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'], vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f7da4408130>),
  question: RunnablePassthrough()
} middle=[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))]), VLLMOpenAI(client=<openai.resources.completions.Completions object at 0x7f7db4b52770>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7f7da455e770>, model_name='meta-llama/Llama-2-7b-chat-hf', model_kwargs={'stop': ['.']}, openai_api_key='EMPTY', openai_api_base='http://localhost:8500/v1', openai_proxy='', streaming=True)] last=StrOutputParser() and retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f7da4408130>
2024-06-19 15:27:01,046 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 400 Bad Request"
2024-06-19 15:27:01,048 - ERROR - An error occurred: Error code: 400 - {'object': 'error', 'message': "This model's maximum context length is 512 tokens. However, you requested 712 tokens (456 in the messages, 256 in the completion). Please reduce the length of the messages or completion.", 'type': 'BadRequestError', 'param': None, 'code': 400}
2024-06-19 15:34:36,626 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:34:36,648 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:34:36,707 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:34:36,771 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7f7d8bb27a90>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:34:36,771 - INFO - Checking the embedder : sentence-transformers/all-MiniLM-L6-v2 and the current llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:34:36,791 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:34:36,808 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:34:36,871 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:34:36,891 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 15:34:38,037 - INFO - collection is admin_test_class_7, and client: <weaviate.client.WeaviateClient object at 0x7f7d8b9cec80>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 15:34:38,040 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:34:38,042 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:34:38,042 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f7da42ed420>
2024-06-19 15:34:38,044 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f7da42ed420>
2024-06-19 15:34:38,045 - INFO - Checkpoint RAG
2024-06-19 15:34:38,045 - INFO - Checking prompt before RAG: input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))] and checking if retriever exists: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f7da42ed420>, checking llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:34:38,046 - INFO - logging the rag chain: first={
  context: VectorStoreRetriever(tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'], vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f7da42ed420>),
  question: RunnablePassthrough()
} middle=[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))]), VLLMOpenAI(client=<openai.resources.completions.Completions object at 0x7f7e7dcee3e0>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7f7d8b9cca30>, model_name='meta-llama/Llama-2-7b-chat-hf', model_kwargs={'stop': ['.']}, openai_api_key='EMPTY', openai_api_base='http://localhost:8500/v1', openai_proxy='', streaming=True)] last=StrOutputParser() and retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f7da42ed420>
2024-06-19 15:34:38,097 - INFO - Retrying request to /completions in 0.961940 seconds
2024-06-19 15:34:39,063 - INFO - Retrying request to /completions in 1.786003 seconds
2024-06-19 15:34:40,859 - ERROR - An error occurred: Connection error.
2024-06-19 15:35:05,888 - INFO - At initilization stage embedding model is: None and vllm model is: None
2024-06-19 15:35:57,001 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:35:57,019 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:35:57,079 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:35:57,098 - INFO - HTTP Request: GET http://localhost:8080/v1/schema "HTTP/1.1 200 OK"
2024-06-19 15:35:57,099 - INFO - classes: {'Admin_test_class_5': _CollectionConfigSimple(name='Admin_test_class_5', description=None, generative_config=None, properties=[_Property(name='title', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='body', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='page_content', description="This property was generated by Weaviate's auto-schema feature on Wed Jun 19 14:44:12 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface'), _Property(name='document_title', description="This property was generated by Weaviate's auto-schema feature on Wed Jun 19 14:44:12 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface')], references=[], reranker_config=None, vectorizer_config=_VectorizerConfig(vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, model={'model': 'sentence-transformers/all-MiniLM-L6-v2'}, vectorize_collection_name=True), vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, vector_config=None), 'Admin_test_class_6': _CollectionConfigSimple(name='Admin_test_class_6', description=None, generative_config=None, properties=[_Property(name='title', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='body', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='page_content', description="This property was generated by Weaviate's auto-schema feature on Wed Jun 19 14:50:16 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface'), _Property(name='document_title', description="This property was generated by Weaviate's auto-schema feature on Wed Jun 19 14:50:16 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface')], references=[], reranker_config=None, vectorizer_config=_VectorizerConfig(vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, model={'model': 'sentence-transformers/all-MiniLM-L6-v2'}, vectorize_collection_name=True), vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, vector_config=None), 'Admin_test_class_4': _CollectionConfigSimple(name='Admin_test_class_4', description=None, generative_config=None, properties=[_Property(name='title', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='body', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='page_content', description="This property was generated by Weaviate's auto-schema feature on Wed Jun 19 13:59:24 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface'), _Property(name='document_title', description="This property was generated by Weaviate's auto-schema feature on Wed Jun 19 13:59:24 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface')], references=[], reranker_config=None, vectorizer_config=_VectorizerConfig(vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, model={'model': 'sentence-transformers/all-MiniLM-L6-v2'}, vectorize_collection_name=True), vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, vector_config=None), 'Admin_test_class': _CollectionConfigSimple(name='Admin_test_class', description=None, generative_config=None, properties=[_Property(name='title', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='body', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='page_content', description="This property was generated by Weaviate's auto-schema feature on Tue Jun 18 20:45:56 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface'), _Property(name='document_title', description="This property was generated by Weaviate's auto-schema feature on Tue Jun 18 20:45:56 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface')], references=[], reranker_config=None, vectorizer_config=_VectorizerConfig(vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, model={'model': 'sentence-transformers/all-MiniLM-L6-v2'}, vectorize_collection_name=True), vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, vector_config=None), 'Admin_test_class_2': _CollectionConfigSimple(name='Admin_test_class_2', description=None, generative_config=None, properties=[_Property(name='title', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='body', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='source', description="This property was generated by Weaviate's auto-schema feature on Wed Jun 19 13:43:15 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface'), _Property(name='page', description="This property was generated by Weaviate's auto-schema feature on Wed Jun 19 13:43:15 2024", data_type=<DataType.NUMBER: 'number'>, index_filterable=True, index_searchable=False, nested_properties=None, tokenization=None, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface'), _Property(name='text', description="This property was generated by Weaviate's auto-schema feature on Wed Jun 19 13:43:15 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface')], references=[], reranker_config=None, vectorizer_config=_VectorizerConfig(vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, model={'model': 'sentence-transformers/all-MiniLM-L6-v2'}, vectorize_collection_name=True), vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, vector_config=None), 'Admin_test_class_3': _CollectionConfigSimple(name='Admin_test_class_3', description=None, generative_config=None, properties=[_Property(name='title', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='body', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='page_content', description="This property was generated by Weaviate's auto-schema feature on Wed Jun 19 13:23:21 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface'), _Property(name='document_title', description="This property was generated by Weaviate's auto-schema feature on Wed Jun 19 13:23:21 2024", data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=False), vectorizer='text2vec-huggingface')], references=[], reranker_config=None, vectorizer_config=_VectorizerConfig(vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, model={'model': 'sentence-transformers/all-MiniLM-L6-v2'}, vectorize_collection_name=True), vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, vector_config=None), 'Admin_test_class_7': _CollectionConfigSimple(name='Admin_test_class_7', description=None, generative_config=None, properties=[_Property(name='document_title', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface'), _Property(name='page_content', description=None, data_type=<DataType.TEXT: 'text'>, index_filterable=True, index_searchable=True, nested_properties=None, tokenization=<Tokenization.WORD: 'word'>, vectorizer_config=_PropertyVectorizerConfig(skip=False, vectorize_property_name=True), vectorizer='text2vec-huggingface')], references=[], reranker_config=None, vectorizer_config=_VectorizerConfig(vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, model={'model': 'sentence-transformers/all-MiniLM-L6-v2'}, vectorize_collection_name=True), vectorizer=<Vectorizers.TEXT2VEC_HUGGINGFACE: 'text2vec-huggingface'>, vector_config=None)}: %s
2024-06-19 15:36:13,687 - INFO - query success: 1 documents found
2024-06-19 15:36:26,358 - INFO - query success: 1 documents found
2024-06-19 15:37:04,780 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:37:04,804 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:37:04,860 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:37:05,383 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7f23cc42a3b0>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:37:05,383 - INFO - Checking the embedder : sentence-transformers/all-MiniLM-L6-v2 and the current llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:37:05,399 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:37:05,416 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:37:05,475 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:37:05,493 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 15:37:06,531 - INFO - collection is admin_test_class_7, and client: <weaviate.client.WeaviateClient object at 0x7f22dcc88c10>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 15:37:06,533 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:37:06,535 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:37:06,535 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22dc8ff430>
2024-06-19 15:37:06,535 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22dc8ff430>
2024-06-19 15:37:06,536 - INFO - Checkpoint RAG
2024-06-19 15:37:06,536 - INFO - Checking prompt before RAG: input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))] and checking if retriever exists: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22dc8ff430>, checking llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:37:06,538 - INFO - logging the rag chain: first={
  context: VectorStoreRetriever(tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'], vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22dc8ff430>),
  question: RunnablePassthrough()
} middle=[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))]), VLLMOpenAI(client=<openai.resources.completions.Completions object at 0x7f22fb9f4c10>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7f22dcc6a800>, model_name='meta-llama/Llama-2-7b-chat-hf', model_kwargs={'stop': ['.']}, openai_api_key='EMPTY', openai_api_base='http://localhost:8500/v1', openai_proxy='', streaming=True)] last=StrOutputParser() and retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22dc8ff430>
2024-06-19 15:37:07,106 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 15:37:08,216 - INFO - Checking the response of do rag:  The prize for the winner of UEFA Euro 2024 is â‚¬28
2024-06-19 15:37:08,218 - INFO - Checking the response of do rag:  The prize for the winner of UEFA Euro 2024 is â‚¬28
2024-06-19 15:37:38,228 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:37:38,244 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:37:38,307 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:37:38,355 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7f22fb9f6980>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:37:38,356 - INFO - Checking the embedder : sentence-transformers/all-MiniLM-L6-v2 and the current llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:37:38,374 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:37:38,390 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:37:38,449 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:37:38,469 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 15:37:39,163 - INFO - collection is admin_test_class_7, and client: <weaviate.client.WeaviateClient object at 0x7f22d6b04460>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 15:37:39,165 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:37:39,167 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:37:39,167 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22dccbc0a0>
2024-06-19 15:37:39,169 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22dccbc0a0>
2024-06-19 15:37:39,169 - INFO - Checkpoint RAG
2024-06-19 15:37:39,169 - INFO - Checking prompt before RAG: input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))] and checking if retriever exists: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22dccbc0a0>, checking llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:37:39,170 - INFO - logging the rag chain: first={
  context: VectorStoreRetriever(tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'], vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22dccbc0a0>),
  question: RunnablePassthrough()
} middle=[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))]), VLLMOpenAI(client=<openai.resources.completions.Completions object at 0x7f22d6c1b2e0>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7f22d6ab26e0>, model_name='meta-llama/Llama-2-7b-chat-hf', model_kwargs={'stop': ['.']}, openai_api_key='EMPTY', openai_api_base='http://localhost:8500/v1', openai_proxy='', streaming=True)] last=StrOutputParser() and retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22dccbc0a0>
2024-06-19 15:37:39,220 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 15:37:40,311 - INFO - Checking the response of do rag:  The price range for tickets at the Euro 2024 is â‚¬2
2024-06-19 15:37:40,312 - INFO - Checking the response of do rag:  The price range for tickets at the Euro 2024 is â‚¬2
2024-06-19 15:38:07,160 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:38:07,176 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:38:07,236 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:38:07,284 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7f22d6c1ad10>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:38:07,284 - INFO - Checking the embedder : sentence-transformers/all-MiniLM-L6-v2 and the current llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:38:07,300 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:38:07,315 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:38:07,371 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:38:07,391 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 15:38:08,131 - INFO - collection is admin_test_class_7, and client: <weaviate.client.WeaviateClient object at 0x7f22d6b07280>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 15:38:08,133 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:38:08,135 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:38:08,135 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22d6b1e7d0>
2024-06-19 15:38:08,137 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22d6b1e7d0>
2024-06-19 15:38:08,137 - INFO - Checkpoint RAG
2024-06-19 15:38:08,138 - INFO - Checking prompt before RAG: input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))] and checking if retriever exists: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22d6b1e7d0>, checking llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:38:08,138 - INFO - logging the rag chain: first={
  context: VectorStoreRetriever(tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'], vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22d6b1e7d0>),
  question: RunnablePassthrough()
} middle=[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))]), VLLMOpenAI(client=<openai.resources.completions.Completions object at 0x7f22d6c1b4c0>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7f22d6e2f5e0>, model_name='meta-llama/Llama-2-7b-chat-hf', model_kwargs={'stop': ['.']}, openai_api_key='EMPTY', openai_api_base='http://localhost:8500/v1', openai_proxy='', streaming=True)] last=StrOutputParser() and retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22d6b1e7d0>
2024-06-19 15:38:08,181 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 15:38:09,397 - INFO - Checking the response of do rag:  I apologize, but I don't know who hosts the Euro 2024
2024-06-19 15:38:09,399 - INFO - Checking the response of do rag:  I apologize, but I don't know who hosts the Euro 2024
2024-06-19 15:38:30,234 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:38:30,250 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:38:30,312 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:38:30,359 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7f22d6b05900>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:38:30,360 - INFO - Checking the embedder : sentence-transformers/all-MiniLM-L6-v2 and the current llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:38:30,375 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:38:30,391 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:38:30,447 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:38:30,471 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 15:38:31,143 - INFO - collection is admin_test_class_7, and client: <weaviate.client.WeaviateClient object at 0x7f22d693c0d0>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 15:38:31,145 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:38:31,147 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:38:31,147 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22d6be2a40>
2024-06-19 15:38:31,149 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22d6be2a40>
2024-06-19 15:38:31,149 - INFO - Checkpoint RAG
2024-06-19 15:38:31,150 - INFO - Checking prompt before RAG: input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))] and checking if retriever exists: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22d6be2a40>, checking llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:38:31,150 - INFO - logging the rag chain: first={
  context: VectorStoreRetriever(tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'], vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22d6be2a40>),
  question: RunnablePassthrough()
} middle=[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))]), VLLMOpenAI(client=<openai.resources.completions.Completions object at 0x7f22d6c1acb0>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7f22f80b8c40>, model_name='meta-llama/Llama-2-7b-chat-hf', model_kwargs={'stop': ['.']}, openai_api_key='EMPTY', openai_api_base='http://localhost:8500/v1', openai_proxy='', streaming=True)] last=StrOutputParser() and retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22d6be2a40>
2024-06-19 15:38:31,219 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 15:38:33,238 - INFO - Checking the response of do rag: 
        Certainly! The Euro 2024 tournament is scheduled to take place in Germany, as voted by UEFA members in September 2018
2024-06-19 15:38:33,240 - INFO - Checking the response of do rag: 
        Certainly! The Euro 2024 tournament is scheduled to take place in Germany, as voted by UEFA members in September 2018
2024-06-19 15:39:22,337 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:39:22,353 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:39:22,411 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:39:22,458 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7f22dcc88730>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:39:22,458 - INFO - Checking the embedder : sentence-transformers/all-MiniLM-L6-v2 and the current llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:39:22,473 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:39:22,488 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:39:22,543 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:39:22,561 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 15:39:23,336 - INFO - collection is admin_test_class_7, and client: <weaviate.client.WeaviateClient object at 0x7f22d69f59c0>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 15:39:23,338 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:39:23,340 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:39:23,341 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22d693cee0>
2024-06-19 15:39:23,342 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22d693cee0>
2024-06-19 15:39:23,342 - INFO - Checkpoint RAG
2024-06-19 15:39:23,343 - INFO - Checking prompt before RAG: input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))] and checking if retriever exists: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22d693cee0>, checking llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256, 'stop': ['.']}
2024-06-19 15:39:23,343 - INFO - logging the rag chain: first={
  context: VectorStoreRetriever(tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'], vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22d693cee0>),
  question: RunnablePassthrough()
} middle=[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))]), VLLMOpenAI(client=<openai.resources.completions.Completions object at 0x7f22dcc8add0>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7f22d6b1e980>, model_name='meta-llama/Llama-2-7b-chat-hf', model_kwargs={'stop': ['.']}, openai_api_key='EMPTY', openai_api_base='http://localhost:8500/v1', openai_proxy='', streaming=True)] last=StrOutputParser() and retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7f22d693cee0>
2024-06-19 15:39:23,384 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 15:39:25,053 - INFO - Checking the response of do rag:  The participating teams in the UEFA Euro 2024 tournament are Group A, Group B, Group C, and so on
2024-06-19 15:39:25,055 - INFO - Checking the response of do rag:  The participating teams in the UEFA Euro 2024 tournament are Group A, Group B, Group C, and so on
2024-06-19 15:42:33,112 - INFO - At initilization stage embedding model is: None and vllm model is: None
2024-06-19 15:43:08,884 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:43:08,901 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:43:08,959 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:43:09,363 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7fb4a7a8a3b0>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 15:43:09,364 - INFO - Checking the embedder : sentence-transformers/all-MiniLM-L6-v2 and the current llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 15:43:09,379 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:43:09,396 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:43:09,455 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:43:09,473 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 15:43:10,390 - INFO - collection is admin_test_class_7, and client: <weaviate.client.WeaviateClient object at 0x7fb3dc2c0bb0>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 15:43:10,392 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:43:10,394 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:43:10,395 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fb3dc2f4160>
2024-06-19 15:43:10,395 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fb3dc2f4160>
2024-06-19 15:43:10,395 - INFO - Checkpoint RAG
2024-06-19 15:43:10,396 - INFO - Checking prompt before RAG: input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))] and checking if retriever exists: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fb3dc2f4160>, checking llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 15:43:10,397 - INFO - logging the rag chain: first={
  context: VectorStoreRetriever(tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'], vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fb3dc2f4160>),
  question: RunnablePassthrough()
} middle=[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))]), VLLMOpenAI(client=<openai.resources.completions.Completions object at 0x7fb3de04a7a0>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7fb3dc4a67a0>, model_name='meta-llama/Llama-2-7b-chat-hf', max_tokens=1084, model_kwargs={'stop': ['.']}, openai_api_key='EMPTY', openai_api_base='http://localhost:8500/v1', openai_proxy='')] last=StrOutputParser() and retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fb3dc2f4160>
2024-06-19 15:43:12,982 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 15:43:12,994 - INFO - Checking the response of do rag:  The participating teams in the UEFA Euro 2024 tournament are Group A, Group B, Group C, Group D, Group E, Group F, and Group G
2024-06-19 15:43:12,995 - INFO - Checking the response of do rag:  The participating teams in the UEFA Euro 2024 tournament are Group A, Group B, Group C, Group D, Group E, Group F, and Group G
2024-06-19 15:51:11,354 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:51:11,370 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:51:11,431 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:51:11,491 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7fb3dc6e24d0>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 15:51:11,491 - INFO - Checking the embedder : sentence-transformers/all-MiniLM-L6-v2 and the current llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 15:51:11,518 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:51:11,543 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:51:11,602 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:51:11,620 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 15:51:12,417 - INFO - collection is admin_test_class_7, and client: <weaviate.client.WeaviateClient object at 0x7fb3b57837f0>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 15:51:12,419 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:51:12,421 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:51:12,421 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fb3dc18f940>
2024-06-19 15:51:12,423 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fb3dc18f940>
2024-06-19 15:51:12,423 - INFO - Checkpoint RAG
2024-06-19 15:51:12,424 - INFO - Checking prompt before RAG: input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))] and checking if retriever exists: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fb3dc18f940>, checking llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 15:51:12,424 - INFO - logging the rag chain: first={
  context: VectorStoreRetriever(tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'], vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fb3dc18f940>),
  question: RunnablePassthrough()
} middle=[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))]), VLLMOpenAI(client=<openai.resources.completions.Completions object at 0x7fb4a7a8beb0>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7fb3b5781ab0>, model_name='meta-llama/Llama-2-7b-chat-hf', max_tokens=1084, model_kwargs={'stop': ['.']}, openai_api_key='EMPTY', openai_api_base='http://localhost:8500/v1', openai_proxy='')] last=StrOutputParser() and retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fb3dc18f940>
2024-06-19 15:51:14,445 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 15:51:14,447 - INFO - Checking the response of do rag:  The stadiums for the UEFA Euro 2024 are Berlin, Cologne, Dortmund, Hamburg, Leipzig, Munich, Stuttgart, and Zurich
2024-06-19 15:51:14,448 - INFO - Checking the response of do rag:  The stadiums for the UEFA Euro 2024 are Berlin, Cologne, Dortmund, Hamburg, Leipzig, Munich, Stuttgart, and Zurich
2024-06-19 15:52:59,803 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:52:59,819 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:52:59,883 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:52:59,930 - INFO - check the success init status: <backend_vectordatabase.VLLMManager object at 0x7fb3de114d00>, and the current llm : [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 15:52:59,930 - INFO - Checking the embedder : sentence-transformers/all-MiniLM-L6-v2 and the current llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 15:52:59,945 - INFO - HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
2024-06-19 15:52:59,961 - INFO - HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
2024-06-19 15:53:00,018 - INFO - HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
2024-06-19 15:53:00,042 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-06-19 15:53:00,708 - INFO - collection is admin_test_class_7, and client: <weaviate.client.WeaviateClient object at 0x7fb3b582c640>, and embedder: client=SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
) model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': False} multi_process=False show_progress=False 
2024-06-19 15:53:00,710 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:53:00,712 - INFO - HTTP Request: GET http://localhost:8080/v1/schema/Admin_test_class_7 "HTTP/1.1 200 OK"
2024-06-19 15:53:00,712 - INFO - checking the vectorstore 2 : <langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fb3dc6e1ae0>
2024-06-19 15:53:00,714 - INFO - Check the retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fb3dc6e1ae0>
2024-06-19 15:53:00,714 - INFO - Checkpoint RAG
2024-06-19 15:53:00,714 - INFO - Checking prompt before RAG: input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))] and checking if retriever exists: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fb3dc6e1ae0>, checking llm: [1mVLLMOpenAI[0m
Params: {'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 1084, 'stop': ['.']}
2024-06-19 15:53:00,715 - INFO - logging the rag chain: first={
  context: VectorStoreRetriever(tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'], vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fb3dc6e1ae0>),
  question: RunnablePassthrough()
} middle=[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. \n        Use the following pieces of retrieved context to answer the question. \n        If you don't know the answer, just say that you don't know. \n        Use five sentences maximum and keep the answer concise.\n        Question: {question} \n        Context: {context} \n        Answer:\n        "))]), VLLMOpenAI(client=<openai.resources.completions.Completions object at 0x7fb3b5783730>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7fb3dc6e1ff0>, model_name='meta-llama/Llama-2-7b-chat-hf', max_tokens=1084, model_kwargs={'stop': ['.']}, openai_api_key='EMPTY', openai_api_base='http://localhost:8500/v1', openai_proxy='')] last=StrOutputParser() and retriever: tags=['WeaviateVectorStore', 'HuggingFaceEmbeddings'] vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x7fb3dc6e1ae0>
2024-06-19 15:53:06,194 - INFO - HTTP Request: POST http://localhost:8500/v1/completions "HTTP/1.1 200 OK"
2024-06-19 15:53:06,197 - INFO - Checking the response of do rag:  The UEFA Euro 2024 tournament will be hosted in 10 different cities across Germany, including:
         - Dortmund
         - DÃ¼sseldorf
         - Frankfurt
         - Gelsenkirchen
         - Hamburg
         - Munich
         - Berlin
         - Cologne
         - Stuttgart
         - Leipzig

I don't know the exact number of venues, as the context doesn't provide that information
2024-06-19 15:53:06,199 - INFO - Checking the response of do rag:  The UEFA Euro 2024 tournament will be hosted in 10 different cities across Germany, including:
         - Dortmund
         - DÃ¼sseldorf
         - Frankfurt
         - Gelsenkirchen
         - Hamburg
         - Munich
         - Berlin
         - Cologne
         - Stuttgart
         - Leipzig

I don't know the exact number of venues, as the context doesn't provide that information
