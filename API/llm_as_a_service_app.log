2024-06-20 08:52:32,526 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:52:32,526 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:52:32,526 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:52:32,526 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:52:32,526 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:52:32,526 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:52:32,526 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:52:37,534 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5a5240>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:52:37,534 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5a5240>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:52:37,534 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5a5240>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:52:37,534 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5a5240>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:52:37,534 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5a5240>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:52:37,534 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5a5240>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:52:37,534 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5a5240>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:52:42,542 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5a7cd0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:52:42,542 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5a7cd0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:52:42,542 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5a7cd0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:52:42,542 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5a7cd0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:52:42,542 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5a7cd0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:52:42,542 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5a7cd0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:52:42,542 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5a7cd0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:52:47,549 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5d4490>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:52:47,549 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5d4490>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:52:47,549 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5d4490>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:52:47,549 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5d4490>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:52:47,549 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5d4490>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:52:47,549 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5d4490>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:52:47,549 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5d4490>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:52:52,557 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5d4c70>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:52:52,557 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5d4c70>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:52:52,557 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5d4c70>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:52:52,557 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5d4c70>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:52:52,557 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5d4c70>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:52:52,557 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5d4c70>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:52:52,557 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5d4c70>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:52:57,565 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5d5450>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:52:57,565 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5d5450>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:52:57,565 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5d5450>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:52:57,565 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5d5450>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:52:57,565 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5d5450>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:52:57,565 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5d5450>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:52:57,565 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5d5450>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:53:02,569 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5a7400>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:53:02,569 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5a7400>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:53:02,569 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5a7400>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:53:02,569 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5a7400>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:53:02,569 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5a7400>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:53:02,569 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5a7400>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:53:02,569 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5a7400>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:53:07,577 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5a7a60>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:53:07,577 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5a7a60>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:53:07,577 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5a7a60>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:53:07,577 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5a7a60>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:53:07,577 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5a7a60>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:53:07,577 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5a7a60>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:53:07,577 - llm_as_a_service_logger - ERROR - Health check error: HTTPConnectionPool(host='localhost', port=8500): Max retries exceeded with url: /metrics (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f720b5a7a60>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-06-20 08:53:20,878 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
2024-06-20 08:53:20,878 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
2024-06-20 08:53:20,878 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
2024-06-20 08:53:20,878 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
2024-06-20 08:53:20,878 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
2024-06-20 08:53:20,878 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
2024-06-20 08:53:20,878 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
2024-06-20 08:53:25,886 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:25,886 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:25,886 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:25,886 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:25,886 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:25,886 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:25,886 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:30,896 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:30,896 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:30,896 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:30,896 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:30,896 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:30,896 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:30,896 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:35,904 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:35,904 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:35,904 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:35,904 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:35,904 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:35,904 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:35,904 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:40,913 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:40,913 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:40,913 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:40,913 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:40,913 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:40,913 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:40,913 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:45,922 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:45,922 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:45,922 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:45,922 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:45,922 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:45,922 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:45,922 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:50,932 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:50,932 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:50,932 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:50,932 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:50,932 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:50,932 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:50,932 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:55,942 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:55,942 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:55,942 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:55,942 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:55,942 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:55,942 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:53:55,942 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:54:00,951 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:54:00,951 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:54:00,951 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:54:00,951 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:54:00,951 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:54:00,951 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:54:00,951 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:54:05,960 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:54:05,960 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:54:05,960 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:54:05,960 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:54:05,960 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:54:05,960 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:54:05,960 - llm_as_a_service_logger - ERROR - Health check error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
2024-06-20 08:54:10,975 - llm_as_a_service_logger - INFO - Container started successfully. Port: 8500
2024-06-20 08:54:10,975 - llm_as_a_service_logger - INFO - Container started successfully. Port: 8500
2024-06-20 08:54:10,975 - llm_as_a_service_logger - INFO - Container started successfully. Port: 8500
2024-06-20 08:54:10,975 - llm_as_a_service_logger - INFO - Container started successfully. Port: 8500
2024-06-20 08:54:10,975 - llm_as_a_service_logger - INFO - Container started successfully. Port: 8500
2024-06-20 08:54:10,975 - llm_as_a_service_logger - INFO - Container started successfully. Port: 8500
2024-06-20 08:54:10,975 - llm_as_a_service_logger - INFO - Container started successfully. Port: 8500
2024-06-20 08:54:10,976 - llm_as_a_service_logger - INFO - User info: Amin-nsclae
2024-06-20 08:54:10,976 - llm_as_a_service_logger - INFO - User info: Amin-nsclae
2024-06-20 08:54:10,976 - llm_as_a_service_logger - INFO - User info: Amin-nsclae
2024-06-20 08:54:10,976 - llm_as_a_service_logger - INFO - User info: Amin-nsclae
2024-06-20 08:54:10,976 - llm_as_a_service_logger - INFO - User info: Amin-nsclae
2024-06-20 08:54:10,976 - llm_as_a_service_logger - INFO - User info: Amin-nsclae
2024-06-20 08:54:10,976 - llm_as_a_service_logger - INFO - User info: Amin-nsclae
2024-06-20 08:54:10,976 - llm_as_a_service_logger - INFO - Container ID: 59c6c255f0462e0c11a6da193b8063b7f77ad0f41f5cd97ccef7d3d2ed6036c3
2024-06-20 08:54:10,976 - llm_as_a_service_logger - INFO - Container ID: 59c6c255f0462e0c11a6da193b8063b7f77ad0f41f5cd97ccef7d3d2ed6036c3
2024-06-20 08:54:10,976 - llm_as_a_service_logger - INFO - Container ID: 59c6c255f0462e0c11a6da193b8063b7f77ad0f41f5cd97ccef7d3d2ed6036c3
2024-06-20 08:54:10,976 - llm_as_a_service_logger - INFO - Container ID: 59c6c255f0462e0c11a6da193b8063b7f77ad0f41f5cd97ccef7d3d2ed6036c3
2024-06-20 08:54:10,976 - llm_as_a_service_logger - INFO - Container ID: 59c6c255f0462e0c11a6da193b8063b7f77ad0f41f5cd97ccef7d3d2ed6036c3
2024-06-20 08:54:10,976 - llm_as_a_service_logger - INFO - Container ID: 59c6c255f0462e0c11a6da193b8063b7f77ad0f41f5cd97ccef7d3d2ed6036c3
2024-06-20 08:54:10,976 - llm_as_a_service_logger - INFO - Container ID: 59c6c255f0462e0c11a6da193b8063b7f77ad0f41f5cd97ccef7d3d2ed6036c3
2024-06-20 08:54:28,735 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user admin
2024-06-20 08:54:28,735 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user admin
2024-06-20 08:54:28,735 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user admin
2024-06-20 08:54:28,735 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user admin
2024-06-20 08:54:28,735 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user admin
2024-06-20 08:54:28,735 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user admin
2024-06-20 08:54:28,735 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user admin
2024-06-20 08:54:28,735 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user admin
2024-06-20 08:54:54,878 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 08:54:54,878 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 08:54:54,878 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 08:54:54,878 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 08:54:54,878 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 08:54:54,878 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 08:54:54,878 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 08:54:54,878 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 08:55:02,083 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:55:02,083 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:55:02,083 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:55:02,083 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:55:02,083 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:55:02,083 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:55:02,083 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:55:02,083 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:55:02,085 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:55:02,085 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:55:02,085 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:55:02,085 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:55:02,085 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:55:02,085 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:55:02,085 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:55:02,085 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:55:02,578 - llm_as_a_service_logger - ERROR - Error in Inference Call: '<=' not supported between instances of 'NoneType' and 'int'
2024-06-20 08:55:02,578 - llm_as_a_service_logger - ERROR - Error in Inference Call: '<=' not supported between instances of 'NoneType' and 'int'
2024-06-20 08:55:02,578 - llm_as_a_service_logger - ERROR - Error in Inference Call: '<=' not supported between instances of 'NoneType' and 'int'
2024-06-20 08:55:02,578 - llm_as_a_service_logger - ERROR - Error in Inference Call: '<=' not supported between instances of 'NoneType' and 'int'
2024-06-20 08:55:02,578 - llm_as_a_service_logger - ERROR - Error in Inference Call: '<=' not supported between instances of 'NoneType' and 'int'
2024-06-20 08:55:02,578 - llm_as_a_service_logger - ERROR - Error in Inference Call: '<=' not supported between instances of 'NoneType' and 'int'
2024-06-20 08:55:02,578 - llm_as_a_service_logger - ERROR - Error in Inference Call: '<=' not supported between instances of 'NoneType' and 'int'
2024-06-20 08:55:02,578 - llm_as_a_service_logger - ERROR - Error in Inference Call: '<=' not supported between instances of 'NoneType' and 'int'
2024-06-20 08:55:18,617 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:55:18,617 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:55:18,617 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:55:18,617 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:55:18,617 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:55:18,617 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:55:18,617 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:55:18,617 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:55:18,619 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:55:18,619 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:55:18,619 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:55:18,619 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:55:18,619 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:55:18,619 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:55:18,619 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:55:18,619 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:55:18,640 - llm_as_a_service_logger - ERROR - Error in Inference Call: '<=' not supported between instances of 'NoneType' and 'int'
2024-06-20 08:55:18,640 - llm_as_a_service_logger - ERROR - Error in Inference Call: '<=' not supported between instances of 'NoneType' and 'int'
2024-06-20 08:55:18,640 - llm_as_a_service_logger - ERROR - Error in Inference Call: '<=' not supported between instances of 'NoneType' and 'int'
2024-06-20 08:55:18,640 - llm_as_a_service_logger - ERROR - Error in Inference Call: '<=' not supported between instances of 'NoneType' and 'int'
2024-06-20 08:55:18,640 - llm_as_a_service_logger - ERROR - Error in Inference Call: '<=' not supported between instances of 'NoneType' and 'int'
2024-06-20 08:55:18,640 - llm_as_a_service_logger - ERROR - Error in Inference Call: '<=' not supported between instances of 'NoneType' and 'int'
2024-06-20 08:55:18,640 - llm_as_a_service_logger - ERROR - Error in Inference Call: '<=' not supported between instances of 'NoneType' and 'int'
2024-06-20 08:55:18,640 - llm_as_a_service_logger - ERROR - Error in Inference Call: '<=' not supported between instances of 'NoneType' and 'int'
2024-06-20 08:55:59,242 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 08:55:59,242 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 08:55:59,242 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 08:55:59,242 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 08:55:59,242 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 08:55:59,242 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 08:55:59,242 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 08:55:59,242 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 08:56:11,353 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'hi. my name is amin', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:56:11,353 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'hi. my name is amin', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:56:11,353 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'hi. my name is amin', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:56:11,353 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'hi. my name is amin', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:56:11,353 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'hi. my name is amin', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:56:11,353 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'hi. my name is amin', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:56:11,353 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'hi. my name is amin', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:56:11,353 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'hi. my name is amin', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:56:11,354 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'hi. my name is amin', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:56:11,354 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'hi. my name is amin', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:56:11,354 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'hi. my name is amin', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:56:11,354 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'hi. my name is amin', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:56:11,354 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'hi. my name is amin', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:56:11,354 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'hi. my name is amin', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:56:11,354 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'hi. my name is amin', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:56:11,354 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'hi. my name is amin', 'memory': True, 'conversation_number': None, 'username': 'amin'}
2024-06-20 08:56:11,374 - llm_as_a_service_logger - ERROR - Error in Inference Call: '<=' not supported between instances of 'NoneType' and 'int'
2024-06-20 08:56:11,374 - llm_as_a_service_logger - ERROR - Error in Inference Call: '<=' not supported between instances of 'NoneType' and 'int'
2024-06-20 08:56:11,374 - llm_as_a_service_logger - ERROR - Error in Inference Call: '<=' not supported between instances of 'NoneType' and 'int'
2024-06-20 08:56:11,374 - llm_as_a_service_logger - ERROR - Error in Inference Call: '<=' not supported between instances of 'NoneType' and 'int'
2024-06-20 08:56:11,374 - llm_as_a_service_logger - ERROR - Error in Inference Call: '<=' not supported between instances of 'NoneType' and 'int'
2024-06-20 08:56:11,374 - llm_as_a_service_logger - ERROR - Error in Inference Call: '<=' not supported between instances of 'NoneType' and 'int'
2024-06-20 08:56:11,374 - llm_as_a_service_logger - ERROR - Error in Inference Call: '<=' not supported between instances of 'NoneType' and 'int'
2024-06-20 08:56:11,374 - llm_as_a_service_logger - ERROR - Error in Inference Call: '<=' not supported between instances of 'NoneType' and 'int'
2024-06-20 08:56:52,117 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi. May name is Amin.', 'memory': True, 'conversation_number': 3, 'username': 'amin'}
2024-06-20 08:56:52,117 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi. May name is Amin.', 'memory': True, 'conversation_number': 3, 'username': 'amin'}
2024-06-20 08:56:52,117 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi. May name is Amin.', 'memory': True, 'conversation_number': 3, 'username': 'amin'}
2024-06-20 08:56:52,117 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi. May name is Amin.', 'memory': True, 'conversation_number': 3, 'username': 'amin'}
2024-06-20 08:56:52,117 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi. May name is Amin.', 'memory': True, 'conversation_number': 3, 'username': 'amin'}
2024-06-20 08:56:52,117 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi. May name is Amin.', 'memory': True, 'conversation_number': 3, 'username': 'amin'}
2024-06-20 08:56:52,117 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi. May name is Amin.', 'memory': True, 'conversation_number': 3, 'username': 'amin'}
2024-06-20 08:56:52,117 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi. May name is Amin.', 'memory': True, 'conversation_number': 3, 'username': 'amin'}
2024-06-20 08:56:52,118 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi. May name is Amin.', 'memory': True, 'conversation_number': 3, 'username': 'amin'}
2024-06-20 08:56:52,118 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi. May name is Amin.', 'memory': True, 'conversation_number': 3, 'username': 'amin'}
2024-06-20 08:56:52,118 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi. May name is Amin.', 'memory': True, 'conversation_number': 3, 'username': 'amin'}
2024-06-20 08:56:52,118 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi. May name is Amin.', 'memory': True, 'conversation_number': 3, 'username': 'amin'}
2024-06-20 08:56:52,118 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi. May name is Amin.', 'memory': True, 'conversation_number': 3, 'username': 'amin'}
2024-06-20 08:56:52,118 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi. May name is Amin.', 'memory': True, 'conversation_number': 3, 'username': 'amin'}
2024-06-20 08:56:52,118 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi. May name is Amin.', 'memory': True, 'conversation_number': 3, 'username': 'amin'}
2024-06-20 08:56:52,118 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi. May name is Amin.', 'memory': True, 'conversation_number': 3, 'username': 'amin'}
2024-06-20 08:56:52,139 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 08:56:52,139 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 08:56:52,139 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 08:56:52,139 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 08:56:52,139 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 08:56:52,139 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 08:56:52,139 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 08:56:52,139 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 08:56:52,140 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f8dbce7b3a0>
2024-06-20 08:56:52,140 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f8dbce7b3a0>
2024-06-20 08:56:52,140 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f8dbce7b3a0>
2024-06-20 08:56:52,140 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f8dbce7b3a0>
2024-06-20 08:56:52,140 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f8dbce7b3a0>
2024-06-20 08:56:52,140 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f8dbce7b3a0>
2024-06-20 08:56:52,140 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f8dbce7b3a0>
2024-06-20 08:56:52,140 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f8dbce7b3a0>
2024-06-20 08:56:52,143 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f8dbce7b910>
2024-06-20 08:56:52,143 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f8dbce7b910>
2024-06-20 08:56:52,143 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f8dbce7b910>
2024-06-20 08:56:52,143 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f8dbce7b910>
2024-06-20 08:56:52,143 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f8dbce7b910>
2024-06-20 08:56:52,143 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f8dbce7b910>
2024-06-20 08:56:52,143 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f8dbce7b910>
2024-06-20 08:56:52,143 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f8dbce7b910>
2024-06-20 08:56:52,143 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 08:56:52,143 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 08:56:52,143 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 08:56:52,143 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 08:56:52,143 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 08:56:52,143 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 08:56:52,143 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 08:56:52,143 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 08:57:55,068 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user admin
2024-06-20 08:57:55,068 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user admin
2024-06-20 08:57:55,068 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user admin
2024-06-20 08:57:55,068 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user admin
2024-06-20 08:57:55,068 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user admin
2024-06-20 08:57:55,068 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user admin
2024-06-20 08:57:55,068 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user admin
2024-06-20 08:58:22,961 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 08:58:22,961 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 08:58:22,961 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 08:58:22,961 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 08:58:22,961 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 08:58:22,961 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 08:58:22,961 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 08:58:40,957 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi. my name is Amin.', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 08:58:40,957 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi. my name is Amin.', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 08:58:40,957 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi. my name is Amin.', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 08:58:40,957 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi. my name is Amin.', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 08:58:40,957 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi. my name is Amin.', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 08:58:40,957 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi. my name is Amin.', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 08:58:40,957 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi. my name is Amin.', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 08:58:40,957 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi. my name is Amin.', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 08:58:40,957 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi. my name is Amin.', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 08:58:40,957 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi. my name is Amin.', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 08:58:40,957 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi. my name is Amin.', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 08:58:40,957 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi. my name is Amin.', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 08:58:40,957 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi. my name is Amin.', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 08:58:40,957 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi. my name is Amin.', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 08:58:41,222 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 08:58:41,222 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 08:58:41,222 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 08:58:41,222 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 08:58:41,222 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 08:58:41,222 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 08:58:41,222 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 08:58:41,230 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7fe512033fd0>
2024-06-20 08:58:41,230 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7fe512033fd0>
2024-06-20 08:58:41,230 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7fe512033fd0>
2024-06-20 08:58:41,230 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7fe512033fd0>
2024-06-20 08:58:41,230 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7fe512033fd0>
2024-06-20 08:58:41,230 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7fe512033fd0>
2024-06-20 08:58:41,230 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7fe512033fd0>
2024-06-20 08:58:41,233 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7fe513f6f100>
2024-06-20 08:58:41,233 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7fe513f6f100>
2024-06-20 08:58:41,233 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7fe513f6f100>
2024-06-20 08:58:41,233 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7fe513f6f100>
2024-06-20 08:58:41,233 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7fe513f6f100>
2024-06-20 08:58:41,233 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7fe513f6f100>
2024-06-20 08:58:41,233 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7fe513f6f100>
2024-06-20 08:58:41,233 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 08:58:41,233 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 08:58:41,233 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 08:58:41,233 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 08:58:41,233 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 08:58:41,233 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 08:58:41,233 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 08:58:58,952 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of the UK?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 08:58:58,952 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of the UK?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 08:58:58,952 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of the UK?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 08:58:58,952 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of the UK?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 08:58:58,952 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of the UK?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 08:58:58,952 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of the UK?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 08:58:58,952 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of the UK?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 08:58:58,954 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of the UK?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 08:58:58,954 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of the UK?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 08:58:58,954 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of the UK?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 08:58:58,954 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of the UK?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 08:58:58,954 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of the UK?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 08:58:58,954 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of the UK?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 08:58:58,954 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of the UK?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 08:58:58,976 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 08:58:58,976 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 08:58:58,976 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 08:58:58,976 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 08:58:58,976 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 08:58:58,976 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 08:58:58,976 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 08:58:58,977 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7fe511fe6800>
2024-06-20 08:58:58,977 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7fe511fe6800>
2024-06-20 08:58:58,977 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7fe511fe6800>
2024-06-20 08:58:58,977 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7fe511fe6800>
2024-06-20 08:58:58,977 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7fe511fe6800>
2024-06-20 08:58:58,977 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7fe511fe6800>
2024-06-20 08:58:58,977 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7fe511fe6800>
2024-06-20 08:58:58,978 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7fe511fe6a40>
2024-06-20 08:58:58,978 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7fe511fe6a40>
2024-06-20 08:58:58,978 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7fe511fe6a40>
2024-06-20 08:58:58,978 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7fe511fe6a40>
2024-06-20 08:58:58,978 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7fe511fe6a40>
2024-06-20 08:58:58,978 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7fe511fe6a40>
2024-06-20 08:58:58,978 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7fe511fe6a40>
2024-06-20 08:58:58,979 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 08:58:58,979 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 08:58:58,979 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 08:58:58,979 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 08:58:58,979 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 08:58:58,979 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 08:58:58,979 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:05:10,822 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'where is the capital of France?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:05:10,822 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'where is the capital of France?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:05:10,822 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'where is the capital of France?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:05:10,822 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'where is the capital of France?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:05:10,822 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'where is the capital of France?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:05:10,822 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'where is the capital of France?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:05:10,822 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'where is the capital of France?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:05:10,822 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'where is the capital of France?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:05:10,822 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'where is the capital of France?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:05:10,822 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'where is the capital of France?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:05:10,822 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'where is the capital of France?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:05:10,822 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'where is the capital of France?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:05:10,822 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'where is the capital of France?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:05:10,822 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'where is the capital of France?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:05:11,090 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:05:11,090 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:05:11,090 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:05:11,090 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:05:11,090 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:05:11,090 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:05:11,090 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:05:11,099 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e8350ffd0>
2024-06-20 09:05:11,099 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e8350ffd0>
2024-06-20 09:05:11,099 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e8350ffd0>
2024-06-20 09:05:11,099 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e8350ffd0>
2024-06-20 09:05:11,099 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e8350ffd0>
2024-06-20 09:05:11,099 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e8350ffd0>
2024-06-20 09:05:11,099 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e8350ffd0>
2024-06-20 09:05:11,102 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e8350e140>
2024-06-20 09:05:11,102 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e8350e140>
2024-06-20 09:05:11,102 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e8350e140>
2024-06-20 09:05:11,102 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e8350e140>
2024-06-20 09:05:11,102 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e8350e140>
2024-06-20 09:05:11,102 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e8350e140>
2024-06-20 09:05:11,102 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e8350e140>
2024-06-20 09:05:11,102 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:05:11,102 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:05:11,102 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:05:11,102 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:05:11,102 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:05:11,102 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:05:11,102 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:06:07,520 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of Iran', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:06:07,520 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of Iran', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:06:07,520 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of Iran', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:06:07,520 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of Iran', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:06:07,520 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of Iran', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:06:07,520 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of Iran', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:06:07,520 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of Iran', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:06:07,521 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of Iran', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:06:07,521 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of Iran', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:06:07,521 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of Iran', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:06:07,521 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of Iran', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:06:07,521 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of Iran', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:06:07,521 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of Iran', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:06:07,521 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of Iran', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:06:07,543 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:06:07,543 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:06:07,543 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:06:07,543 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:06:07,543 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:06:07,543 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:06:07,543 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:06:07,545 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e832f2800>
2024-06-20 09:06:07,545 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e832f2800>
2024-06-20 09:06:07,545 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e832f2800>
2024-06-20 09:06:07,545 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e832f2800>
2024-06-20 09:06:07,545 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e832f2800>
2024-06-20 09:06:07,545 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e832f2800>
2024-06-20 09:06:07,545 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e832f2800>
2024-06-20 09:06:07,546 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e832f2a40>
2024-06-20 09:06:07,546 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e832f2a40>
2024-06-20 09:06:07,546 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e832f2a40>
2024-06-20 09:06:07,546 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e832f2a40>
2024-06-20 09:06:07,546 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e832f2a40>
2024-06-20 09:06:07,546 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e832f2a40>
2024-06-20 09:06:07,546 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e832f2a40>
2024-06-20 09:06:07,548 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:06:07,548 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:06:07,548 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:06:07,548 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:06:07,548 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:06:07,548 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:06:07,548 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:06:24,415 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of US?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:06:24,415 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of US?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:06:24,415 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of US?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:06:24,415 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of US?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:06:24,415 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of US?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:06:24,415 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of US?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:06:24,415 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of US?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:06:24,416 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of US?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:06:24,416 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of US?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:06:24,416 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of US?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:06:24,416 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of US?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:06:24,416 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of US?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:06:24,416 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of US?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:06:24,416 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Where is the capital of US?', 'memory': True, 'conversation_number': 1, 'username': 'amin'}
2024-06-20 09:06:24,438 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:06:24,438 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:06:24,438 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:06:24,438 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:06:24,438 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:06:24,438 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:06:24,438 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:06:24,439 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e80cf8100>
2024-06-20 09:06:24,439 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e80cf8100>
2024-06-20 09:06:24,439 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e80cf8100>
2024-06-20 09:06:24,439 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e80cf8100>
2024-06-20 09:06:24,439 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e80cf8100>
2024-06-20 09:06:24,439 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e80cf8100>
2024-06-20 09:06:24,439 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e80cf8100>
2024-06-20 09:06:24,440 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e80cf8310>
2024-06-20 09:06:24,440 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e80cf8310>
2024-06-20 09:06:24,440 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e80cf8310>
2024-06-20 09:06:24,440 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e80cf8310>
2024-06-20 09:06:24,440 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e80cf8310>
2024-06-20 09:06:24,440 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e80cf8310>
2024-06-20 09:06:24,440 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e80cf8310>
2024-06-20 09:06:24,441 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:06:24,441 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:06:24,441 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:06:24,441 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:06:24,441 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:06:24,441 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:06:24,441 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:12:00,442 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is the theory of nuclear  science?', 'memory': True, 'conversation_number': 2, 'username': 'amin'}
2024-06-20 09:12:00,442 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is the theory of nuclear  science?', 'memory': True, 'conversation_number': 2, 'username': 'amin'}
2024-06-20 09:12:00,442 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is the theory of nuclear  science?', 'memory': True, 'conversation_number': 2, 'username': 'amin'}
2024-06-20 09:12:00,442 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is the theory of nuclear  science?', 'memory': True, 'conversation_number': 2, 'username': 'amin'}
2024-06-20 09:12:00,442 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is the theory of nuclear  science?', 'memory': True, 'conversation_number': 2, 'username': 'amin'}
2024-06-20 09:12:00,442 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is the theory of nuclear  science?', 'memory': True, 'conversation_number': 2, 'username': 'amin'}
2024-06-20 09:12:00,442 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is the theory of nuclear  science?', 'memory': True, 'conversation_number': 2, 'username': 'amin'}
2024-06-20 09:12:00,443 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is the theory of nuclear  science?', 'memory': True, 'conversation_number': 2, 'username': 'amin'}
2024-06-20 09:12:00,443 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is the theory of nuclear  science?', 'memory': True, 'conversation_number': 2, 'username': 'amin'}
2024-06-20 09:12:00,443 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is the theory of nuclear  science?', 'memory': True, 'conversation_number': 2, 'username': 'amin'}
2024-06-20 09:12:00,443 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is the theory of nuclear  science?', 'memory': True, 'conversation_number': 2, 'username': 'amin'}
2024-06-20 09:12:00,443 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is the theory of nuclear  science?', 'memory': True, 'conversation_number': 2, 'username': 'amin'}
2024-06-20 09:12:00,443 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is the theory of nuclear  science?', 'memory': True, 'conversation_number': 2, 'username': 'amin'}
2024-06-20 09:12:00,443 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is the theory of nuclear  science?', 'memory': True, 'conversation_number': 2, 'username': 'amin'}
2024-06-20 09:12:00,464 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:12:00,464 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:12:00,464 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:12:00,464 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:12:00,464 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:12:00,464 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:12:00,464 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:12:00,465 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e80d30100>
2024-06-20 09:12:00,465 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e80d30100>
2024-06-20 09:12:00,465 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e80d30100>
2024-06-20 09:12:00,465 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e80d30100>
2024-06-20 09:12:00,465 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e80d30100>
2024-06-20 09:12:00,465 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e80d30100>
2024-06-20 09:12:00,465 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e80d30100>
2024-06-20 09:12:00,467 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e80d30310>
2024-06-20 09:12:00,467 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e80d30310>
2024-06-20 09:12:00,467 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e80d30310>
2024-06-20 09:12:00,467 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e80d30310>
2024-06-20 09:12:00,467 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e80d30310>
2024-06-20 09:12:00,467 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e80d30310>
2024-06-20 09:12:00,467 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e80d30310>
2024-06-20 09:12:00,467 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:12:00,467 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:12:00,467 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:12:00,467 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:12:00,467 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:12:00,467 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:12:00,467 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:12:32,568 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 09:12:32,568 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 09:12:32,568 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 09:12:32,568 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 09:12:32,568 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 09:12:32,568 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 09:12:32,568 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 09:12:49,818 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is the theoru of solar panel?', 'memory': True, 'conversation_number': 2, 'username': 'amin'}
2024-06-20 09:12:49,818 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is the theoru of solar panel?', 'memory': True, 'conversation_number': 2, 'username': 'amin'}
2024-06-20 09:12:49,818 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is the theoru of solar panel?', 'memory': True, 'conversation_number': 2, 'username': 'amin'}
2024-06-20 09:12:49,818 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is the theoru of solar panel?', 'memory': True, 'conversation_number': 2, 'username': 'amin'}
2024-06-20 09:12:49,818 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is the theoru of solar panel?', 'memory': True, 'conversation_number': 2, 'username': 'amin'}
2024-06-20 09:12:49,818 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is the theoru of solar panel?', 'memory': True, 'conversation_number': 2, 'username': 'amin'}
2024-06-20 09:12:49,818 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is the theoru of solar panel?', 'memory': True, 'conversation_number': 2, 'username': 'amin'}
2024-06-20 09:12:49,819 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is the theoru of solar panel?', 'memory': True, 'conversation_number': 2, 'username': 'amin'}
2024-06-20 09:12:49,819 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is the theoru of solar panel?', 'memory': True, 'conversation_number': 2, 'username': 'amin'}
2024-06-20 09:12:49,819 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is the theoru of solar panel?', 'memory': True, 'conversation_number': 2, 'username': 'amin'}
2024-06-20 09:12:49,819 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is the theoru of solar panel?', 'memory': True, 'conversation_number': 2, 'username': 'amin'}
2024-06-20 09:12:49,819 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is the theoru of solar panel?', 'memory': True, 'conversation_number': 2, 'username': 'amin'}
2024-06-20 09:12:49,819 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is the theoru of solar panel?', 'memory': True, 'conversation_number': 2, 'username': 'amin'}
2024-06-20 09:12:49,819 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is the theoru of solar panel?', 'memory': True, 'conversation_number': 2, 'username': 'amin'}
2024-06-20 09:12:49,840 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:12:49,840 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:12:49,840 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:12:49,840 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:12:49,840 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:12:49,840 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:12:49,840 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:12:49,841 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e80d5ebc0>
2024-06-20 09:12:49,841 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e80d5ebc0>
2024-06-20 09:12:49,841 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e80d5ebc0>
2024-06-20 09:12:49,841 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e80d5ebc0>
2024-06-20 09:12:49,841 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e80d5ebc0>
2024-06-20 09:12:49,841 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e80d5ebc0>
2024-06-20 09:12:49,841 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e80d5ebc0>
2024-06-20 09:12:49,842 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e80d5edd0>
2024-06-20 09:12:49,842 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e80d5edd0>
2024-06-20 09:12:49,842 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e80d5edd0>
2024-06-20 09:12:49,842 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e80d5edd0>
2024-06-20 09:12:49,842 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e80d5edd0>
2024-06-20 09:12:49,842 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e80d5edd0>
2024-06-20 09:12:49,842 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e80d5edd0>
2024-06-20 09:12:49,843 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:12:49,843 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:12:49,843 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:12:49,843 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:12:49,843 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:12:49,843 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:12:49,843 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:18:10,897 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is theory on nuclear energy?', 'memory': True, 'conversation_number': 3, 'username': 'amin'}
2024-06-20 09:18:10,897 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is theory on nuclear energy?', 'memory': True, 'conversation_number': 3, 'username': 'amin'}
2024-06-20 09:18:10,897 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is theory on nuclear energy?', 'memory': True, 'conversation_number': 3, 'username': 'amin'}
2024-06-20 09:18:10,897 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is theory on nuclear energy?', 'memory': True, 'conversation_number': 3, 'username': 'amin'}
2024-06-20 09:18:10,897 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is theory on nuclear energy?', 'memory': True, 'conversation_number': 3, 'username': 'amin'}
2024-06-20 09:18:10,897 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is theory on nuclear energy?', 'memory': True, 'conversation_number': 3, 'username': 'amin'}
2024-06-20 09:18:10,897 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is theory on nuclear energy?', 'memory': True, 'conversation_number': 3, 'username': 'amin'}
2024-06-20 09:18:10,898 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is theory on nuclear energy?', 'memory': True, 'conversation_number': 3, 'username': 'amin'}
2024-06-20 09:18:10,898 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is theory on nuclear energy?', 'memory': True, 'conversation_number': 3, 'username': 'amin'}
2024-06-20 09:18:10,898 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is theory on nuclear energy?', 'memory': True, 'conversation_number': 3, 'username': 'amin'}
2024-06-20 09:18:10,898 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is theory on nuclear energy?', 'memory': True, 'conversation_number': 3, 'username': 'amin'}
2024-06-20 09:18:10,898 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is theory on nuclear energy?', 'memory': True, 'conversation_number': 3, 'username': 'amin'}
2024-06-20 09:18:10,898 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is theory on nuclear energy?', 'memory': True, 'conversation_number': 3, 'username': 'amin'}
2024-06-20 09:18:10,898 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is theory on nuclear energy?', 'memory': True, 'conversation_number': 3, 'username': 'amin'}
2024-06-20 09:18:10,919 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:18:10,919 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:18:10,919 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:18:10,919 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:18:10,919 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:18:10,919 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:18:10,919 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:18:10,920 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e832fa140>
2024-06-20 09:18:10,920 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e832fa140>
2024-06-20 09:18:10,920 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e832fa140>
2024-06-20 09:18:10,920 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e832fa140>
2024-06-20 09:18:10,920 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e832fa140>
2024-06-20 09:18:10,920 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e832fa140>
2024-06-20 09:18:10,920 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e832fa140>
2024-06-20 09:18:10,921 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e832fa170>
2024-06-20 09:18:10,921 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e832fa170>
2024-06-20 09:18:10,921 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e832fa170>
2024-06-20 09:18:10,921 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e832fa170>
2024-06-20 09:18:10,921 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e832fa170>
2024-06-20 09:18:10,921 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e832fa170>
2024-06-20 09:18:10,921 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e832fa170>
2024-06-20 09:18:10,921 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:18:10,921 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:18:10,921 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:18:10,921 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:18:10,921 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:18:10,921 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:18:10,921 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:18:35,176 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is theory on nuclear energy?', 'memory': True, 'conversation_number': 4, 'username': 'amin'}
2024-06-20 09:18:35,176 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is theory on nuclear energy?', 'memory': True, 'conversation_number': 4, 'username': 'amin'}
2024-06-20 09:18:35,176 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is theory on nuclear energy?', 'memory': True, 'conversation_number': 4, 'username': 'amin'}
2024-06-20 09:18:35,176 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is theory on nuclear energy?', 'memory': True, 'conversation_number': 4, 'username': 'amin'}
2024-06-20 09:18:35,176 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is theory on nuclear energy?', 'memory': True, 'conversation_number': 4, 'username': 'amin'}
2024-06-20 09:18:35,176 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is theory on nuclear energy?', 'memory': True, 'conversation_number': 4, 'username': 'amin'}
2024-06-20 09:18:35,176 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is theory on nuclear energy?', 'memory': True, 'conversation_number': 4, 'username': 'amin'}
2024-06-20 09:18:35,177 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is theory on nuclear energy?', 'memory': True, 'conversation_number': 4, 'username': 'amin'}
2024-06-20 09:18:35,177 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is theory on nuclear energy?', 'memory': True, 'conversation_number': 4, 'username': 'amin'}
2024-06-20 09:18:35,177 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is theory on nuclear energy?', 'memory': True, 'conversation_number': 4, 'username': 'amin'}
2024-06-20 09:18:35,177 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is theory on nuclear energy?', 'memory': True, 'conversation_number': 4, 'username': 'amin'}
2024-06-20 09:18:35,177 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is theory on nuclear energy?', 'memory': True, 'conversation_number': 4, 'username': 'amin'}
2024-06-20 09:18:35,177 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is theory on nuclear energy?', 'memory': True, 'conversation_number': 4, 'username': 'amin'}
2024-06-20 09:18:35,177 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is theory on nuclear energy?', 'memory': True, 'conversation_number': 4, 'username': 'amin'}
2024-06-20 09:18:35,199 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:18:35,199 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:18:35,199 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:18:35,199 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:18:35,199 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:18:35,199 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:18:35,199 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:18:35,200 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e80cf9ed0>
2024-06-20 09:18:35,200 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e80cf9ed0>
2024-06-20 09:18:35,200 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e80cf9ed0>
2024-06-20 09:18:35,200 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e80cf9ed0>
2024-06-20 09:18:35,200 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e80cf9ed0>
2024-06-20 09:18:35,200 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e80cf9ed0>
2024-06-20 09:18:35,200 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f6e80cf9ed0>
2024-06-20 09:18:35,201 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e80cfa6b0>
2024-06-20 09:18:35,201 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e80cfa6b0>
2024-06-20 09:18:35,201 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e80cfa6b0>
2024-06-20 09:18:35,201 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e80cfa6b0>
2024-06-20 09:18:35,201 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e80cfa6b0>
2024-06-20 09:18:35,201 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e80cfa6b0>
2024-06-20 09:18:35,201 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f6e80cfa6b0>
2024-06-20 09:18:35,202 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:18:35,202 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:18:35,202 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:18:35,202 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:18:35,202 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:18:35,202 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:18:35,202 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:20:54,857 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:20:54,857 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:20:54,857 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:20:54,857 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:20:54,857 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:20:54,857 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:20:54,857 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:20:54,858 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:20:54,858 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:20:54,858 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:20:54,858 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:20:54,858 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:20:54,858 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:20:54,858 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:22:46,360 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:22:46,360 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:22:46,360 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:22:46,360 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:22:46,360 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:22:46,360 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:22:46,360 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:22:46,361 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:22:46,361 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:22:46,361 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:22:46,361 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:22:46,361 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:22:46,361 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:22:46,361 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:24:33,755 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:24:33,755 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:24:33,755 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:24:33,755 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:24:33,755 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:24:33,755 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:24:33,755 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:24:33,756 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:24:33,756 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:24:33,756 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:24:33,756 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:24:33,756 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:24:33,756 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:24:33,756 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:24:34,028 - llm_as_a_service_logger - ERROR - Error in Inference Call: Completions.create() got an unexpected keyword argument 'repetition_penalty'
2024-06-20 09:24:34,028 - llm_as_a_service_logger - ERROR - Error in Inference Call: Completions.create() got an unexpected keyword argument 'repetition_penalty'
2024-06-20 09:24:34,028 - llm_as_a_service_logger - ERROR - Error in Inference Call: Completions.create() got an unexpected keyword argument 'repetition_penalty'
2024-06-20 09:24:34,028 - llm_as_a_service_logger - ERROR - Error in Inference Call: Completions.create() got an unexpected keyword argument 'repetition_penalty'
2024-06-20 09:24:34,028 - llm_as_a_service_logger - ERROR - Error in Inference Call: Completions.create() got an unexpected keyword argument 'repetition_penalty'
2024-06-20 09:24:34,028 - llm_as_a_service_logger - ERROR - Error in Inference Call: Completions.create() got an unexpected keyword argument 'repetition_penalty'
2024-06-20 09:24:34,028 - llm_as_a_service_logger - ERROR - Error in Inference Call: Completions.create() got an unexpected keyword argument 'repetition_penalty'
2024-06-20 09:25:35,123 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:25:35,123 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:25:35,123 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:25:35,123 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:25:35,123 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:25:35,123 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:25:35,123 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:25:35,125 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:25:35,125 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:25:35,125 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:25:35,125 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:25:35,125 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:25:35,125 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:25:35,125 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:25:35,397 - llm_as_a_service_logger - ERROR - Error in Inference Call: Completions.create() got an unexpected keyword argument 'repetition_penalty'
2024-06-20 09:25:35,397 - llm_as_a_service_logger - ERROR - Error in Inference Call: Completions.create() got an unexpected keyword argument 'repetition_penalty'
2024-06-20 09:25:35,397 - llm_as_a_service_logger - ERROR - Error in Inference Call: Completions.create() got an unexpected keyword argument 'repetition_penalty'
2024-06-20 09:25:35,397 - llm_as_a_service_logger - ERROR - Error in Inference Call: Completions.create() got an unexpected keyword argument 'repetition_penalty'
2024-06-20 09:25:35,397 - llm_as_a_service_logger - ERROR - Error in Inference Call: Completions.create() got an unexpected keyword argument 'repetition_penalty'
2024-06-20 09:25:35,397 - llm_as_a_service_logger - ERROR - Error in Inference Call: Completions.create() got an unexpected keyword argument 'repetition_penalty'
2024-06-20 09:25:35,397 - llm_as_a_service_logger - ERROR - Error in Inference Call: Completions.create() got an unexpected keyword argument 'repetition_penalty'
2024-06-20 09:27:38,994 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:27:38,994 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:27:38,994 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:27:38,994 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:27:38,994 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:27:38,994 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:27:38,994 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:27:38,995 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:27:38,995 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:27:38,995 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:27:38,995 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:27:38,995 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:27:38,995 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:27:38,995 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear energy in 200 words.', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:28:02,204 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear fusion in 300 words.', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:28:02,204 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear fusion in 300 words.', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:28:02,204 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear fusion in 300 words.', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:28:02,204 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear fusion in 300 words.', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:28:02,204 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear fusion in 300 words.', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:28:02,204 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear fusion in 300 words.', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:28:02,204 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear fusion in 300 words.', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:28:02,204 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear fusion in 300 words.', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:28:02,204 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear fusion in 300 words.', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:28:02,204 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear fusion in 300 words.', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:28:02,204 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear fusion in 300 words.', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:28:02,204 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear fusion in 300 words.', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:28:02,204 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear fusion in 300 words.', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:28:02,204 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Explain the science of nuclear fusion in 300 words.', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:28:19,400 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:28:19,400 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:28:19,400 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:28:19,400 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:28:19,400 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:28:19,400 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:28:19,400 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:28:19,401 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:28:19,401 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:28:19,401 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:28:19,401 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:28:19,401 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:28:19,401 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:28:19,401 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:31:39,590 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:31:39,590 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:31:39,590 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:31:39,590 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:31:39,590 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:31:39,590 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:31:39,590 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:31:39,591 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:31:39,591 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:31:39,591 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:31:39,591 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:31:39,591 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:31:39,591 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:31:39,591 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:37:22,424 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:37:22,424 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:37:22,424 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:37:22,424 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:37:22,424 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:37:22,424 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:37:22,424 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:37:22,425 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:37:22,425 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:37:22,425 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:37:22,425 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:37:22,425 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:37:22,425 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:37:22,425 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:37:22,718 - llm_as_a_service_logger - ERROR - Error in Inference Call: Error code: 400 - {'object': 'error', 'message': "This model's maximum context length is 1024 tokens. However, you requested 2040 tokens (16 in the messages, 2024 in the completion). Please reduce the length of the messages or completion.", 'type': 'BadRequestError', 'param': None, 'code': 400}
2024-06-20 09:37:22,718 - llm_as_a_service_logger - ERROR - Error in Inference Call: Error code: 400 - {'object': 'error', 'message': "This model's maximum context length is 1024 tokens. However, you requested 2040 tokens (16 in the messages, 2024 in the completion). Please reduce the length of the messages or completion.", 'type': 'BadRequestError', 'param': None, 'code': 400}
2024-06-20 09:37:22,718 - llm_as_a_service_logger - ERROR - Error in Inference Call: Error code: 400 - {'object': 'error', 'message': "This model's maximum context length is 1024 tokens. However, you requested 2040 tokens (16 in the messages, 2024 in the completion). Please reduce the length of the messages or completion.", 'type': 'BadRequestError', 'param': None, 'code': 400}
2024-06-20 09:37:22,718 - llm_as_a_service_logger - ERROR - Error in Inference Call: Error code: 400 - {'object': 'error', 'message': "This model's maximum context length is 1024 tokens. However, you requested 2040 tokens (16 in the messages, 2024 in the completion). Please reduce the length of the messages or completion.", 'type': 'BadRequestError', 'param': None, 'code': 400}
2024-06-20 09:37:22,718 - llm_as_a_service_logger - ERROR - Error in Inference Call: Error code: 400 - {'object': 'error', 'message': "This model's maximum context length is 1024 tokens. However, you requested 2040 tokens (16 in the messages, 2024 in the completion). Please reduce the length of the messages or completion.", 'type': 'BadRequestError', 'param': None, 'code': 400}
2024-06-20 09:37:22,718 - llm_as_a_service_logger - ERROR - Error in Inference Call: Error code: 400 - {'object': 'error', 'message': "This model's maximum context length is 1024 tokens. However, you requested 2040 tokens (16 in the messages, 2024 in the completion). Please reduce the length of the messages or completion.", 'type': 'BadRequestError', 'param': None, 'code': 400}
2024-06-20 09:37:22,718 - llm_as_a_service_logger - ERROR - Error in Inference Call: Error code: 400 - {'object': 'error', 'message': "This model's maximum context length is 1024 tokens. However, you requested 2040 tokens (16 in the messages, 2024 in the completion). Please reduce the length of the messages or completion.", 'type': 'BadRequestError', 'param': None, 'code': 400}
2024-06-20 09:37:55,282 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:37:55,282 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:37:55,282 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:37:55,282 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:37:55,282 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:37:55,282 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:37:55,282 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:37:55,284 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:37:55,284 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:37:55,284 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:37:55,284 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:37:55,284 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:37:55,284 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:37:55,284 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:37:55,572 - llm_as_a_service_logger - ERROR - Error in Inference Call: Error code: 400 - {'object': 'error', 'message': "This model's maximum context length is 1024 tokens. However, you requested 1040 tokens (16 in the messages, 1024 in the completion). Please reduce the length of the messages or completion.", 'type': 'BadRequestError', 'param': None, 'code': 400}
2024-06-20 09:37:55,572 - llm_as_a_service_logger - ERROR - Error in Inference Call: Error code: 400 - {'object': 'error', 'message': "This model's maximum context length is 1024 tokens. However, you requested 1040 tokens (16 in the messages, 1024 in the completion). Please reduce the length of the messages or completion.", 'type': 'BadRequestError', 'param': None, 'code': 400}
2024-06-20 09:37:55,572 - llm_as_a_service_logger - ERROR - Error in Inference Call: Error code: 400 - {'object': 'error', 'message': "This model's maximum context length is 1024 tokens. However, you requested 1040 tokens (16 in the messages, 1024 in the completion). Please reduce the length of the messages or completion.", 'type': 'BadRequestError', 'param': None, 'code': 400}
2024-06-20 09:37:55,572 - llm_as_a_service_logger - ERROR - Error in Inference Call: Error code: 400 - {'object': 'error', 'message': "This model's maximum context length is 1024 tokens. However, you requested 1040 tokens (16 in the messages, 1024 in the completion). Please reduce the length of the messages or completion.", 'type': 'BadRequestError', 'param': None, 'code': 400}
2024-06-20 09:37:55,572 - llm_as_a_service_logger - ERROR - Error in Inference Call: Error code: 400 - {'object': 'error', 'message': "This model's maximum context length is 1024 tokens. However, you requested 1040 tokens (16 in the messages, 1024 in the completion). Please reduce the length of the messages or completion.", 'type': 'BadRequestError', 'param': None, 'code': 400}
2024-06-20 09:37:55,572 - llm_as_a_service_logger - ERROR - Error in Inference Call: Error code: 400 - {'object': 'error', 'message': "This model's maximum context length is 1024 tokens. However, you requested 1040 tokens (16 in the messages, 1024 in the completion). Please reduce the length of the messages or completion.", 'type': 'BadRequestError', 'param': None, 'code': 400}
2024-06-20 09:37:55,572 - llm_as_a_service_logger - ERROR - Error in Inference Call: Error code: 400 - {'object': 'error', 'message': "This model's maximum context length is 1024 tokens. However, you requested 1040 tokens (16 in the messages, 1024 in the completion). Please reduce the length of the messages or completion.", 'type': 'BadRequestError', 'param': None, 'code': 400}
2024-06-20 09:38:13,209 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:38:13,209 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:38:13,209 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:38:13,209 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:38:13,209 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:38:13,209 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:38:13,209 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:38:13,211 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:38:13,211 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:38:13,211 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:38:13,211 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:38:13,211 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:38:13,211 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:38:13,211 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:42:50,018 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:42:50,018 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:42:50,018 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:42:50,018 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:42:50,018 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:42:50,018 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:42:50,018 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 09:42:50,019 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:42:50,019 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:42:50,019 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:42:50,019 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:42:50,019 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:42:50,019 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:42:50,019 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'in 300 words. Explain the science of nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 09:50:38,449 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 09:50:38,449 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 09:50:38,449 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 09:50:38,449 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 09:50:38,449 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 09:50:38,449 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 09:50:38,449 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user amin
2024-06-20 09:50:49,463 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': 5, 'username': 'amin'}
2024-06-20 09:50:49,463 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': 5, 'username': 'amin'}
2024-06-20 09:50:49,463 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': 5, 'username': 'amin'}
2024-06-20 09:50:49,463 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': 5, 'username': 'amin'}
2024-06-20 09:50:49,463 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': 5, 'username': 'amin'}
2024-06-20 09:50:49,463 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': 5, 'username': 'amin'}
2024-06-20 09:50:49,463 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': 5, 'username': 'amin'}
2024-06-20 09:50:49,465 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': 5, 'username': 'amin'}
2024-06-20 09:50:49,465 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': 5, 'username': 'amin'}
2024-06-20 09:50:49,465 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': 5, 'username': 'amin'}
2024-06-20 09:50:49,465 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': 5, 'username': 'amin'}
2024-06-20 09:50:49,465 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': 5, 'username': 'amin'}
2024-06-20 09:50:49,465 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': 5, 'username': 'amin'}
2024-06-20 09:50:49,465 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': 5, 'username': 'amin'}
2024-06-20 09:50:49,725 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:50:49,725 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:50:49,725 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:50:49,725 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:50:49,725 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:50:49,725 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:50:49,725 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 09:50:49,735 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7fb16acc9ae0>
2024-06-20 09:50:49,735 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7fb16acc9ae0>
2024-06-20 09:50:49,735 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7fb16acc9ae0>
2024-06-20 09:50:49,735 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7fb16acc9ae0>
2024-06-20 09:50:49,735 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7fb16acc9ae0>
2024-06-20 09:50:49,735 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7fb16acc9ae0>
2024-06-20 09:50:49,735 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7fb16acc9ae0>
2024-06-20 09:50:49,739 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7fb168daaf80>
2024-06-20 09:50:49,739 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7fb168daaf80>
2024-06-20 09:50:49,739 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7fb168daaf80>
2024-06-20 09:50:49,739 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7fb168daaf80>
2024-06-20 09:50:49,739 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7fb168daaf80>
2024-06-20 09:50:49,739 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7fb168daaf80>
2024-06-20 09:50:49,739 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7fb168daaf80>
2024-06-20 09:50:49,740 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:50:49,740 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:50:49,740 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:50:49,740 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:50:49,740 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:50:49,740 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 09:50:49,740 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 10:33:26,344 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi, my name is Amin.', 'memory': True, 'conversation_number': 6, 'username': 'amin'}
2024-06-20 10:33:26,344 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi, my name is Amin.', 'memory': True, 'conversation_number': 6, 'username': 'amin'}
2024-06-20 10:33:26,344 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi, my name is Amin.', 'memory': True, 'conversation_number': 6, 'username': 'amin'}
2024-06-20 10:33:26,344 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi, my name is Amin.', 'memory': True, 'conversation_number': 6, 'username': 'amin'}
2024-06-20 10:33:26,344 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi, my name is Amin.', 'memory': True, 'conversation_number': 6, 'username': 'amin'}
2024-06-20 10:33:26,344 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi, my name is Amin.', 'memory': True, 'conversation_number': 6, 'username': 'amin'}
2024-06-20 10:33:26,344 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi, my name is Amin.', 'memory': True, 'conversation_number': 6, 'username': 'amin'}
2024-06-20 10:33:26,345 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi, my name is Amin.', 'memory': True, 'conversation_number': 6, 'username': 'amin'}
2024-06-20 10:33:26,345 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi, my name is Amin.', 'memory': True, 'conversation_number': 6, 'username': 'amin'}
2024-06-20 10:33:26,345 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi, my name is Amin.', 'memory': True, 'conversation_number': 6, 'username': 'amin'}
2024-06-20 10:33:26,345 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi, my name is Amin.', 'memory': True, 'conversation_number': 6, 'username': 'amin'}
2024-06-20 10:33:26,345 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi, my name is Amin.', 'memory': True, 'conversation_number': 6, 'username': 'amin'}
2024-06-20 10:33:26,345 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi, my name is Amin.', 'memory': True, 'conversation_number': 6, 'username': 'amin'}
2024-06-20 10:33:26,345 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi, my name is Amin.', 'memory': True, 'conversation_number': 6, 'username': 'amin'}
2024-06-20 10:33:26,608 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 10:33:26,608 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 10:33:26,608 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 10:33:26,608 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 10:33:26,608 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 10:33:26,608 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 10:33:26,608 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 10:33:26,616 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f94d28c33a0>
2024-06-20 10:33:26,616 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f94d28c33a0>
2024-06-20 10:33:26,616 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f94d28c33a0>
2024-06-20 10:33:26,616 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f94d28c33a0>
2024-06-20 10:33:26,616 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f94d28c33a0>
2024-06-20 10:33:26,616 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f94d28c33a0>
2024-06-20 10:33:26,616 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f94d28c33a0>
2024-06-20 10:33:26,620 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f94d09a7760>
2024-06-20 10:33:26,620 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f94d09a7760>
2024-06-20 10:33:26,620 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f94d09a7760>
2024-06-20 10:33:26,620 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f94d09a7760>
2024-06-20 10:33:26,620 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f94d09a7760>
2024-06-20 10:33:26,620 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f94d09a7760>
2024-06-20 10:33:26,620 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f94d09a7760>
2024-06-20 10:33:26,620 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 10:33:26,620 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 10:33:26,620 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 10:33:26,620 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 10:33:26,620 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 10:33:26,620 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 10:33:26,620 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 10:34:16,251 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi my name is Amin', 'memory': True, 'conversation_number': 7, 'username': 'amin'}
2024-06-20 10:34:16,251 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi my name is Amin', 'memory': True, 'conversation_number': 7, 'username': 'amin'}
2024-06-20 10:34:16,251 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi my name is Amin', 'memory': True, 'conversation_number': 7, 'username': 'amin'}
2024-06-20 10:34:16,251 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi my name is Amin', 'memory': True, 'conversation_number': 7, 'username': 'amin'}
2024-06-20 10:34:16,251 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi my name is Amin', 'memory': True, 'conversation_number': 7, 'username': 'amin'}
2024-06-20 10:34:16,251 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi my name is Amin', 'memory': True, 'conversation_number': 7, 'username': 'amin'}
2024-06-20 10:34:16,251 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi my name is Amin', 'memory': True, 'conversation_number': 7, 'username': 'amin'}
2024-06-20 10:34:16,252 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi my name is Amin', 'memory': True, 'conversation_number': 7, 'username': 'amin'}
2024-06-20 10:34:16,252 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi my name is Amin', 'memory': True, 'conversation_number': 7, 'username': 'amin'}
2024-06-20 10:34:16,252 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi my name is Amin', 'memory': True, 'conversation_number': 7, 'username': 'amin'}
2024-06-20 10:34:16,252 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi my name is Amin', 'memory': True, 'conversation_number': 7, 'username': 'amin'}
2024-06-20 10:34:16,252 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi my name is Amin', 'memory': True, 'conversation_number': 7, 'username': 'amin'}
2024-06-20 10:34:16,252 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi my name is Amin', 'memory': True, 'conversation_number': 7, 'username': 'amin'}
2024-06-20 10:34:16,252 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi my name is Amin', 'memory': True, 'conversation_number': 7, 'username': 'amin'}
2024-06-20 10:34:16,514 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 10:34:16,514 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 10:34:16,514 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 10:34:16,514 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 10:34:16,514 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 10:34:16,514 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 10:34:16,514 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 10:34:16,523 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f490702f3a0>
2024-06-20 10:34:16,523 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f490702f3a0>
2024-06-20 10:34:16,523 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f490702f3a0>
2024-06-20 10:34:16,523 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f490702f3a0>
2024-06-20 10:34:16,523 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f490702f3a0>
2024-06-20 10:34:16,523 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f490702f3a0>
2024-06-20 10:34:16,523 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7f490702f3a0>
2024-06-20 10:34:16,526 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f49050931f0>
2024-06-20 10:34:16,526 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f49050931f0>
2024-06-20 10:34:16,526 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f49050931f0>
2024-06-20 10:34:16,526 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f49050931f0>
2024-06-20 10:34:16,526 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f49050931f0>
2024-06-20 10:34:16,526 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f49050931f0>
2024-06-20 10:34:16,526 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7f49050931f0>
2024-06-20 10:34:16,527 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 10:34:16,527 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 10:34:16,527 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 10:34:16,527 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 10:34:16,527 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 10:34:16,527 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 10:34:16,527 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 10:35:35,664 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': 8, 'username': 'amin'}
2024-06-20 10:35:35,664 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': 8, 'username': 'amin'}
2024-06-20 10:35:35,664 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': 8, 'username': 'amin'}
2024-06-20 10:35:35,664 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': 8, 'username': 'amin'}
2024-06-20 10:35:35,664 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': 8, 'username': 'amin'}
2024-06-20 10:35:35,664 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': 8, 'username': 'amin'}
2024-06-20 10:35:35,664 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': 8, 'username': 'amin'}
2024-06-20 10:35:35,665 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': 8, 'username': 'amin'}
2024-06-20 10:35:35,665 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': 8, 'username': 'amin'}
2024-06-20 10:35:35,665 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': 8, 'username': 'amin'}
2024-06-20 10:35:35,665 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': 8, 'username': 'amin'}
2024-06-20 10:35:35,665 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': 8, 'username': 'amin'}
2024-06-20 10:35:35,665 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': 8, 'username': 'amin'}
2024-06-20 10:35:35,665 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi', 'memory': True, 'conversation_number': 8, 'username': 'amin'}
2024-06-20 10:35:35,926 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 10:35:35,926 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 10:35:35,926 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 10:35:35,926 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 10:35:35,926 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 10:35:35,926 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 10:35:35,926 - llm_as_a_service_logger - INFO - Retrieving conversation...
2024-06-20 10:35:35,935 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7ff933127280>
2024-06-20 10:35:35,935 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7ff933127280>
2024-06-20 10:35:35,935 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7ff933127280>
2024-06-20 10:35:35,935 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7ff933127280>
2024-06-20 10:35:35,935 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7ff933127280>
2024-06-20 10:35:35,935 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7ff933127280>
2024-06-20 10:35:35,935 - llm_as_a_service_logger - INFO - User found: <app.routers.LLM.backend_database.User object at 0x7ff933127280>
2024-06-20 10:35:35,938 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7ff9311f3f70>
2024-06-20 10:35:35,938 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7ff9311f3f70>
2024-06-20 10:35:35,938 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7ff9311f3f70>
2024-06-20 10:35:35,938 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7ff9311f3f70>
2024-06-20 10:35:35,938 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7ff9311f3f70>
2024-06-20 10:35:35,938 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7ff9311f3f70>
2024-06-20 10:35:35,938 - llm_as_a_service_logger - INFO - Conversation found: <app.routers.LLM.backend_database.Conversation object at 0x7ff9311f3f70>
2024-06-20 10:35:35,939 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 10:35:35,939 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 10:35:35,939 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 10:35:35,939 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 10:35:35,939 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 10:35:35,939 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 10:35:35,939 - llm_as_a_service_logger - INFO - Conversation successfully retrieved
2024-06-20 10:36:38,706 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 10:36:38,706 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 10:36:38,706 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 10:36:38,706 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 10:36:38,706 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 10:36:38,706 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 10:36:38,706 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 10:36:38,707 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 10:36:38,707 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 10:36:38,707 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 10:36:38,707 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 10:36:38,707 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 10:36:38,707 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 10:36:38,707 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 10:37:13,883 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 10:37:13,883 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 10:37:13,883 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 10:37:13,883 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 10:37:13,883 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 10:37:13,883 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 10:37:13,883 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 10:37:13,884 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 10:37:13,884 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 10:37:13,884 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 10:37:13,884 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 10:37:13,884 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 10:37:13,884 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 10:37:13,884 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:08:25,362 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:08:25,362 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:08:25,362 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:08:25,362 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:08:25,362 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:08:25,362 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:08:25,362 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:08:25,363 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:08:25,363 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:08:25,363 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:08:25,363 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:08:25,363 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:08:25,363 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:08:25,363 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:15:04,738 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:15:04,738 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:15:04,738 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:15:04,738 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:15:04,738 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:15:04,738 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:15:04,738 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:15:04,738 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:15:04,738 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:15:04,738 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:15:04,738 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:15:04,738 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:15:04,738 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:15:04,738 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:16:13,427 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user admin
2024-06-20 11:16:13,427 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user admin
2024-06-20 11:16:13,427 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user admin
2024-06-20 11:16:13,427 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user admin
2024-06-20 11:16:13,427 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user admin
2024-06-20 11:16:13,427 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user admin
2024-06-20 11:16:13,427 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user admin
2024-06-20 11:16:19,594 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:16:19,594 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:16:19,594 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:16:19,594 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:16:19,594 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:16:19,594 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:16:19,594 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:16:19,595 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:16:19,595 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:16:19,595 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:16:19,595 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:16:19,595 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:16:19,595 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:16:19,595 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'what is nuclear fusion ', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:18:09,856 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user admin
2024-06-20 11:18:09,856 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user admin
2024-06-20 11:18:09,856 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user admin
2024-06-20 11:18:09,856 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user admin
2024-06-20 11:18:09,856 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user admin
2024-06-20 11:18:09,856 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user admin
2024-06-20 11:18:09,856 - llm_as_a_service_logger - INFO - login_for_access_token: valid credentials for user admin
2024-06-20 11:18:17,888 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:18:17,888 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:18:17,888 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:18:17,888 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:18:17,888 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:18:17,888 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:18:17,888 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:18:17,888 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:18:17,888 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:18:17,888 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:18:17,888 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:18:17,888 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:18:17,888 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:18:17,888 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:19:50,513 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:19:50,513 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:19:50,513 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:19:50,513 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:19:50,513 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:19:50,513 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:19:50,513 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:19:50,514 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:19:50,514 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:19:50,514 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:19:50,514 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:19:50,514 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:19:50,514 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:19:50,514 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:23:49,557 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:23:49,557 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:23:49,557 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:23:49,557 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:23:49,557 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:23:49,557 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:23:49,557 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:23:49,558 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:23:49,558 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:23:49,558 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:23:49,558 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:23:49,558 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:23:49,558 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:23:49,558 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:23:49,822 - llm_as_a_service_logger - ERROR - Error in Inference Call: Missing keys ['session_id'] in config['configurable'] Expected keys are ['session_id'].When using via .invoke() or .stream(), pass in a config; e.g., chain.invoke({'input': 'foo'}, {'configurable': {'session_id': '[your-value-here]'}})
2024-06-20 11:23:49,822 - llm_as_a_service_logger - ERROR - Error in Inference Call: Missing keys ['session_id'] in config['configurable'] Expected keys are ['session_id'].When using via .invoke() or .stream(), pass in a config; e.g., chain.invoke({'input': 'foo'}, {'configurable': {'session_id': '[your-value-here]'}})
2024-06-20 11:23:49,822 - llm_as_a_service_logger - ERROR - Error in Inference Call: Missing keys ['session_id'] in config['configurable'] Expected keys are ['session_id'].When using via .invoke() or .stream(), pass in a config; e.g., chain.invoke({'input': 'foo'}, {'configurable': {'session_id': '[your-value-here]'}})
2024-06-20 11:23:49,822 - llm_as_a_service_logger - ERROR - Error in Inference Call: Missing keys ['session_id'] in config['configurable'] Expected keys are ['session_id'].When using via .invoke() or .stream(), pass in a config; e.g., chain.invoke({'input': 'foo'}, {'configurable': {'session_id': '[your-value-here]'}})
2024-06-20 11:23:49,822 - llm_as_a_service_logger - ERROR - Error in Inference Call: Missing keys ['session_id'] in config['configurable'] Expected keys are ['session_id'].When using via .invoke() or .stream(), pass in a config; e.g., chain.invoke({'input': 'foo'}, {'configurable': {'session_id': '[your-value-here]'}})
2024-06-20 11:23:49,822 - llm_as_a_service_logger - ERROR - Error in Inference Call: Missing keys ['session_id'] in config['configurable'] Expected keys are ['session_id'].When using via .invoke() or .stream(), pass in a config; e.g., chain.invoke({'input': 'foo'}, {'configurable': {'session_id': '[your-value-here]'}})
2024-06-20 11:23:49,822 - llm_as_a_service_logger - ERROR - Error in Inference Call: Missing keys ['session_id'] in config['configurable'] Expected keys are ['session_id'].When using via .invoke() or .stream(), pass in a config; e.g., chain.invoke({'input': 'foo'}, {'configurable': {'session_id': '[your-value-here]'}})
2024-06-20 11:29:00,464 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:29:00,464 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:29:00,464 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:29:00,464 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:29:00,464 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:29:00,464 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:29:00,464 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:29:00,465 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:29:00,465 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:29:00,465 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:29:00,465 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:29:00,465 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:29:00,465 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:29:00,465 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:30:29,656 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:30:29,656 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:30:29,656 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:30:29,656 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:30:29,656 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:30:29,656 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:30:29,656 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:30:29,656 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:30:29,656 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:30:29,656 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:30:29,656 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:30:29,656 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:30:29,656 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:30:29,656 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:31:13,646 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:31:13,646 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:31:13,646 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:31:13,646 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:31:13,646 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:31:13,646 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:31:13,646 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:31:13,647 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:31:13,647 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:31:13,647 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:31:13,647 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:31:13,647 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:31:13,647 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:31:13,647 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:31:31,475 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot, my name is Amin', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:31:31,475 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot, my name is Amin', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:31:31,475 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot, my name is Amin', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:31:31,475 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot, my name is Amin', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:31:31,475 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot, my name is Amin', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:31:31,475 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot, my name is Amin', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:31:31,475 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot, my name is Amin', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:31:31,476 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot, my name is Amin', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:31:31,476 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot, my name is Amin', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:31:31,476 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot, my name is Amin', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:31:31,476 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot, my name is Amin', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:31:31,476 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot, my name is Amin', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:31:31,476 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot, my name is Amin', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:31:31,476 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot, my name is Amin', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:34:24,558 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot, my name is Amin', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:34:24,558 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot, my name is Amin', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:34:24,558 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot, my name is Amin', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:34:24,558 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot, my name is Amin', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:34:24,558 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot, my name is Amin', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:34:24,558 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot, my name is Amin', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:34:24,558 - llm_as_a_service_logger - INFO - Received request by router: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot, my name is Amin', 'memory': False, 'conversation_number': -1, 'username': 'None'}
2024-06-20 11:34:24,559 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot, my name is Amin', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:34:24,559 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot, my name is Amin', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:34:24,559 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot, my name is Amin', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:34:24,559 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot, my name is Amin', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:34:24,559 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot, my name is Amin', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:34:24,559 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot, my name is Amin', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
2024-06-20 11:34:24,559 - llm_as_a_service_logger - INFO - Received request by backend: {'model': 'meta-llama/Llama-2-7b-chat-hf', 'inference_endpoint': 'http://localhost:8500/v1', 'prompt': 'Hi Nscale chatbot, my name is Amin', 'memory': False, 'conversation_number': -1, 'username': 'admin'}
